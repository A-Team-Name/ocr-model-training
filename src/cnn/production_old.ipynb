{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1046, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Leon\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_21992\\501440772.py\", line 18, in <module>\n",
      "    from torchvision.transforms.functional import rotate, affine, resize, center_crop\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\__init__.py\", line 6, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\__init__.py\", line 17, in <module>\n",
      "    from . import detection, optical_flow, quantization, segmentation, video\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\faster_rcnn.py\", line 16, in <module>\n",
      "    from .anchor_utils import AnchorGenerator\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py\", line 10, in <module>\n",
      "    class AnchorGenerator(nn.Module):\n",
      "  File \"c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py\", line 63, in AnchorGenerator\n",
      "    device: torch.device = torch.device(\"cpu\"),\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from typing import Any\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import torch.nn.functional as torch_func\n",
    "from torchvision.transforms.functional import rotate, affine, resize, center_crop\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.allcnn2d import AllCNN2D, AllCNN2D_Prod\n",
    "from drawing.interactive import draw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = os.path.abspath(\".\")\n",
    "root_path: str = os.path.join(file_path, os.pardir, os.pardir)\n",
    "checkpoint_path: str = os.path.join(\n",
    "    root_path, \n",
    "    \"checkpoints\", \n",
    "    \"GeckoFull_epoch1980_trainacc0.90605_valacc0.98089_Tloss0.78068_Vloss0.20101_lr6.320194197753456e-11.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AllCNN2D_Prod                            [1, 44]                   --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
      "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
      "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
      "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
      "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
      "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
      "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
      "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
      "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
      "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
      "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
      "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
      "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
      "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
      "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
      "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
      "├─Sequential: 1-2                        [1, 128]                  --\n",
      "│    └─Flatten: 2-6                      [1, 128]                  --\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─Sequential: 2-7                   [1, 64]                   --\n",
      "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
      "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
      "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
      "│    └─Sequential: 2-8                   [1, 44]                   --\n",
      "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
      "==========================================================================================\n",
      "Total params: 83,548\n",
      "Trainable params: 83,548\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 14.05\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.18\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 2.53\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "alphabet: list[str] = ['(', ')', '+', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'λ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '×', '÷']\n",
    "\n",
    "model: AllCNN2D_Prod = AllCNN2D_Prod(\n",
    "    labels_map=alphabet,\n",
    "    **{\n",
    "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
    "        \"fully_connected_features\": (64, 44),\n",
    "        \"expected_input_size\": (64, 64),\n",
    "        \"device\": \"cpu\",\n",
    "        \"conv_dropout\": 0.0,\n",
    "        \"verbose\": True,\n",
    "        \"name_prefix\": \"GeckoFinal\",\n",
    "        #\"checkpoint_path\": checkpoint_path\n",
    "    }\n",
    "\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '(', 40),\n",
       " (1, ')', 41),\n",
       " (2, '+', 43),\n",
       " (3, '-', 45),\n",
       " (4, '.', 46),\n",
       " (5, '0', 48),\n",
       " (6, '1', 49),\n",
       " (7, '2', 50),\n",
       " (8, '3', 51),\n",
       " (9, '4', 52),\n",
       " (10, '5', 53),\n",
       " (11, '6', 54),\n",
       " (12, '7', 55),\n",
       " (13, '8', 56),\n",
       " (14, '9', 57),\n",
       " (15, 'λ', 955),\n",
       " (16, 'a', 97),\n",
       " (17, 'b', 98),\n",
       " (18, 'c', 99),\n",
       " (19, 'd', 100),\n",
       " (20, 'e', 101),\n",
       " (21, 'f', 102),\n",
       " (22, 'g', 103),\n",
       " (23, 'h', 104),\n",
       " (24, 'i', 105),\n",
       " (25, 'j', 106),\n",
       " (26, 'k', 107),\n",
       " (27, 'l', 108),\n",
       " (28, 'm', 109),\n",
       " (29, 'n', 110),\n",
       " (30, 'o', 111),\n",
       " (31, 'p', 112),\n",
       " (32, 'q', 113),\n",
       " (33, 'r', 114),\n",
       " (34, 's', 115),\n",
       " (35, 't', 116),\n",
       " (36, 'u', 117),\n",
       " (37, 'v', 118),\n",
       " (38, 'w', 119),\n",
       " (39, 'x', 120),\n",
       " (40, 'y', 121),\n",
       " (41, 'z', 122),\n",
       " (42, '×', 215),\n",
       " (43, '÷', 247)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, char, ord(char)) for i, char in enumerate(alphabet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model saved to model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\models\\allcnn2d.py:573: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  softmaxed_char.tolist(),\n",
      "c:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\models\\allcnn2d.py:585: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  softmax_ordered: Tensor = tensor(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
    "\n",
    "# Define the ONNX file path\n",
    "onnx_file_path = \"model.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    input_names=[\"input\"],  # Name of the input layer\n",
    "    output_names=[\"logits\", \"softmax\", \"softmax_ordered\"],  # Names of the output layers\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"},  # Variable batch size\n",
    "                  \"logits\": {0: \"batch_size\"},\n",
    "                  \"softmax\": {0: \"batch_size\"},\n",
    "                  \"softmax_ordered\": {0: \"batch_size\"}},\n",
    "    opset_version=11  # Specify the ONNX opset version\n",
    ")\n",
    "\n",
    "print(f\"Model saved to {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"model.onnx\"\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "input_image = np.random.random(\n",
    "    (\n",
    "        1,  # batch: stack as many images as you like here\n",
    "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
    "        64, # height: fixed to 64 pixels for now\n",
    "        64  # width: fixed to 64 pixels for now\n",
    "    )\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
    "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
    "\n",
    "input_name: list[str] = inputs[0].name\n",
    "output_names: list[str] = [out.name for out in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "softmax: np.ndarray\n",
    "softmax_ordered: np.ndarray\n",
    "logits: np.ndarray\n",
    "\n",
    "logits, softmax, softmax_ordered = session.run(\n",
    "    output_names, \n",
    "    {input_name: input_image}\n",
    ")\n",
    "\n",
    "# logits.shape is shape (batch, character) for all character labels\n",
    "# softmax.shape is shape (batch, character) for all character labels\n",
    "# softmax_ordered is shape (batch, character, [label index, label prob, unicode character value])\n",
    "\n",
    "# character dim is 44 (there are 44 character labels)\n",
    "# label index is from 0 to 44 (corresponding to each ordered label index)\n",
    "# label prob is a softmaxed probability for this label prediction\n",
    "# unicode character value is the unicode character for this prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 44), (1, 44), (1, 44, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, softmax.shape, softmax_ordered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top_character_probs: list[list[float]] = softmax_ordered[:, :, 1].tolist()\n",
    "\n",
    "top_characters: list[list[str]] = [\n",
    "    [\n",
    "        chr(int(softmax_ordered[batch_i, i, 2])) \n",
    "        for i in range(softmax_ordered.shape[1])\n",
    "    ] for batch_i in \n",
    "    range(softmax_ordered.shape[0])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextReShitter(nn.Module):\n",
    "    \n",
    "    def __init__(self, alphabet: list[str], text: str):\n",
    "        \n",
    "        super(TextReShitter, self).__init__()\n",
    "        \n",
    "        self.alphabet: list[str] = sorted(alphabet)\n",
    "        self.text: str = text\n",
    "        \n",
    "        self.logits: list[torch.Tensor] = []\n",
    "        \n",
    "        for c in self.text:\n",
    "            c_index: int = self.alphabet.index(c)\n",
    "            \n",
    "            char_logits: torch.Tensor = torch.rand((len(alphabet),))\n",
    "            char_logits[c_index] += 2\n",
    "            char_logits *= (torch.rand(1)+0.1)\n",
    "\n",
    "            self.logits.append(char_logits)\n",
    "\n",
    "        self.logits_tensor: torch.Tensor = torch.stack(self.logits, dim=0)\n",
    "        \n",
    "        self.logits_tensor = self.logits_tensor.unsqueeze(0) # batch, chars, logit\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        self.logits_tensor = self.logits_tensor + x.sum() * 0.0  \n",
    "\n",
    "        softmax = torch.softmax(self.logits_tensor, dim=-1)\n",
    "        \n",
    "        # Create indices for the first dimension (char_prob_i)\n",
    "        char_prob_indices = torch.arange(softmax.size(-1), device=softmax.device).reshape(1, 1, -1, 1)\n",
    "        \n",
    "        # Create indices for the third dimension (ord(self.alphabet[char_prob_i]))\n",
    "        alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n",
    "        \n",
    "        # Expand dimensions to match softmax shape\n",
    "        char_prob_indices = char_prob_indices.expand(*softmax.shape, 1)\n",
    "        alphabet_indices = alphabet_indices.expand(*softmax.shape, 1)\n",
    "        \n",
    "        # Concatenate along the last dimension\n",
    "        softmax_ordered = torch.cat([char_prob_indices, softmax.unsqueeze(-1), alphabet_indices], dim=-1)\n",
    "        \n",
    "        # Sort along the probability dimension (dim=-2)\n",
    "        sorting_indices = softmax_ordered[..., 1].argsort(dim=-1, descending=True)\n",
    "        sorted_tensor = torch.gather(softmax_ordered, -2, sorting_indices.unsqueeze(-1).expand(-1, -1, -1, 3))\n",
    "        \n",
    "        x = x * 0.5\n",
    "        \n",
    "        return self.logits_tensor, softmax, sorted_tensor\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 11, 44]), torch.Size([1, 11, 44]), torch.Size([1, 11, 44, 3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_world_model = TextReShitter(alphabet, \"hello.world\")\n",
    "\n",
    "logits, softmax, softmax_ord = hello_world_model.forward(torch.zeros(1, 1, 1, 1))\n",
    "\n",
    "logits.shape, softmax.shape, softmax_ord.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX: Save Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model saved to hello_world.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_21992\\3231084438.py:35: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 1, 1, 1)  # Example input\n",
    "\n",
    "# Define the ONNX file path\n",
    "onnx_file_path = \"hello_world.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    hello_world_model,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    input_names=[\"input\"],  # Name of the input layer\n",
    "    output_names=[\"logits\", \"softmax\", \"softmax_ordered\"],  # Names of the output layers\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # Variable batch size\n",
    "                  \"logits\": {0: \"batch_size\"},\n",
    "                  \"softmax\": {0: \"batch_size\"},\n",
    "                  \"softmax_ordered\": {0: \"batch_size\"}},\n",
    "    opset_version=11  # Specify the ONNX opset version\n",
    ")\n",
    "\n",
    "print(f\"Model saved to {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Hello World Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[104. 120. 115.]\n",
      "  [101. 105. 109.]\n",
      "  [108.  45. 118.]\n",
      "  [108.  50.  55.]\n",
      "  [111. 247.  41.]\n",
      "  [ 46. 111.  52.]\n",
      "  [119. 104. 118.]\n",
      "  [111. 117.  53.]\n",
      "  [114.  57.  45.]\n",
      "  [108.  97. 112.]\n",
      "  [100. 111. 104.]]]\n",
      "[[[0.13487485 0.03051421 0.03024892]\n",
      "  [0.03483254 0.02470074 0.0245183 ]\n",
      "  [0.12618831 0.03325844 0.03256983]\n",
      "  [0.14055243 0.02988121 0.02889052]\n",
      "  [0.03572701 0.02537489 0.02510708]\n",
      "  [0.0649533  0.0259773  0.02597003]\n",
      "  [0.03041911 0.02436568 0.02433025]\n",
      "  [0.09235369 0.03029493 0.03025205]\n",
      "  [0.08532248 0.03095609 0.0284572 ]\n",
      "  [0.03220282 0.02440088 0.02422533]\n",
      "  [0.02972033 0.02469328 0.0244364 ]]]\n",
      "#1 @ 13.49% - h\n",
      "#2 @ 3.051% - x\n",
      "#3 @ 3.025% - s\n",
      "--\n",
      "#1 @ 3.483% - e\n",
      "#2 @ 2.47% - i\n",
      "#3 @ 2.452% - m\n",
      "--\n",
      "#1 @ 12.62% - l\n",
      "#2 @ 3.326% - -\n",
      "#3 @ 3.257% - v\n",
      "--\n",
      "#1 @ 14.06% - l\n",
      "#2 @ 2.988% - 2\n",
      "#3 @ 2.889% - 7\n",
      "--\n",
      "#1 @ 3.573% - o\n",
      "#2 @ 2.537% - ÷\n",
      "#3 @ 2.511% - )\n",
      "--\n",
      "#1 @ 6.495% - .\n",
      "#2 @ 2.598% - o\n",
      "#3 @ 2.597% - 4\n",
      "--\n",
      "#1 @ 3.042% - w\n",
      "#2 @ 2.437% - h\n",
      "#3 @ 2.433% - v\n",
      "--\n",
      "#1 @ 9.235% - o\n",
      "#2 @ 3.029% - u\n",
      "#3 @ 3.025% - 5\n",
      "--\n",
      "#1 @ 8.532% - r\n",
      "#2 @ 3.096% - 9\n",
      "#3 @ 2.846% - -\n",
      "--\n",
      "#1 @ 3.22% - l\n",
      "#2 @ 2.44% - a\n",
      "#3 @ 2.423% - p\n",
      "--\n",
      "#1 @ 2.972% - d\n",
      "#2 @ 2.469% - o\n",
      "#3 @ 2.444% - h\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"hello_world_dyn.onnx\"\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "input_image = np.random.random(\n",
    "    (\n",
    "        1,  # batch: stack as many images as you like here\n",
    "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
    "        2,  # height: fixed to 1 for now\n",
    "        1   # width: fixed to 1 for now\n",
    "    )\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
    "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
    "\n",
    "input_name: str = inputs[0].name\n",
    "output_names: list[str] = [out.name for out in outputs]\n",
    "softmax: np.ndarray\n",
    "softmax_ordered: np.ndarray\n",
    "logits: np.ndarray\n",
    "\n",
    "logits, softmax, softmax_ordered = session.run(\n",
    "    output_names, \n",
    "    {input_name: input_image}\n",
    ")\n",
    "\n",
    "top3_preds: np.ndarray = softmax_ordered[:, :, :3, 2] # top three predictions for each character (unicode value, convert to string with chr)\n",
    "top3_pred_probs: np.ndarray = softmax_ordered[:, :, :3, 1] # top three prediction probabilities\n",
    "\n",
    "print(top3_preds)\n",
    "\n",
    "print(top3_pred_probs)\n",
    "\n",
    "for letter_i in range(top3_preds.shape[1]):\n",
    "    for k in range(3):\n",
    "        \n",
    "        pred_percentage: float = top3_pred_probs[0, letter_i, k]*100\n",
    "        \n",
    "        print(f\"#{k+1} @ {pred_percentage:.4}% - {chr(int(top3_preds[0, letter_i, k]))}\")\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: encoder_conv_blocks.0.0.weight\n",
      "Loaded: encoder_conv_blocks.0.0.bias\n",
      "Loaded: encoder_conv_blocks.0.2.weight\n",
      "Loaded: encoder_conv_blocks.0.2.bias\n",
      "Loaded: encoder_conv_blocks.0.2.running_mean\n",
      "Loaded: encoder_conv_blocks.0.2.running_var\n",
      "Loaded: encoder_conv_blocks.0.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.0.4.weight\n",
      "Loaded: encoder_conv_blocks.0.4.bias\n",
      "Loaded: encoder_conv_blocks.0.6.weight\n",
      "Loaded: encoder_conv_blocks.0.6.bias\n",
      "Loaded: encoder_conv_blocks.0.6.running_mean\n",
      "Loaded: encoder_conv_blocks.0.6.running_var\n",
      "Loaded: encoder_conv_blocks.0.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.1.0.weight\n",
      "Loaded: encoder_conv_blocks.1.0.bias\n",
      "Loaded: encoder_conv_blocks.1.2.weight\n",
      "Loaded: encoder_conv_blocks.1.2.bias\n",
      "Loaded: encoder_conv_blocks.1.2.running_mean\n",
      "Loaded: encoder_conv_blocks.1.2.running_var\n",
      "Loaded: encoder_conv_blocks.1.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.1.4.weight\n",
      "Loaded: encoder_conv_blocks.1.4.bias\n",
      "Loaded: encoder_conv_blocks.1.6.weight\n",
      "Loaded: encoder_conv_blocks.1.6.bias\n",
      "Loaded: encoder_conv_blocks.1.6.running_mean\n",
      "Loaded: encoder_conv_blocks.1.6.running_var\n",
      "Loaded: encoder_conv_blocks.1.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.2.0.weight\n",
      "Loaded: encoder_conv_blocks.2.0.bias\n",
      "Loaded: encoder_conv_blocks.2.2.weight\n",
      "Loaded: encoder_conv_blocks.2.2.bias\n",
      "Loaded: encoder_conv_blocks.2.2.running_mean\n",
      "Loaded: encoder_conv_blocks.2.2.running_var\n",
      "Loaded: encoder_conv_blocks.2.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.2.4.weight\n",
      "Loaded: encoder_conv_blocks.2.4.bias\n",
      "Loaded: encoder_conv_blocks.2.6.weight\n",
      "Loaded: encoder_conv_blocks.2.6.bias\n",
      "Loaded: encoder_conv_blocks.2.6.running_mean\n",
      "Loaded: encoder_conv_blocks.2.6.running_var\n",
      "Loaded: encoder_conv_blocks.2.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.3.0.weight\n",
      "Loaded: encoder_conv_blocks.3.0.bias\n",
      "Loaded: encoder_conv_blocks.3.2.weight\n",
      "Loaded: encoder_conv_blocks.3.2.bias\n",
      "Loaded: encoder_conv_blocks.3.2.running_mean\n",
      "Loaded: encoder_conv_blocks.3.2.running_var\n",
      "Loaded: encoder_conv_blocks.3.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.3.4.weight\n",
      "Loaded: encoder_conv_blocks.3.4.bias\n",
      "Loaded: encoder_conv_blocks.3.6.weight\n",
      "Loaded: encoder_conv_blocks.3.6.bias\n",
      "Loaded: encoder_conv_blocks.3.6.running_mean\n",
      "Loaded: encoder_conv_blocks.3.6.running_var\n",
      "Loaded: encoder_conv_blocks.3.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.4.0.weight\n",
      "Loaded: encoder_conv_blocks.4.0.bias\n",
      "Loaded: encoder_conv_blocks.4.2.weight\n",
      "Loaded: encoder_conv_blocks.4.2.bias\n",
      "Loaded: encoder_conv_blocks.4.2.running_mean\n",
      "Loaded: encoder_conv_blocks.4.2.running_var\n",
      "Loaded: encoder_conv_blocks.4.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.4.4.weight\n",
      "Loaded: encoder_conv_blocks.4.4.bias\n",
      "Loaded: encoder_conv_blocks.4.6.weight\n",
      "Loaded: encoder_conv_blocks.4.6.bias\n",
      "Loaded: encoder_conv_blocks.4.6.running_mean\n",
      "Loaded: encoder_conv_blocks.4.6.running_var\n",
      "Loaded: encoder_conv_blocks.4.6.num_batches_tracked\n",
      "Loaded: fully_connected_blocks.0.0.weight\n",
      "Loaded: fully_connected_blocks.0.0.bias\n",
      "Loaded: fully_connected_blocks.1.0.weight\n",
      "Loaded: fully_connected_blocks.1.0.bias\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AllCNN2D                                 [1, 44]                   --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
      "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
      "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
      "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
      "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
      "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
      "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
      "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
      "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
      "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
      "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
      "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
      "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
      "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
      "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
      "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
      "├─Sequential: 1-2                        [1, 128]                  --\n",
      "│    └─Flatten: 2-6                      [1, 128]                  --\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─Sequential: 2-7                   [1, 64]                   --\n",
      "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
      "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
      "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
      "│    └─Sequential: 2-8                   [1, 44]                   --\n",
      "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
      "==========================================================================================\n",
      "Total params: 83,548\n",
      "Trainable params: 83,548\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 14.05\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.18\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 2.53\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_21992\\3063487262.py:33: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if coords.shape[0] > 0:\n",
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_21992\\3063487262.py:34: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  min_x, min_y = coords.min(dim=0)[0].tolist()[2:]\n",
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_21992\\3063487262.py:35: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_x, max_y = coords.max(dim=0)[0].tolist()[2:]\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:306: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model saved to krud_char_model.onnx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class KrudModel(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        alphabet: list[str], \n",
    "        target_width: int = 64, \n",
    "        target_height: int = 64, \n",
    "        padding: int = 1,\n",
    "        model: torch.nn.Module = None\n",
    "    ):\n",
    "        \n",
    "        super(KrudModel, self).__init__()\n",
    "        \n",
    "        self.alphabet: list[str] = sorted(alphabet)\n",
    "        self.alphabet_float: torch.Tensor = torch.tensor([ord(a) for a in self.alphabet], dtype=torch.float32) \n",
    "        self.target_width: int = target_width\n",
    "        self.target_height: int = target_height\n",
    "        self.padding: int = padding\n",
    "        self._unpadded_dims: tuple[int] = (\n",
    "            self.target_height-self.padding*2,\n",
    "            self.target_width-self.padding*2\n",
    "        )\n",
    "        self.model: torch.nn.Module = model.eval()\n",
    "    \n",
    "    def preprocess_image(\n",
    "        self, \n",
    "        im: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        \n",
    "        mask = im > 0.5  # Assuming foreground is black (0) and background is white (1)\n",
    "        coords = torch.nonzero(mask)\n",
    "        if coords.shape[0] > 0:\n",
    "            min_x, min_y = coords.min(dim=0)[0].tolist()[2:]\n",
    "            max_x, max_y = coords.max(dim=0)[0].tolist()[2:]\n",
    "            \n",
    "            im = im[:, :, min_x:max_x + 1, min_y:max_y + 1]\n",
    "        else:\n",
    "            # If no black pixels are found, return the original image\n",
    "            pass\n",
    "\n",
    "        im: torch.Tensor = torch_func.interpolate(\n",
    "            im, \n",
    "            size=self._unpadded_dims, \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        im = (im > 0.5).type(torch.uint8).type(torch.float32)\n",
    "\n",
    "        im = torch_func.pad(\n",
    "            im,\n",
    "            (\n",
    "                self.padding, \n",
    "                self.padding, \n",
    "                self.padding, \n",
    "                self.padding\n",
    "            ),\n",
    "            mode='constant',\n",
    "            value=0.0\n",
    "        )\n",
    "        return im\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.preprocess_image(x)\n",
    "        \n",
    "        y_hat: torch.Tensor = self.model(x)\n",
    "        y_hat = y_hat.squeeze().squeeze()\n",
    "        \n",
    "        softmax = torch.softmax(y_hat, dim=-1)\n",
    "        \n",
    "        stacked = torch.stack([self.alphabet_float, softmax], dim=1)\n",
    "        sorted_indices = torch.argsort(stacked[:, 1], descending=True)\n",
    "        sorted_tensor = stacked[sorted_indices]\n",
    "\n",
    "        return sorted_tensor\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "label_map: list[str] = ['(', ')', '+', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'λ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '×', '÷']\n",
    "\n",
    "checkpoint_path: str = os.path.join(\n",
    "    root_path, \n",
    "    \"checkpoints\", \n",
    "    \"Krud_epoch29_trainacc0.93466_valacc0.97159_Tloss0.013132_Vloss0.0056677_lr0.0007224.pkl\"\n",
    ")\n",
    "\n",
    "model: AllCNN2D = AllCNN2D(\n",
    "    **{\n",
    "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
    "        \"fully_connected_features\": (64, 44),\n",
    "        \"expected_input_size\": (64, 64),\n",
    "        \"device\": \"cpu\",\n",
    "        \"conv_dropout\": 0.0,\n",
    "        \"verbose\": True,\n",
    "        \"name_prefix\": \"KrudEval\",\n",
    "        \"checkpoint_path\": checkpoint_path\n",
    "    }\n",
    "\n",
    ").eval()\n",
    "krud_model: KrudModel = KrudModel(\n",
    "    label_map,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 100, 100)  # Example input\n",
    "\n",
    "# Define the ONNX file path\n",
    "onnx_file_path = \"krud_char_model.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    krud_model,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    input_names=[\"input\"],  # Name of the input layer\n",
    "    output_names=[\"softmax_ordered\"],  # Names of the output layers\n",
    "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}},\n",
    "    opset_version=11  # Specify the ONNX opset version\n",
    ")\n",
    "\n",
    "print(f\"Model saved to {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3000e+01, 9.3858e-01],\n",
       "        [2.4700e+02, 5.7565e-02],\n",
       "        [9.5500e+02, 1.5453e-03],\n",
       "        [5.5000e+01, 1.4862e-03],\n",
       "        [1.2100e+02, 5.4839e-04],\n",
       "        [9.7000e+01, 1.8937e-04],\n",
       "        [1.0300e+02, 4.8974e-05],\n",
       "        [1.2200e+02, 2.8834e-05],\n",
       "        [1.0500e+02, 1.5799e-06],\n",
       "        [1.0800e+02, 8.6764e-07],\n",
       "        [5.2000e+01, 6.3947e-07],\n",
       "        [1.1700e+02, 1.6689e-07],\n",
       "        [1.0100e+02, 1.0515e-07],\n",
       "        [4.9000e+01, 9.0722e-08],\n",
       "        [2.1500e+02, 6.1379e-08],\n",
       "        [1.1000e+02, 4.4690e-08],\n",
       "        [1.1400e+02, 3.3480e-08],\n",
       "        [5.7000e+01, 1.2193e-08],\n",
       "        [1.1500e+02, 3.4486e-09],\n",
       "        [1.1900e+02, 9.5328e-10],\n",
       "        [5.3000e+01, 3.0586e-10],\n",
       "        [5.6000e+01, 1.8158e-10],\n",
       "        [9.8000e+01, 1.6832e-10],\n",
       "        [1.2000e+02, 7.8393e-11],\n",
       "        [4.5000e+01, 4.7620e-11],\n",
       "        [1.1100e+02, 3.2993e-11],\n",
       "        [5.1000e+01, 1.9548e-11],\n",
       "        [1.1300e+02, 5.6589e-12],\n",
       "        [1.0600e+02, 2.6825e-12],\n",
       "        [1.0400e+02, 2.6386e-12],\n",
       "        [5.0000e+01, 5.4703e-13],\n",
       "        [9.9000e+01, 4.1399e-13],\n",
       "        [1.1800e+02, 3.9216e-13],\n",
       "        [1.1600e+02, 1.3443e-13],\n",
       "        [5.4000e+01, 1.4182e-14],\n",
       "        [4.1000e+01, 8.0470e-15],\n",
       "        [1.0200e+02, 4.4367e-15],\n",
       "        [1.0900e+02, 6.8224e-16],\n",
       "        [4.0000e+01, 3.5865e-16],\n",
       "        [1.0700e+02, 1.7180e-17],\n",
       "        [1.0000e+02, 1.5004e-17],\n",
       "        [4.8000e+01, 1.1964e-17],\n",
       "        [1.1200e+02, 6.5156e-19],\n",
       "        [4.6000e+01, 2.4627e-22]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plus_im: np.ndarray = np.asarray(Image.open(r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\data\\lambda\\u2b-1736974496947.png\"))\n",
    "\n",
    "plus_im = plus_im[:, :, 3]\n",
    "\n",
    "\n",
    "plus_im = plus_im.astype(np.float32)/255\n",
    "\n",
    "plus_im_ten = torch.tensor(plus_im, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "krud_model.forward(plus_im_ten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Krud ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.6000000e+01, 3.2618961e-01],\n",
       "       [4.9000000e+01, 2.3115756e-01],\n",
       "       [1.0600000e+02, 1.2512454e-01],\n",
       "       [4.0000000e+01, 6.1482534e-02],\n",
       "       [4.5000000e+01, 3.7283048e-02],\n",
       "       [1.0900000e+02, 2.9148577e-02],\n",
       "       [1.0000000e+02, 1.5275126e-02],\n",
       "       [5.5000000e+01, 1.4711087e-02],\n",
       "       [1.1500000e+02, 1.3206963e-02],\n",
       "       [1.0700000e+02, 1.1207433e-02],\n",
       "       [1.1100000e+02, 1.0872698e-02],\n",
       "       [1.1900000e+02, 1.0655009e-02],\n",
       "       [1.1600000e+02, 1.0529072e-02],\n",
       "       [9.5500000e+02, 8.3793169e-03],\n",
       "       [1.1300000e+02, 7.5303125e-03],\n",
       "       [2.1500000e+02, 6.5017096e-03],\n",
       "       [1.2200000e+02, 5.9972908e-03],\n",
       "       [1.1400000e+02, 5.5102506e-03],\n",
       "       [4.8000000e+01, 5.3096456e-03],\n",
       "       [4.1000000e+01, 5.0732223e-03],\n",
       "       [5.0000000e+01, 4.9058683e-03],\n",
       "       [4.3000000e+01, 4.7829896e-03],\n",
       "       [9.8000000e+01, 4.7386358e-03],\n",
       "       [1.1200000e+02, 4.3750950e-03],\n",
       "       [1.1700000e+02, 4.2789825e-03],\n",
       "       [2.4700000e+02, 4.2561321e-03],\n",
       "       [1.2100000e+02, 3.9752182e-03],\n",
       "       [5.7000000e+01, 3.9071329e-03],\n",
       "       [5.6000000e+01, 3.1561861e-03],\n",
       "       [1.0200000e+02, 2.9661744e-03],\n",
       "       [1.0300000e+02, 2.5561342e-03],\n",
       "       [5.3000000e+01, 2.2261152e-03],\n",
       "       [1.0500000e+02, 2.0313645e-03],\n",
       "       [5.4000000e+01, 1.8100183e-03],\n",
       "       [1.1800000e+02, 1.4839948e-03],\n",
       "       [9.9000000e+01, 1.3461252e-03],\n",
       "       [1.2000000e+02, 1.1245210e-03],\n",
       "       [9.7000000e+01, 1.1232899e-03],\n",
       "       [5.1000000e+01, 1.0419880e-03],\n",
       "       [1.0800000e+02, 9.1773330e-04],\n",
       "       [1.1000000e+02, 5.6454161e-04],\n",
       "       [1.0100000e+02, 5.4341706e-04],\n",
       "       [5.2000000e+01, 3.9379593e-04],\n",
       "       [1.0400000e+02, 3.4951992e-04]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"char_model_lamda_calculus.onnx\"\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "input_image = np.random.random(\n",
    "    (\n",
    "        1,\n",
    "        1,\n",
    "        100,  # height: any size\n",
    "        100   # width: any size\n",
    "    )\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
    "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
    "\n",
    "input_name: str = inputs[0].name\n",
    "output_name: list[str] = outputs[0].name\n",
    "softmax: np.ndarray\n",
    "softmax_ordered: np.ndarray\n",
    "logits: np.ndarray\n",
    "\n",
    "model_outputs = session.run(\n",
    "    [output_name], \n",
    "    {input_name: np.expand_dims(np.expand_dims(plus_im, 0), 0)}\n",
    ")\n",
    "\n",
    "softmax_ordered = model_outputs[0]\n",
    "probs: torch.Tensor = softmax_ordered[:, 1]\n",
    "chars: torch.Tensor = softmax_ordered[:, 0]\n",
    "\n",
    "top_3_chars: torch.Tensor = chars[:3]\n",
    "top_3_probs: torch.Tensor = probs[:3]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
