{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "from einops import rearrange\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.optim import AdamW\n",
                "from torch import nn\n",
                "from typing import Any\n",
                "import glob\n",
                "import re\n",
                "from datetime import datetime\n",
                "import random\n",
                "from collections import defaultdict\n",
                "import numpy as np\n",
                "\n",
                "import torch.nn.functional as torch_func\n",
                "from torchvision.transforms.functional import rotate, affine, resize, center_crop\n",
                "from PIL import Image\n",
                "import cv2\n",
                "\n",
                "random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "import onnxruntime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "from models.allcnn2d import AllCNN2D, AllCNN2D_Prod\n",
                "from drawing.interactive import draw_image"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Global"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path: str = os.path.abspath(\".\")\n",
                "root_path: str = os.path.join(file_path, os.pardir, os.pardir)\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\Mongo_epoch17_trainacc0.9517_valacc0.99432_Tloss0.010203_Vloss0.002475_lr0.0007224.pkl\"\n",
                "model_name: str = \"MongoCNN_Prod\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "44"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "alphabet: list[str] = list('∀∃().0123456789:λμabcdefghijklmnopqrstuvwxyz')\n",
                "len(alphabet)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "AllCNN2D                                 [1, 44]                   --\n",
                        "├─ModuleList: 1-1                        --                        --\n",
                        "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
                        "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
                        "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
                        "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
                        "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
                        "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
                        "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
                        "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
                        "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
                        "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
                        "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
                        "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
                        "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
                        "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
                        "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
                        "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
                        "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
                        "├─Sequential: 1-2                        [1, 128]                  --\n",
                        "│    └─Flatten: 2-6                      [1, 128]                  --\n",
                        "├─ModuleList: 1-3                        --                        --\n",
                        "│    └─Sequential: 2-7                   [1, 64]                   --\n",
                        "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
                        "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
                        "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
                        "│    └─Sequential: 2-8                   [1, 44]                   --\n",
                        "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
                        "==========================================================================================\n",
                        "Total params: 83,548\n",
                        "Trainable params: 83,548\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (M): 14.05\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.02\n",
                        "Forward/backward pass size (MB): 2.18\n",
                        "Params size (MB): 0.33\n",
                        "Estimated Total Size (MB): 2.53\n",
                        "==========================================================================================\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
                "        \"fully_connected_features\": (64, len(alphabet)), \n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cuda\",\n",
                "        \"conv_dropout\": 0.075,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": model_name,\n",
                "    }\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to Mongo_LC_Prod.onnx\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Create a dummy input tensor\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = f\"Mongo_LC_Prod.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    cnn_model.to(device=DEVICE),\n",
                "    dummy_input.to(device=DEVICE),\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"logits\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'cnn_model' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[1], line 81\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m percentages\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Assuming cnn_model is defined somewhere (e.g., a CNN model architecture)\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m cnn \u001b[38;5;241m=\u001b[39m CNN_ONNX_WRAPPER(\u001b[43mcnn_model\u001b[49m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Create a dummy input tensor\u001b[39;00m\n\u001b[0;32m     84\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Example input with the shape [batch_size, channels, height, width]\u001b[39;00m\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'cnn_model' is not defined"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class CNN_ONNX_WRAPPER(nn.Module):\n",
                "    alphabet: list[str] = list(\"∃().0123456789:λμabcdefghijklmnopqrstuvwxyz\")\n",
                "\n",
                "    model: nn.Module  # This should be set to your actual CNN model\n",
                "\n",
                "    def __init__(self, cnn_model: nn.Module):\n",
                "        super(CNN_ONNX_WRAPPER, self).__init__()\n",
                "        self.model = cnn_model\n",
                "\n",
                "    @staticmethod\n",
                "    def preprocess_im(im: torch.Tensor, threshold: float, image_dims: tuple, pad: int) -> torch.Tensor:\n",
                "        # Apply the thresholding\n",
                "        image = torch.where(\n",
                "            im.squeeze() > threshold, \n",
                "            torch.tensor(1.0), \n",
                "            torch.tensor(0.0)\n",
                "        )\n",
                "\n",
                "        # Optionally, flip the image based on the top-left corner pixel value\n",
                "        # Compare the value directly without using .item()\n",
                "        if image[0, 0] > 0.5:  # Tensor comparison (element-wise)\n",
                "            image = 1.0 - image\n",
                "\n",
                "        # Crop around black pixels (foreground)\n",
                "        mask = image > 0.5  # Assume the foreground is black\n",
                "        coords = torch.nonzero(mask)\n",
                "\n",
                "        if coords.shape[0] > 0:\n",
                "            # Get min and max coordinates directly as tensors, no need for .item()\n",
                "            min_coords = coords.min(dim=0)[0]  # Get the min values\n",
                "            max_coords = coords.max(dim=0)[0]  # Get the max values\n",
                "            \n",
                "            # Use tensor indexing for cropping\n",
                "            x_min, y_min = min_coords[0], min_coords[1]\n",
                "            x_max, y_max = max_coords[0], max_coords[1]\n",
                "            \n",
                "            # Crop the image based on coordinates\n",
                "            image = image[x_min:x_max + 1, y_min:y_max + 1]\n",
                "\n",
                "        # Resize the image using interpolate (ONNX-compatible)\n",
                "        image = F.interpolate(image.unsqueeze(0).unsqueeze(0), size=[d - pad * 2 for d in image_dims])\n",
                "\n",
                "        # Apply threshold again after resizing\n",
                "        image = (image > threshold).to(torch.float32)\n",
                "\n",
                "        # Pad the image\n",
                "        image = F.pad(image, (pad, pad, pad, pad), value=0.0)\n",
                "\n",
                "        return image\n",
                "\n",
                "    @staticmethod\n",
                "    def softmax(x: torch.Tensor) -> torch.Tensor:\n",
                "        # Softmax on the logits\n",
                "        return torch.nn.functional.softmax(x, dim=1)\n",
                "\n",
                "    def forward(self, im: torch.Tensor) -> torch.Tensor:\n",
                "        # Preprocess the input image\n",
                "        im = self.preprocess_im(im, 0.5, (64, 64), 1)\n",
                "\n",
                "        # Debugging step: Check tensor shape before model inference\n",
                "        print(f\"Processed image shape: {im.shape}\")\n",
                "\n",
                "        # Forward pass through the model\n",
                "        logits = self.model(im)  # Add batch dimension and pass through the model\n",
                "\n",
                "        # Debugging step: Check the shape of logits\n",
                "        print(f\"Logits shape: {logits.shape}\")\n",
                "\n",
                "        # Apply softmax to get the percentages\n",
                "        percentages = self.softmax(logits)\n",
                "\n",
                "        return percentages\n",
                "\n",
                "\n",
                "# Assuming cnn_model is defined somewhere (e.g., a CNN model architecture)\n",
                "\n",
                "cnn = CNN_ONNX_WRAPPER(cnn_model)\n",
                "\n",
                "# Create a dummy input tensor\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input with the shape [batch_size, channels, height, width]\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"Lake_FINAL_Prod.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    cnn.eval(),\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"prob\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}, \"prob\": {0: \"batch_size\"}},\n",
                "    opset_version=12  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load Onnx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 132,
            "metadata": {},
            "outputs": [],
            "source": [
                "import onnxruntime\n",
                "import numpy as np\n",
                "import os\n",
                "from PIL import Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 133,
            "metadata": {},
            "outputs": [],
            "source": [
                "onnx_file_path = f\"Lake_FINAL_Prod.onnx\"\n",
                "\n",
                "# Load the ONNX model\n",
                "model_path = onnx_file_path\n",
                "\n",
                "# Define the execution providers without TensorRT\n",
                "providers = ['CPUExecutionProvider']\n",
                "\n",
                "# Create the InferenceSession with the explicit providers\n",
                "session = onnxruntime.InferenceSession(model_path, providers=providers)\n",
                "\n",
                "# Prepare the input image\n",
                "input_image = np.random.random(\n",
                "    (\n",
                "        1,  # batch: stack as many images as you like here\n",
                "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
                "        64, # height: fixed to 64 pixels for now\n",
                "        64  # width: fixed to 64 pixels for now\n",
                "    )\n",
                ").astype(np.float32)\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 134,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processed image shape: torch.Size([1, 1, 64, 64])\n",
                        "Logits shape: torch.Size([1, 43])\n",
                        "Top indices shape: torch.Size([1, 43])\n"
                    ]
                }
            ],
            "source": [
                "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
                "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
                "\n",
                "input_name: list[str] = inputs[0].name\n",
                "output_names: list[str] = [out.name for out in outputs]\n",
                "\n",
                "test_im_path: str = r\"test_im.png\"\n",
                "test_im: Image.Image = Image.open(test_im_path).convert(\"L\")\n",
                "test_im_np: np.ndarray = np.asarray(test_im).astype(np.float32)\n",
                "\n",
                "c_prob = cnn.eval().forward(torch.tensor(test_im_np))\n",
                "prob = session.run(\n",
                "    output_names, \n",
                "    {input_name: np.expand_dims(np.expand_dims(test_im_np, 0), 0)}\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 135,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(tensor([[9.9999e-01, 5.2548e-06, 1.7248e-06, 8.1988e-07, 7.9782e-07, 5.8198e-07,\n",
                            "           3.0758e-07, 2.0259e-07, 1.6081e-07, 1.2946e-07, 2.3220e-08, 2.1835e-08,\n",
                            "           1.7244e-08, 1.7012e-08, 1.5099e-08, 1.0663e-08, 1.0223e-08, 2.6446e-09,\n",
                            "           2.5364e-09, 2.4631e-09, 2.3556e-09, 1.3106e-09, 9.8690e-10, 8.5152e-10,\n",
                            "           1.5691e-10, 8.2225e-11, 5.7212e-11, 4.9990e-11, 3.8997e-11, 1.2193e-11,\n",
                            "           2.4857e-12, 9.5809e-13, 9.4636e-13, 5.4760e-13, 1.8145e-13, 1.3831e-13,\n",
                            "           1.3262e-13, 3.5077e-14, 2.8431e-16, 1.8610e-16, 1.0641e-16, 6.3491e-17,\n",
                            "           1.4506e-17]], device='cuda:0', grad_fn=<GatherBackward0>),\n",
                            "  array([[9.9998999e-01, 5.2570049e-06, 1.7245728e-06, 8.1932870e-07,\n",
                            "          7.9734730e-07, 5.8144116e-07, 3.0757428e-07, 2.0254525e-07,\n",
                            "          1.6080422e-07, 1.2947180e-07, 2.3234454e-08, 2.1837661e-08,\n",
                            "          1.7260124e-08, 1.7012930e-08, 1.5071729e-08, 1.0674785e-08,\n",
                            "          1.0232568e-08, 2.6464866e-09, 2.5363978e-09, 2.4632330e-09,\n",
                            "          2.3562956e-09, 1.3109808e-09, 9.8771524e-10, 8.5169272e-10,\n",
                            "          1.5688881e-10, 8.2367668e-11, 5.7273134e-11, 5.0072991e-11,\n",
                            "          3.9002836e-11, 1.2206091e-11, 2.4869085e-12, 9.5866425e-13,\n",
                            "          9.4728956e-13, 5.4838718e-13, 1.8144352e-13, 1.3844312e-13,\n",
                            "          1.3272616e-13, 3.5106602e-14, 2.8464672e-16, 1.8622565e-16,\n",
                            "          1.0655152e-16, 6.3578961e-17, 1.4504423e-17]], dtype=float32)),\n",
                            " (tensor([ 956,  955,  120,  107,  121,  117,  100,   97,  108,  116,  101,  104,\n",
                            "            49,  119,   52,  122,  112,  114,   40,  118,   98,   58,  113,  102,\n",
                            "           105,  110,   55,  109,   54,   50,   57,   51, 8707,   48,   53,   56,\n",
                            "           103,   99,   46,  106,  111,   41,  115]),\n",
                            "  array([  56,  100,  116,  102,  103,  120,   98,  113,  112,  101,  109,\n",
                            "          122,   97,   54,  956,   57,  121,  107,   51, 8707,  117,   52,\n",
                            "          118,   53,  119,  108,   48,  111,  115,  104,   50,  955,   49,\n",
                            "           58,  110,   55,  105,  106,  114,   99,   40,   41,   46],\n",
                            "        dtype=int64))]"
                        ]
                    },
                    "execution_count": 135,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "list(zip([x for x in c_prob], [x for x in prob]))\n",
                "# Run inference\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from torchvision.transforms import functional as F\n",
                "\n",
                "def preprocess_im(im: str, threshold: float, image_dims: tuple, pad: int) -> np.ndarray:\n",
                "    # Load the image\n",
                "    image = Image.open(im)\n",
                "    \n",
                "    # Convert to grayscale (if not already)\n",
                "    image = image.convert(\"L\")\n",
                "    \n",
                "    # Convert the image to a tensor\n",
                "    image = F.to_tensor(image).squeeze()\n",
                "    \n",
                "    # Thresholding the image\n",
                "    image = torch.where(image > threshold, torch.tensor(1.0), torch.tensor(0.0))\n",
                "\n",
                "    \n",
                "    if image[0, 0] > 0.5:\n",
                "        image = 1.0 - image\n",
                "    \n",
                "    # Crop the image around the black pixels (foreground)\n",
                "    mask = image > 0.5  # Assuming foreground is black (0) and background is white (1)\n",
                "    coords = torch.nonzero(mask)\n",
                "    \n",
                "    if coords.shape[0] > 0:\n",
                "        # Get min and max coordinates\n",
                "        min_coords = coords.min(dim=0)[0]  # Getting the min values\n",
                "        max_coords = coords.max(dim=0)[0]  # Getting the max values\n",
                "        \n",
                "        x_min, y_min = min_coords[0].item(), min_coords[1].item()  # Convert tensor to scalar\n",
                "        x_max, y_max = max_coords[0].item(), max_coords[1].item()  # Convert tensor to scalar\n",
                "        \n",
                "        # Crop the image based on coordinates\n",
                "        image = image[x_min:x_max + 1, y_min:y_max + 1]\n",
                "    \n",
                "    # Resize to the desired dimensions\n",
                "    image = F.resize(image.unsqueeze(0), [d - pad * 2 for d in image_dims])\n",
                "    \n",
                "    # Threshold again after resize\n",
                "    image = (image > threshold).type(torch.uint8).type(torch.float32)\n",
                "\n",
                "    # Pad the image\n",
                "    image = F.pad(image, (pad, pad, pad, pad), fill=0.0)\n",
                "\n",
                "    # Convert to NumPy array\n",
                "    image = image.squeeze().cpu().numpy()  # Remove the batch dimension and convert to NumPy\n",
                "\n",
                "    return image\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJoAAACPCAAAAAAnj53jAAAADmVYSWZNTQAqAAAACAAAAAAAAADSU5MAAAI7SURBVHic7ZxbbsUgDAVx1f1vmX7ch4IKAR/8IJbno2pFWkbHEG6aBKrlVH68BcakGkKqIaQaQqohpBpCqiGkGkKqIaQaQqohpBpCqiGkGkKqIfxu/C6Vovl/po3USM6iy2ZBNfUOHmuphrAzQ4fITF2N1ITmhlZBBfQ21Q495Y6QOtcdPENxNe11KmZq6uypqd57k09NbAzCauqzIO5YUyWqmu7NcVRNfxaELagu8dQMhlrA1Eop2ucOhdTEhDG1+VA79DpUiKBq/bLSTRsLSM3irKZQUDlttbG2ryitJlhrRI3+fdM21dpvZIKm1j+l0rtJZEWAC9rrnZqG3dgAtUuXdO3++oNESYmdPX367vdc26P4f/8LWNBaJgPq1UjUBssCU6uXr21DbQ/ZQfI6tNY6buTDHgvtLJwfiitCtzQWe9vMLdDnNSrqlysf4qRm8yHyBZCa1XsdcQpqCU/NcH4GSs2UKGqmQy1MarZw1CxXqcJPzfDtQ4aacWjc1Cxf2VxXsw4tygy1ree6mnk9wxTU9pXqVTX7erJSM34PfVHNIbQg08B6X4E1NY96MlIz34xhSc0ltPXU7HewWFHzCW05NYdtPxbUnEJbTc1jr5S5mldoczXGfQJhHryG+oU2U3M0m6h5mj12rLmGNk/Nb2OqOzW3k+2LGzVns/Gt2reY40Zjo9T8zUZqB5gN1E4w64+1zUcNhBjPUG+zrprvIvDlYWvo9FkmGzYe4tLmYQU9hFRDSDWEg9X+APAMRyZZK8mFAAAAAElFTkSuQmCC",
                        "text/plain": [
                            "<PIL.Image.Image image mode=L size=154x143>"
                        ]
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_im_path: str = r\"test_im.png\"\n",
                "test_im: Image.Image = Image.open(test_im_path).convert(\"L\")\n",
                "test_im"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "test_im_np = preprocess_im(test_im_path, 0.1, (64, 64), 1)\n",
                "\n",
                "plt.imshow(test_im_np)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(array([9.94052410e-01, 1.23858731e-03, 9.81083489e-04, 6.86358311e-04,\n",
                            "        6.23539032e-04, 6.17766578e-04, 6.12980337e-04, 4.79297625e-04,\n",
                            "        1.40747419e-04, 1.03887825e-04, 1.03488579e-04, 1.00947203e-04,\n",
                            "        9.20660677e-05, 8.27235781e-05, 1.91916297e-05, 1.06729467e-05,\n",
                            "        9.47296121e-06, 8.12046710e-06, 7.98343717e-06, 7.91991897e-06,\n",
                            "        4.30528098e-06, 3.55534780e-06, 3.19691048e-06, 2.17775141e-06,\n",
                            "        1.96297742e-06, 1.89501952e-06, 1.31547233e-06, 6.72157057e-07,\n",
                            "        6.70021336e-07, 4.55316723e-07, 1.70466848e-07, 1.66958785e-07,\n",
                            "        6.93960942e-08, 3.81598682e-08, 3.45308315e-08, 2.14136531e-09,\n",
                            "        1.18820531e-09, 9.17822873e-10, 1.36637798e-10, 5.52658717e-11,\n",
                            "        5.39423055e-11, 2.60328044e-11, 1.79959415e-11], dtype=float32),\n",
                            " array([ 956,  101,  116,  107,  122,   98,  108,  120,  104,   49,  102,\n",
                            "         955,  112,  121,  100,   97,  117,   58,  113,  114,   40,  110,\n",
                            "         118,  105,   54,  119,   52,   50,   55,  109,   48,   56,   53,\n",
                            "        8707,   51,   57,  103,   99,  111,  106,   46,  115,   41]))"
                        ]
                    },
                    "execution_count": 87,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(64, 64)"
                        ]
                    },
                    "execution_count": 58,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_im_np.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "softmax: np.ndarray\n",
                "softmax_ordered: np.ndarray\n",
                "logits: np.ndarray\n",
                "\n",
                "logits = session.run(\n",
                "    output_names, \n",
                "    {input_name: np.expand_dims(np.expand_dims(test_im_np, 0), 0)}\n",
                ")[0]\n",
                "\n",
                "# logits.shape is shape (batch, character) for all character labels\n",
                "# softmax.shape is shape (batch, character) for all character labels\n",
                "# softmax_ordered is shape (batch, character, [label index, label prob, unicode character value])\n",
                "\n",
                "# character dim is 44 (there are 44 character labels)\n",
                "# label index is from 0 to 44 (corresponding to each ordered label index)\n",
                "# label prob is a softmaxed probability for this label prediction\n",
                "# unicode character value is the unicode character for this prediction\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "43"
                        ]
                    },
                    "execution_count": 61,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'μ'"
                        ]
                    },
                    "execution_count": 62,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "index = np.argmax(softmax(logits.squeeze()))\n",
                "alphabet[index]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[5.30846650e-04, 1.68099464e-03, 1.37032261e-02, 1.18300821e-04,\n",
                            "        5.17544569e-03, 1.18788116e-01, 7.00770020e-02, 2.72251527e-05,\n",
                            "        5.65034361e-06, 2.70747951e-05, 7.02182706e-06, 1.98335965e-05,\n",
                            "        2.47772950e-05, 2.26152275e-04, 2.94749648e-03, 5.39878034e-04,\n",
                            "        1.65653706e-01, 1.00289382e-01, 2.63870694e-03, 1.34053471e-05,\n",
                            "        6.00268249e-04, 2.87390407e-03, 3.22919041e-05, 2.28433162e-02,\n",
                            "        7.75476918e-04, 1.12298233e-06, 1.69513543e-04, 2.67672760e-07,\n",
                            "        3.25475936e-03, 7.96237418e-06, 1.93356220e-02, 5.77072799e-03,\n",
                            "        3.84508690e-04, 9.18507576e-03, 4.11047113e-05, 5.12989820e-04,\n",
                            "        1.17080397e-07, 9.30333436e-02, 1.01616504e-02, 3.10798088e-04,\n",
                            "        8.07399279e-04, 3.47387671e-01, 1.58225430e-05]], dtype=float32)"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "\n",
                "softmax(logits)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'softmax_ordered' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m top_character_probs: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_ordered\u001b[49m[:, :, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m top_characters: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     [\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mchr\u001b[39m(\u001b[38;5;28mint\u001b[39m(softmax_ordered[batch_i, i, \u001b[38;5;241m2\u001b[39m])) \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mrange\u001b[39m(softmax_ordered\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      9\u001b[0m ]\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'softmax_ordered' is not defined"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "top_character_probs: list[list[float]] = softmax_ordered[:, :, 1].tolist()\n",
                "\n",
                "top_characters: list[list[str]] = [\n",
                "    [\n",
                "        chr(int(softmax_ordered[batch_i, i, 2])) \n",
                "        for i in range(softmax_ordered.shape[1])\n",
                "    ] for batch_i in \n",
                "    range(softmax_ordered.shape[0])\n",
                "]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hello World"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextReShitter(nn.Module):\n",
                "    \n",
                "    def __init__(self, alphabet: list[str], text: str):\n",
                "        \n",
                "        super(TextReShitter, self).__init__()\n",
                "        \n",
                "        self.alphabet: list[str] = alphabet\n",
                "        self.text: str = text\n",
                "        \n",
                "        self.logits: list[torch.Tensor] = []\n",
                "        \n",
                "        for c in self.text:\n",
                "            c_index: int = self.alphabet.index(c)\n",
                "            \n",
                "            char_logits: torch.Tensor = torch.rand((len(alphabet),))\n",
                "            char_logits[c_index] += 2\n",
                "            char_logits *= (torch.rand(1)+0.1)\n",
                "\n",
                "            self.logits.append(char_logits)\n",
                "\n",
                "        self.logits_tensor: torch.Tensor = torch.stack(self.logits, dim=0)\n",
                "        \n",
                "        self.logits_tensor = self.logits_tensor.unsqueeze(0) # batch, chars, logit\n",
                "        \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \n",
                "        self.logits_tensor = self.logits_tensor + x.sum() * 0.0  \n",
                "\n",
                "        softmax = torch.softmax(self.logits_tensor, dim=-1)\n",
                "        \n",
                "        # Create indices for the first dimension (char_prob_i)\n",
                "        char_prob_indices = torch.arange(softmax.size(-1), device=softmax.device).reshape(1, 1, -1, 1)\n",
                "        \n",
                "        # Create indices for the third dimension (ord(self.alphabet[char_prob_i]))\n",
                "        alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n",
                "        \n",
                "        # Expand dimensions to match softmax shape\n",
                "        char_prob_indices = char_prob_indices.expand(*softmax.shape, 1)\n",
                "        alphabet_indices = alphabet_indices.expand(*softmax.shape, 1)\n",
                "        \n",
                "        # Concatenate along the last dimension\n",
                "        softmax_ordered = torch.cat([char_prob_indices, softmax.unsqueeze(-1), alphabet_indices], dim=-1)\n",
                "        \n",
                "        # Sort along the probability dimension (dim=-2)\n",
                "        sorting_indices = softmax_ordered[..., 1].argsort(dim=-1, descending=True)\n",
                "        sorted_tensor = torch.gather(softmax_ordered, -2, sorting_indices.unsqueeze(-1).expand(-1, -1, -1, 3))\n",
                "        \n",
                "        x = x * 0.5\n",
                "        \n",
                "        unicodes: torch.Tensor = sorted_tensor[0, :, :, 2]\n",
                "        probs: torch.Tensor = sorted_tensor[0, :, :, 1]\n",
                "        \n",
                "        unicodes = unicodes.to(dtype=torch.int)\n",
                "        \n",
                "        return unicodes, probs \n",
                "        \n",
                "        \n",
                "        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[104,  54,  57,  98, 121, 113, 111, 105,  50, 118,  55, 119,  51,  43,\n",
                            "          116,  40, 108, 117, 247,  97, 215, 115, 122,  99,  52,  45, 110,  53,\n",
                            "           41, 112,  48,  46, 103, 100,  49, 109,  56, 120, 955, 106, 114, 107,\n",
                            "          101, 102],\n",
                            "         [101, 103,  57, 955,  53,  97, 100, 215,  51, 114, 119,  40, 109,  48,\n",
                            "          108,  54,  46,  50, 102, 121, 247, 120, 116, 115, 122,  99,  55, 104,\n",
                            "           98,  41, 107, 110, 106, 113, 118, 105,  43, 117,  45, 111,  52, 112,\n",
                            "           56,  49],\n",
                            "         [108,  51,  50, 103, 105, 100, 113, 116, 247, 109, 119,  99,  45, 104,\n",
                            "          121, 118,  46,  43,  98,  55, 120,  52, 955,  54,  41,  49,  57, 115,\n",
                            "          102,  48,  53, 112, 101,  40, 114, 111, 215, 122,  56, 106,  97, 117,\n",
                            "          107, 110],\n",
                            "         [108,  45, 116,  99,  54,  48, 102, 121, 106,  51,  98,  56,  46, 215,\n",
                            "          115, 247, 101,  40,  52,  49,  57, 104, 111, 120, 955,  55, 110, 122,\n",
                            "          119, 113, 100,  41, 109,  50,  97, 117,  43, 118, 103, 114, 107, 105,\n",
                            "           53, 112],\n",
                            "         [111, 101,  57,  48,  99, 118, 112, 215,  49, 104, 114, 122, 105,  98,\n",
                            "           46, 103, 110, 120, 115,  97,  54,  45,  53,  51, 116,  40,  50, 108,\n",
                            "           56, 117, 113, 109, 119,  43, 121, 107, 100,  41,  52,  55, 955, 102,\n",
                            "          247, 106],\n",
                            "         [ 46, 112, 117, 106, 111, 115, 247, 119,  54, 103, 109, 113, 107, 101,\n",
                            "           97, 122, 110, 121, 114,  43,  40, 116, 118, 105,  49, 215, 100,  57,\n",
                            "           48,  51,  52,  56,  55,  41, 104, 955,  53,  98, 108,  99,  45, 120,\n",
                            "          102,  50],\n",
                            "         [119, 112,  55,  45,  51, 111,  54,  98, 102,  52, 247,  57, 955,  40,\n",
                            "          118, 115, 100,  97, 103, 101, 110, 107, 117, 106, 121, 109,  41,  53,\n",
                            "          120, 105,  49,  46,  56, 114, 104,  48, 108,  43, 215, 122, 116,  99,\n",
                            "          113,  50],\n",
                            "         [111,  55, 107, 120, 100,  98,  57,  41, 113,  51,  99, 103,  50, 108,\n",
                            "          105, 116,  48, 215, 112, 106,  43,  56, 122,  46,  52, 119,  40, 109,\n",
                            "           45, 102,  97, 104, 101, 118,  49, 110, 114,  53, 247, 121,  54, 115,\n",
                            "          955, 117],\n",
                            "         [114,  56, 104,  97, 103,  55, 116,  41, 105, 215,  49,  48, 101, 118,\n",
                            "           57, 107, 119, 109, 247, 955,  40, 111,  52,  98, 108,  51,  54, 120,\n",
                            "          112, 115,  53,  45, 117, 121, 100,  46, 102,  99,  43,  50, 106, 110,\n",
                            "          122, 113],\n",
                            "         [108,  54,  43, 118, 115,  51, 102,  41,  55, 117,  98, 101, 122, 215,\n",
                            "          119, 100,  56, 116, 107,  99,  48, 109,  49, 106,  45, 121,  46, 105,\n",
                            "           53,  52,  97, 112, 955, 104, 113, 111, 114,  50, 247,  40, 103, 120,\n",
                            "           57, 110],\n",
                            "         [100,  46, 103, 105, 215, 117, 110, 104, 113, 119, 118, 247,  54,  48,\n",
                            "          121, 112, 122, 114,  49, 106,  57,  56, 120,  98, 102, 955,  52, 101,\n",
                            "          108, 107,  40, 115, 109,  43,  51,  99,  50, 111,  55,  97, 116,  45,\n",
                            "           53,  41]], dtype=torch.int32),\n",
                            " tensor([[0.1105, 0.0316, 0.0312, 0.0310, 0.0306, 0.0294, 0.0279, 0.0268, 0.0261,\n",
                            "          0.0260, 0.0260, 0.0257, 0.0255, 0.0242, 0.0237, 0.0235, 0.0226, 0.0226,\n",
                            "          0.0222, 0.0219, 0.0219, 0.0214, 0.0208, 0.0202, 0.0198, 0.0196, 0.0191,\n",
                            "          0.0187, 0.0187, 0.0170, 0.0165, 0.0162, 0.0154, 0.0151, 0.0150, 0.0145,\n",
                            "          0.0142, 0.0142, 0.0136, 0.0125, 0.0122, 0.0114, 0.0114, 0.0113],\n",
                            "         [0.0369, 0.0249, 0.0248, 0.0248, 0.0248, 0.0248, 0.0246, 0.0245, 0.0243,\n",
                            "          0.0243, 0.0241, 0.0240, 0.0239, 0.0239, 0.0238, 0.0238, 0.0236, 0.0233,\n",
                            "          0.0231, 0.0231, 0.0229, 0.0229, 0.0227, 0.0226, 0.0224, 0.0224, 0.0223,\n",
                            "          0.0219, 0.0219, 0.0217, 0.0216, 0.0212, 0.0212, 0.0210, 0.0208, 0.0201,\n",
                            "          0.0201, 0.0199, 0.0196, 0.0193, 0.0193, 0.0192, 0.0190, 0.0190],\n",
                            "         [0.0446, 0.0253, 0.0252, 0.0250, 0.0250, 0.0248, 0.0247, 0.0247, 0.0240,\n",
                            "          0.0240, 0.0240, 0.0239, 0.0237, 0.0235, 0.0235, 0.0234, 0.0232, 0.0229,\n",
                            "          0.0227, 0.0227, 0.0223, 0.0221, 0.0219, 0.0219, 0.0216, 0.0215, 0.0214,\n",
                            "          0.0211, 0.0210, 0.0210, 0.0208, 0.0206, 0.0206, 0.0205, 0.0205, 0.0205,\n",
                            "          0.0203, 0.0203, 0.0202, 0.0202, 0.0201, 0.0199, 0.0196, 0.0194],\n",
                            "         [0.0365, 0.0246, 0.0246, 0.0244, 0.0243, 0.0242, 0.0241, 0.0241, 0.0240,\n",
                            "          0.0239, 0.0239, 0.0237, 0.0237, 0.0234, 0.0234, 0.0234, 0.0230, 0.0228,\n",
                            "          0.0227, 0.0226, 0.0225, 0.0225, 0.0225, 0.0222, 0.0221, 0.0220, 0.0220,\n",
                            "          0.0220, 0.0219, 0.0218, 0.0217, 0.0215, 0.0213, 0.0212, 0.0210, 0.0208,\n",
                            "          0.0208, 0.0208, 0.0206, 0.0205, 0.0204, 0.0203, 0.0203, 0.0202],\n",
                            "         [0.1248, 0.0297, 0.0292, 0.0291, 0.0290, 0.0290, 0.0286, 0.0275, 0.0272,\n",
                            "          0.0262, 0.0250, 0.0244, 0.0243, 0.0241, 0.0234, 0.0230, 0.0226, 0.0218,\n",
                            "          0.0217, 0.0214, 0.0207, 0.0202, 0.0202, 0.0192, 0.0190, 0.0188, 0.0184,\n",
                            "          0.0184, 0.0182, 0.0176, 0.0174, 0.0168, 0.0154, 0.0148, 0.0144, 0.0142,\n",
                            "          0.0135, 0.0133, 0.0133, 0.0132, 0.0132, 0.0130, 0.0129, 0.0121],\n",
                            "         [0.0831, 0.0307, 0.0302, 0.0302, 0.0297, 0.0293, 0.0292, 0.0292, 0.0285,\n",
                            "          0.0281, 0.0271, 0.0266, 0.0250, 0.0245, 0.0234, 0.0233, 0.0230, 0.0228,\n",
                            "          0.0226, 0.0214, 0.0211, 0.0208, 0.0207, 0.0204, 0.0200, 0.0194, 0.0189,\n",
                            "          0.0182, 0.0177, 0.0177, 0.0177, 0.0168, 0.0166, 0.0164, 0.0163, 0.0160,\n",
                            "          0.0156, 0.0155, 0.0150, 0.0147, 0.0142, 0.0141, 0.0141, 0.0140],\n",
                            "         [0.0618, 0.0272, 0.0269, 0.0265, 0.0264, 0.0263, 0.0259, 0.0255, 0.0254,\n",
                            "          0.0251, 0.0251, 0.0249, 0.0246, 0.0242, 0.0237, 0.0234, 0.0233, 0.0232,\n",
                            "          0.0227, 0.0227, 0.0224, 0.0223, 0.0221, 0.0203, 0.0202, 0.0200, 0.0199,\n",
                            "          0.0199, 0.0199, 0.0196, 0.0194, 0.0193, 0.0193, 0.0191, 0.0190, 0.0189,\n",
                            "          0.0183, 0.0181, 0.0181, 0.0180, 0.0178, 0.0178, 0.0178, 0.0178],\n",
                            "         [0.0979, 0.0313, 0.0303, 0.0293, 0.0291, 0.0289, 0.0271, 0.0265, 0.0259,\n",
                            "          0.0257, 0.0252, 0.0244, 0.0241, 0.0236, 0.0231, 0.0230, 0.0216, 0.0215,\n",
                            "          0.0213, 0.0207, 0.0207, 0.0205, 0.0204, 0.0203, 0.0198, 0.0195, 0.0189,\n",
                            "          0.0188, 0.0184, 0.0183, 0.0180, 0.0180, 0.0169, 0.0168, 0.0164, 0.0163,\n",
                            "          0.0161, 0.0157, 0.0156, 0.0156, 0.0151, 0.0147, 0.0143, 0.0142],\n",
                            "         [0.0317, 0.0248, 0.0247, 0.0247, 0.0246, 0.0245, 0.0241, 0.0241, 0.0239,\n",
                            "          0.0238, 0.0236, 0.0236, 0.0233, 0.0233, 0.0233, 0.0232, 0.0232, 0.0228,\n",
                            "          0.0227, 0.0226, 0.0225, 0.0225, 0.0224, 0.0224, 0.0224, 0.0220, 0.0219,\n",
                            "          0.0219, 0.0218, 0.0218, 0.0218, 0.0216, 0.0215, 0.0214, 0.0212, 0.0212,\n",
                            "          0.0212, 0.0212, 0.0211, 0.0210, 0.0209, 0.0207, 0.0206, 0.0205],\n",
                            "         [0.0449, 0.0259, 0.0256, 0.0250, 0.0250, 0.0250, 0.0250, 0.0248, 0.0246,\n",
                            "          0.0245, 0.0243, 0.0238, 0.0237, 0.0236, 0.0233, 0.0233, 0.0232, 0.0231,\n",
                            "          0.0230, 0.0230, 0.0230, 0.0226, 0.0220, 0.0220, 0.0218, 0.0218, 0.0217,\n",
                            "          0.0213, 0.0212, 0.0209, 0.0207, 0.0207, 0.0203, 0.0202, 0.0201, 0.0200,\n",
                            "          0.0200, 0.0197, 0.0195, 0.0193, 0.0193, 0.0192, 0.0191, 0.0190],\n",
                            "         [0.0325, 0.0244, 0.0240, 0.0239, 0.0239, 0.0237, 0.0236, 0.0236, 0.0236,\n",
                            "          0.0236, 0.0233, 0.0233, 0.0230, 0.0229, 0.0229, 0.0228, 0.0228, 0.0228,\n",
                            "          0.0228, 0.0228, 0.0228, 0.0227, 0.0227, 0.0227, 0.0226, 0.0226, 0.0226,\n",
                            "          0.0225, 0.0223, 0.0223, 0.0223, 0.0222, 0.0221, 0.0215, 0.0214, 0.0214,\n",
                            "          0.0212, 0.0211, 0.0211, 0.0209, 0.0208, 0.0207, 0.0206, 0.0204]]))"
                        ]
                    },
                    "execution_count": 133,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "hello_world_model = TextReShitter(alphabet, \"hello.world\")\n",
                "\n",
                "hello_world_model.forward(torch.zeros(1, 1, 1, 1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ONNX: Save Hello World"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to hello_world.onnx\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_1824\\427549207.py:35: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n"
                    ]
                }
            ],
            "source": [
                "# Create a dummy input tensor\n",
                "dummy_input = torch.randn(1, 1, 100, 100)  # Example input\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"hello_world.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    hello_world_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load Hello World Onnx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['h', 'e', 'l', 'l', 'o', '.', 'w', 'o', 'r', 'l', 'd']"
                        ]
                    },
                    "execution_count": 135,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "\n",
                "# Load the ONNX model\n",
                "model_path = \"hello_world.onnx\"\n",
                "session = onnxruntime.InferenceSession(model_path)\n",
                "\n",
                "input_image = np.random.random(\n",
                "    (\n",
                "        1,  # batch: stack as many images as you like here\n",
                "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
                "        2,  # height: fixed to 1 for now\n",
                "        1   # width: fixed to 1 for now\n",
                "    )\n",
                ").astype(np.float32)\n",
                "\n",
                "# Run inference\n",
                "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
                "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
                "\n",
                "input_name: str = inputs[0].name\n",
                "output_names: list[str] = [out.name for out in outputs]\n",
                "softmax: np.ndarray\n",
                "softmax_ordered: np.ndarray\n",
                "logits: np.ndarray\n",
                "\n",
                "unicodes, probs = session.run(\n",
                "    output_names, \n",
                "    {input_name: input_image}\n",
                ")\n",
                "\n",
                "list(map(chr, unicodes[:, 0].tolist()))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create CNN Production"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "AllCNN2D                                 [1, 44]                   --\n",
                        "├─ModuleList: 1-1                        --                        --\n",
                        "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
                        "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
                        "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
                        "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
                        "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
                        "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
                        "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
                        "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
                        "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
                        "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
                        "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
                        "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
                        "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
                        "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
                        "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
                        "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
                        "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
                        "├─Sequential: 1-2                        [1, 128]                  --\n",
                        "│    └─Flatten: 2-6                      [1, 128]                  --\n",
                        "├─ModuleList: 1-3                        --                        --\n",
                        "│    └─Sequential: 2-7                   [1, 64]                   --\n",
                        "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
                        "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
                        "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
                        "│    └─Sequential: 2-8                   [1, 44]                   --\n",
                        "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
                        "==========================================================================================\n",
                        "Total params: 83,548\n",
                        "Trainable params: 83,548\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (M): 14.05\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.02\n",
                        "Forward/backward pass size (MB): 2.18\n",
                        "Params size (MB): 0.33\n",
                        "Estimated Total Size (MB): 2.53\n",
                        "==========================================================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_28708\\77278072.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, 0, 0], device=im.device)  # Default value if no foreground\n",
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_28708\\77278072.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, im.size(2) - 1, im.size(3) - 1], device=im.device)  # Default value if no foreground\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:657: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:1127: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to CNN_SARA_LC.onnx\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "class KrudModel(nn.Module):\n",
                "    \n",
                "    def __init__(\n",
                "        self, \n",
                "        alphabet: list[str], \n",
                "        target_width: int = 64, \n",
                "        target_height: int = 64, \n",
                "        padding: int = 1,\n",
                "        model: torch.nn.Module = None\n",
                "    ):\n",
                "        \n",
                "        super(KrudModel, self).__init__()\n",
                "        \n",
                "        self.alphabet: list[str] = alphabet\n",
                "        self.alphabet_float: torch.Tensor = torch.tensor([ord(a) for a in self.alphabet], dtype=torch.float32) \n",
                "        self.target_width: int = target_width\n",
                "        self.target_height: int = target_height\n",
                "        self.padding: int = padding\n",
                "        self._unpadded_dims: tuple[int] = (\n",
                "            self.target_height-self.padding*2,\n",
                "            self.target_width-self.padding*2\n",
                "        )\n",
                "        self.model: torch.nn.Module = model.eval()\n",
                "    \n",
                "        \n",
                "    def preprocess_image(\n",
                "        self, \n",
                "        im: torch.Tensor\n",
                "    ) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Preprocesses the input image by cropping, resizing, binarizing, and padding it.\n",
                "\n",
                "        Args:\n",
                "            im (torch.Tensor): Input image tensor of shape (C, H, W) or (B, C, H, W).\n",
                "\n",
                "        Returns:\n",
                "            torch.Tensor: Preprocessed image tensor.\n",
                "        \"\"\"\n",
                "        # Step 1: Binarize the image (foreground = 0, background = 1)\n",
                "        mask = im > 0.5\n",
                "\n",
                "        # Step 2: Find bounding box coordinates of the foreground\n",
                "        coords = torch.nonzero(mask)  # Get coordinates of non-zero (foreground) pixels\n",
                "\n",
                "        # Step 3: Check if there are any foreground pixels (traceable)\n",
                "        has_foreground = torch.any(mask)\n",
                "\n",
                "        # Step 4: Compute min and max coordinates for cropping (if foreground exists)\n",
                "        min_coords = torch.where(\n",
                "            has_foreground,\n",
                "            coords.min(dim=0)[0],\n",
                "            torch.tensor([0, 0, 0, 0], device=im.device)  # Default value if no foreground\n",
                "        )\n",
                "        max_coords = torch.where(\n",
                "            has_foreground,\n",
                "            coords.max(dim=0)[0],\n",
                "            torch.tensor([0, 0, im.size(2) - 1, im.size(3) - 1], device=im.device)  # Default value if no foreground\n",
                "        )\n",
                "\n",
                "        # Step 5: Extract min and max coordinates for height and width (assuming input is 4D: B, C, H, W)\n",
                "        min_x, min_y = min_coords[2], min_coords[3]\n",
                "        max_x, max_y = max_coords[2], max_coords[3]\n",
                "\n",
                "        # Step 6: Crop the image (use slicing, which is traceable)\n",
                "        im = im[:, :, min_x:max_x + 1, min_y:max_y + 1]\n",
                "\n",
                "        # Step 7: Resize the image to the desired dimensions\n",
                "        im = torch_func.interpolate(\n",
                "            im, \n",
                "            size=self._unpadded_dims, \n",
                "            mode=\"bilinear\", \n",
                "            align_corners=False\n",
                "        )\n",
                "\n",
                "        # Step 8: Binarize the image again (optional, depending on your use case)\n",
                "        im = (im > 0.5).type(torch.uint8).type(torch.float32)\n",
                "\n",
                "        # Step 9: Pad the image\n",
                "        im = torch_func.pad(\n",
                "            im,\n",
                "            (self.padding, self.padding, self.padding, self.padding),\n",
                "            mode='constant',\n",
                "            value=0.0\n",
                "        )\n",
                "\n",
                "        return im\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        x = self.preprocess_image(x)\n",
                "        \n",
                "        #plt.imshow(x.squeeze(), cmap=\"gray\")\n",
                "        #plt.show()\n",
                "        \n",
                "        y_hat: torch.Tensor = self.model(x)\n",
                "        y_hat = y_hat.squeeze().squeeze()\n",
                "        \n",
                "        softmax = torch.softmax(y_hat, dim=-1)\n",
                "        \n",
                "        stacked = torch.stack([self.alphabet_float, softmax], dim=1)\n",
                "        sorted_indices = torch.argsort(stacked[:, 1], descending=True)\n",
                "        sorted_tensor = stacked[sorted_indices]\n",
                "\n",
                "        unicodes: torch.Tensor = sorted_tensor[:, 0]\n",
                "        probabilities: torch.Tensor = sorted_tensor[:, 1]\n",
                "\n",
                "        unicodes = unicodes.to(dtype=torch.int)\n",
                "        probabilities = probabilities.detach()\n",
                "\n",
                "        return unicodes.unsqueeze(0), probabilities.unsqueeze(0)\n",
                "        \n",
                "        \n",
                "       \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "cnn_sara"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "alphabet: list[str] = list('∀∃().0123456789:λμabcdefghijklmnopqrstuvwxyz')\n",
                "\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\Mongo_epoch17_trainacc0.9517_valacc0.99432_Tloss0.010203_Vloss0.002475_lr0.0007224.pkl\"\n",
                "model_name: str = \"MongoCNN_Prod\"\n",
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
                "        \"fully_connected_features\": (64, len(alphabet)), \n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cpu\",\n",
                "        \"conv_dropout\": 0.075,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": model_name,\n",
                "    }\n",
                ").eval()\n",
                "\n",
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
                "\n",
                "krud_model: KrudModel = KrudModel(\n",
                "    alphabet,\n",
                "    model=cnn_model\n",
                ")\n",
                "\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"CNN_SARA_LC.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    krud_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "cnn_py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded: encoder_conv_blocks.0.0.weight\n",
                        "Loaded: encoder_conv_blocks.0.0.bias\n",
                        "Loaded: encoder_conv_blocks.0.2.weight\n",
                        "Loaded: encoder_conv_blocks.0.2.bias\n",
                        "Loaded: encoder_conv_blocks.0.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.0.2.running_var\n",
                        "Loaded: encoder_conv_blocks.0.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.0.4.weight\n",
                        "Loaded: encoder_conv_blocks.0.4.bias\n",
                        "Loaded: encoder_conv_blocks.0.6.weight\n",
                        "Loaded: encoder_conv_blocks.0.6.bias\n",
                        "Loaded: encoder_conv_blocks.0.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.0.6.running_var\n",
                        "Loaded: encoder_conv_blocks.0.6.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.1.0.weight\n",
                        "Loaded: encoder_conv_blocks.1.0.bias\n",
                        "Loaded: encoder_conv_blocks.1.2.weight\n",
                        "Loaded: encoder_conv_blocks.1.2.bias\n",
                        "Loaded: encoder_conv_blocks.1.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.1.2.running_var\n",
                        "Loaded: encoder_conv_blocks.1.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.1.4.weight\n",
                        "Loaded: encoder_conv_blocks.1.4.bias\n",
                        "Loaded: encoder_conv_blocks.1.6.weight\n",
                        "Loaded: encoder_conv_blocks.1.6.bias\n",
                        "Loaded: encoder_conv_blocks.1.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.1.6.running_var\n",
                        "Loaded: encoder_conv_blocks.1.6.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.2.0.weight\n",
                        "Loaded: encoder_conv_blocks.2.0.bias\n",
                        "Loaded: encoder_conv_blocks.2.2.weight\n",
                        "Loaded: encoder_conv_blocks.2.2.bias\n",
                        "Loaded: encoder_conv_blocks.2.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.2.2.running_var\n",
                        "Loaded: encoder_conv_blocks.2.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.2.4.weight\n",
                        "Loaded: encoder_conv_blocks.2.4.bias\n",
                        "Loaded: encoder_conv_blocks.2.6.weight\n",
                        "Loaded: encoder_conv_blocks.2.6.bias\n",
                        "Loaded: encoder_conv_blocks.2.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.2.6.running_var\n",
                        "Loaded: encoder_conv_blocks.2.6.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.3.0.weight\n",
                        "Loaded: encoder_conv_blocks.3.0.bias\n",
                        "Loaded: encoder_conv_blocks.3.2.weight\n",
                        "Loaded: encoder_conv_blocks.3.2.bias\n",
                        "Loaded: encoder_conv_blocks.3.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.3.2.running_var\n",
                        "Loaded: encoder_conv_blocks.3.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.3.4.weight\n",
                        "Loaded: encoder_conv_blocks.3.4.bias\n",
                        "Loaded: encoder_conv_blocks.3.6.weight\n",
                        "Loaded: encoder_conv_blocks.3.6.bias\n",
                        "Loaded: encoder_conv_blocks.3.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.3.6.running_var\n",
                        "Loaded: encoder_conv_blocks.3.6.num_batches_tracked\n",
                        "Loaded: fully_connected_blocks.0.0.weight\n",
                        "Loaded: fully_connected_blocks.0.0.bias\n",
                        "Loaded: fully_connected_blocks.1.0.weight\n",
                        "Loaded: fully_connected_blocks.1.0.bias\n",
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "AllCNN2D                                 [1, 94]                   --\n",
                        "├─ModuleList: 1-1                        --                        --\n",
                        "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
                        "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
                        "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
                        "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
                        "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
                        "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
                        "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
                        "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
                        "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
                        "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
                        "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
                        "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
                        "├─Sequential: 1-2                        [1, 512]                  --\n",
                        "│    └─Flatten: 2-5                      [1, 512]                  --\n",
                        "├─ModuleList: 1-3                        --                        --\n",
                        "│    └─Sequential: 2-6                   [1, 128]                  --\n",
                        "│    │    └─Linear: 3-33                 [1, 128]                  65,664\n",
                        "│    │    └─Dropout: 3-34                [1, 128]                  --\n",
                        "│    │    └─LeakyReLU: 3-35              [1, 128]                  --\n",
                        "│    └─Sequential: 2-7                   [1, 94]                   --\n",
                        "│    │    └─Linear: 3-36                 [1, 94]                   12,126\n",
                        "==========================================================================================\n",
                        "Total params: 131,598\n",
                        "Trainable params: 131,598\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (M): 13.93\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.02\n",
                        "Forward/backward pass size (MB): 2.17\n",
                        "Params size (MB): 0.53\n",
                        "Estimated Total Size (MB): 2.72\n",
                        "==========================================================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_28708\\77278072.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, 0, 0], device=im.device)  # Default value if no foreground\n",
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_28708\\77278072.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, im.size(2) - 1, im.size(3) - 1], device=im.device)  # Default value if no foreground\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:657: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:1127: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to CNN_APPLE_PY.onnx\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "alphabet = list('!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~')\n",
                "\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\z_ApplePie_epoch185_trainacc0.71586_valacc0.74578_Tloss0.063201_Vloss0.060233_lr0.0001673200819949859.pkl\"\n",
                "model_name: str = \"MongoCNN_Prod\"\n",
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32),\n",
                "        \"fully_connected_features\": (128, len(alphabet)),\n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cpu\",\n",
                "        \"conv_dropout\": 0.15,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": \"CNN_py_Apple\",\n",
                "        \"checkpoint_path\": checkpoint_path\n",
                "    }\n",
                ").eval()\n",
                "\n",
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
                "\n",
                "krud_model: KrudModel = KrudModel(\n",
                "    alphabet,\n",
                "    model=cnn_model\n",
                ")\n",
                "\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"CNN_APPLE_PY.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    krud_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "cnn_onnx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "tuple indices must be integers or slices, not tuple",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[33], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m im \u001b[38;5;241m=\u001b[39m (im[:, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      9\u001b[0m im_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(im, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[43mkrud_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem())))\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(im)\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
                        "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
                    ]
                }
            ],
            "source": [
                "dataset_dirpath: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\data\\apl_lora\"\n",
                "for im_filename in os.listdir(dataset_dirpath):\n",
                "    fullpath: str = os.path.join(dataset_dirpath, im_filename)\n",
                "\n",
                "    im = np.asarray(Image.open(fullpath))\n",
                "    \n",
                "    im = (im[:, :, 3] > 0.5).astype(np.uint8).astype(np.float32)\n",
                "    \n",
                "    im_tensor = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
                "\n",
                "    print(chr(int(krud_model.forward(im_tensor)[0, 0].item())))\n",
                "    \n",
                "    plt.imshow(im)\n",
                "    plt.show()\n",
                "\n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Test Krud ONNX"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt \n",
                "import onnxruntime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "input_image = Image.open(\"test_im.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "test_image = np.asarray(input_image)\n",
                "test_image = ((1.0-test_image[:, :, 0]/255) > 0.5).astype(np.uint8)*1.0\n",
                "\n",
                "plt.imshow(test_image, cmap=\"gray\")\n",
                "plt.show()\n",
                "\n",
                "test_image = np.expand_dims(np.expand_dims(test_image, 0), 0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(1, 1, 143, 154)"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_image.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         ...,\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.]]]])"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{ 0.99446714\n",
                        "z 0.0017108566\n",
                        "g 0.00075185206\n",
                        "& 0.0004922176\n",
                        "t 0.00033911664\n",
                        "q 0.00031183037\n",
                        "i 0.00027706032\n",
                        "I 0.0002302916\n",
                        "f 0.00022199009\n",
                        "[ 0.00020543498\n",
                        "l 0.00010953502\n",
                        "8 8.256649e-05\n",
                        "| 7.309804e-05\n",
                        "! 6.5617016e-05\n",
                        "Z 6.191048e-05\n",
                        "s 5.4513974e-05\n",
                        "e 5.3010477e-05\n",
                        "d 4.6966336e-05\n",
                        ": 4.2847674e-05\n",
                        "2 4.185938e-05\n",
                        "T 3.654618e-05\n",
                        "E 3.298688e-05\n",
                        "% 2.7017557e-05\n",
                        "1 2.5089163e-05\n",
                        "x 2.2976896e-05\n",
                        "y 2.124677e-05\n",
                        "= 1.8943676e-05\n",
                        "? 1.885482e-05\n",
                        "$ 1.7141727e-05\n",
                        "J 1.4129502e-05\n",
                        "< 1.3697342e-05\n",
                        "r 1.0265765e-05\n",
                        "j 1.0159013e-05\n",
                        "S 9.6292e-06\n",
                        "a 9.357164e-06\n",
                        "9 8.451333e-06\n",
                        "+ 7.2700263e-06\n",
                        "} 6.9444422e-06\n",
                        "Y 6.0347134e-06\n",
                        "/ 5.1318225e-06\n",
                        "X 4.7260337e-06\n",
                        "# 4.599329e-06\n",
                        "@ 3.4374466e-06\n",
                        "c 3.395164e-06\n",
                        "5 2.4998585e-06\n",
                        "L 2.4570397e-06\n",
                        "; 2.2138606e-06\n",
                        ", 2.1175108e-06\n",
                        "p 1.9161998e-06\n",
                        "3 1.3404176e-06\n",
                        "o 1.0382729e-06\n",
                        "D 8.746286e-07\n",
                        "k 7.521256e-07\n",
                        "7 6.418331e-07\n",
                        "b 5.4538094e-07\n",
                        "\\ 5.391114e-07\n",
                        "v 4.97618e-07\n",
                        "u 4.8817657e-07\n",
                        "] 4.8546985e-07\n",
                        "6 4.060425e-07\n",
                        "F 3.8945186e-07\n",
                        "* 3.7992044e-07\n",
                        "P 3.6354257e-07\n",
                        "> 3.1495009e-07\n",
                        "n 2.7530334e-07\n",
                        ") 1.9622465e-07\n",
                        "C 1.5989689e-07\n",
                        "0 1.2228098e-07\n",
                        "m 1.18939106e-07\n",
                        "h 1.1199432e-07\n",
                        "4 1.0976511e-07\n",
                        "( 1.0749783e-07\n",
                        "G 9.46377e-08\n",
                        "R 7.9344545e-08\n",
                        "- 7.186075e-08\n",
                        "~ 6.507872e-08\n",
                        "' 6.418083e-08\n",
                        "B 4.5612172e-08\n",
                        "Q 4.3985608e-08\n",
                        "w 4.3410523e-08\n",
                        ". 4.1810203e-08\n",
                        "V 4.1363947e-08\n",
                        "` 3.4543277e-08\n",
                        "O 3.2522717e-08\n",
                        "_ 2.4401915e-08\n",
                        "K 1.17915135e-08\n",
                        "W 1.0246246e-08\n",
                        "^ 1.007993e-08\n",
                        "H 6.3095076e-09\n",
                        "N 5.3535434e-09\n",
                        "A 4.8396886e-09\n",
                        "U 4.5926773e-09\n",
                        "M 3.2014513e-09\n",
                        "\" 7.2175643e-10\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Load the ONNX model\n",
                "model_path = \"CNN_APPLE_PY.onnx\"\n",
                "session = onnxruntime.InferenceSession(model_path)\n",
                "\n",
                "# Run inference\n",
                "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
                "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
                "\n",
                "input_name: str = inputs[0].name            # Only one input \n",
                "output_names: list[str] = [output.name for output in outputs]    # Only one output [unicode, prob] pairs for a single character\n",
                "\n",
                "unicodes: torch.Tensor\n",
                "probabilities: torch.Tensor\n",
                "unicodes, probabilities = session.run(\n",
                "    output_names, \n",
                "    {input_name: test_image.astype(np.float32)} # Must be float32 image of shape (batch, channels, height, width) -> (1, 1, Any, Any)\n",
                ")\n",
                "\n",
                "\n",
                "for char, prob in zip(unicodes[0], probabilities[0]):\n",
                "    print(chr(int(char)), prob)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[array([ 48, 112, 100,  54, 116,  56,  53, 111, 113, 102,  98,  40, 119,\n",
                            "        109, 107, 106, 104, 110,  50,  99, 120, 114, 215,  51,  45,  49,\n",
                            "        118,  57, 103, 122, 105, 101,  41, 115, 117, 121,  55,  97,  46,\n",
                            "         52, 108, 247, 955,  43], dtype=int32),\n",
                            " array([6.8143886e-01, 3.1727558e-01, 9.8913838e-04, 1.6000890e-04,\n",
                            "        5.1013758e-05, 2.8218346e-05, 2.4816827e-05, 8.7805711e-06,\n",
                            "        5.5376886e-06, 3.6561419e-06, 3.0133053e-06, 2.6766511e-06,\n",
                            "        2.4739402e-06, 1.5798223e-06, 1.5708143e-06, 8.6630882e-07,\n",
                            "        6.8614111e-07, 5.5757562e-07, 4.7443012e-07, 3.3036901e-07,\n",
                            "        6.3471326e-08, 2.9459986e-08, 2.2759442e-08, 1.8600712e-08,\n",
                            "        1.8000391e-08, 1.2019766e-08, 3.3489278e-09, 3.0067731e-09,\n",
                            "        2.6663645e-09, 2.0556845e-09, 1.1345208e-09, 3.8724163e-10,\n",
                            "        2.8197075e-10, 1.6642417e-10, 3.9026893e-11, 1.4455865e-11,\n",
                            "        1.0275350e-11, 8.5388979e-12, 2.8815811e-13, 2.1714051e-13,\n",
                            "        4.0094240e-14, 2.7764098e-14, 2.0496294e-15, 9.2305130e-16],\n",
                            "       dtype=float32)]"
                        ]
                    },
                    "execution_count": 68,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "tuple indices must be integers or slices, not tuple",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mkrud_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m [\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;28mint\u001b[39m(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m preds]\n",
                        "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
                    ]
                }
            ],
            "source": [
                "preds = krud_model.eval().forward(torch.tensor(test_image.astype(np.float32)))[:, 0].tolist()\n",
                "\n",
                "[chr(int(x)) for x in preds]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
