{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from typing import Any\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "import cv2\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.allcnn2d import AllCNN2D, AllCNN2D_Prod\n",
    "from drawing.interactive import draw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = os.path.abspath(\".\")\n",
    "root_path: str = os.path.join(file_path, os.pardir, os.pardir)\n",
    "checkpoint_path: str = os.path.join(\n",
    "    root_path, \n",
    "    \"checkpoints\", \n",
    "    \"GeckoFull_epoch1980_trainacc0.90605_valacc0.98089_Tloss0.78068_Vloss0.20101_lr6.320194197753456e-11.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AllCNN2D_Prod                            [1, 44]                   --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
      "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
      "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
      "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
      "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
      "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
      "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
      "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
      "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
      "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
      "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
      "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
      "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
      "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
      "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
      "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
      "├─Sequential: 1-2                        [1, 128]                  --\n",
      "│    └─Flatten: 2-6                      [1, 128]                  --\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─Sequential: 2-7                   [1, 64]                   --\n",
      "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
      "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
      "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
      "│    └─Sequential: 2-8                   [1, 44]                   --\n",
      "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
      "==========================================================================================\n",
      "Total params: 83,548\n",
      "Trainable params: 83,548\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 14.05\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.18\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 2.53\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "alphabet: list[str] = ['(', ')', '+', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'λ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '×', '÷']\n",
    "\n",
    "model: AllCNN2D_Prod = AllCNN2D_Prod(\n",
    "    labels_map=alphabet,\n",
    "    **{\n",
    "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
    "        \"fully_connected_features\": (64, 44),\n",
    "        \"expected_input_size\": (64, 64),\n",
    "        \"device\": \"cpu\",\n",
    "        \"conv_dropout\": 0.0,\n",
    "        \"verbose\": True,\n",
    "        \"name_prefix\": \"GeckoFinal\",\n",
    "        #\"checkpoint_path\": checkpoint_path\n",
    "    }\n",
    "\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '(', 40),\n",
       " (1, ')', 41),\n",
       " (2, '+', 43),\n",
       " (3, '-', 45),\n",
       " (4, '.', 46),\n",
       " (5, '0', 48),\n",
       " (6, '1', 49),\n",
       " (7, '2', 50),\n",
       " (8, '3', 51),\n",
       " (9, '4', 52),\n",
       " (10, '5', 53),\n",
       " (11, '6', 54),\n",
       " (12, '7', 55),\n",
       " (13, '8', 56),\n",
       " (14, '9', 57),\n",
       " (15, 'λ', 955),\n",
       " (16, 'a', 97),\n",
       " (17, 'b', 98),\n",
       " (18, 'c', 99),\n",
       " (19, 'd', 100),\n",
       " (20, 'e', 101),\n",
       " (21, 'f', 102),\n",
       " (22, 'g', 103),\n",
       " (23, 'h', 104),\n",
       " (24, 'i', 105),\n",
       " (25, 'j', 106),\n",
       " (26, 'k', 107),\n",
       " (27, 'l', 108),\n",
       " (28, 'm', 109),\n",
       " (29, 'n', 110),\n",
       " (30, 'o', 111),\n",
       " (31, 'p', 112),\n",
       " (32, 'q', 113),\n",
       " (33, 'r', 114),\n",
       " (34, 's', 115),\n",
       " (35, 't', 116),\n",
       " (36, 'u', 117),\n",
       " (37, 'v', 118),\n",
       " (38, 'w', 119),\n",
       " (39, 'x', 120),\n",
       " (40, 'y', 121),\n",
       " (41, 'z', 122),\n",
       " (42, '×', 215),\n",
       " (43, '÷', 247)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, char, ord(char)) for i, char in enumerate(alphabet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model saved to model.onnx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
    "\n",
    "# Define the ONNX file path\n",
    "onnx_file_path = \"model.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    input_names=[\"input\"],  # Name of the input layer\n",
    "    output_names=[\"logits\", \"softmax\", \"softmax_ordered\"],  # Names of the output layers\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"},  # Variable batch size\n",
    "                  \"logits\": {0: \"batch_size\"},\n",
    "                  \"softmax\": {0: \"batch_size\"},\n",
    "                  \"softmax_ordered\": {0: \"batch_size\"}},\n",
    "    opset_version=11  # Specify the ONNX opset version\n",
    ")\n",
    "\n",
    "print(f\"Model saved to {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"model.onnx\"\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "input_image = np.random.random(\n",
    "    (\n",
    "        1,  # batch: stack as many images as you like here\n",
    "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
    "        64, # height: fixed to 64 pixels for now\n",
    "        64  # width: fixed to 64 pixels for now\n",
    "    )\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
    "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
    "\n",
    "input_name: list[str] = inputs[0].name\n",
    "output_names: list[str] = [out.name for out in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "softmax: np.ndarray\n",
    "softmax_ordered: np.ndarray\n",
    "logits: np.ndarray\n",
    "\n",
    "logits, softmax, softmax_ordered = session.run(\n",
    "    output_names, \n",
    "    {input_name: input_image}\n",
    ")\n",
    "\n",
    "# logits.shape is shape (batch, character) for all character labels\n",
    "# softmax.shape is shape (batch, character) for all character labels\n",
    "# softmax_ordered is shape (batch, character, [label index, label prob, unicode character value])\n",
    "\n",
    "# character dim is 44 (there are 44 character labels)\n",
    "# label index is from 0 to 44 (corresponding to each ordered label index)\n",
    "# label prob is a softmaxed probability for this label prediction\n",
    "# unicode character value is the unicode character for this prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 44), (1, 44), (1, 44, 3))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, softmax.shape, softmax_ordered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top_character_probs: list[list[float]] = softmax_ordered[:, :, 1].tolist()\n",
    "\n",
    "top_characters: list[list[str]] = [\n",
    "    [\n",
    "        chr(int(softmax_ordered[batch_i, i, 2])) \n",
    "        for i in range(softmax_ordered.shape[1])\n",
    "    ] for batch_i in \n",
    "    range(softmax_ordered.shape[0])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextReShitter(nn.Module):\n",
    "    \n",
    "    def __init__(self, alphabet: list[str], text: str):\n",
    "        \n",
    "        super(TextReShitter, self).__init__()\n",
    "        \n",
    "        self.alphabet: list[str] = sorted(alphabet)\n",
    "        self.text: str = text\n",
    "        \n",
    "        self.logits: list[torch.Tensor] = []\n",
    "        \n",
    "        for c in self.text:\n",
    "            c_index: int = self.alphabet.index(c)\n",
    "            \n",
    "            char_logits: torch.Tensor = torch.rand((len(alphabet),))\n",
    "            char_logits[c_index] += 2\n",
    "            char_logits *= (torch.rand(1)+0.1)\n",
    "\n",
    "            self.logits.append(char_logits)\n",
    "\n",
    "        self.logits_tensor: torch.Tensor = torch.stack(self.logits, dim=0)\n",
    "        \n",
    "        self.logits_tensor = self.logits_tensor.unsqueeze(0) # batch, chars, logit\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        self.logits_tensor = self.logits_tensor + x.sum() * 0.0  \n",
    "\n",
    "        softmax = torch.softmax(self.logits_tensor, dim=-1)\n",
    "        \n",
    "        # Create indices for the first dimension (char_prob_i)\n",
    "        char_prob_indices = torch.arange(softmax.size(-1), device=softmax.device).reshape(1, 1, -1, 1)\n",
    "        \n",
    "        # Create indices for the third dimension (ord(self.alphabet[char_prob_i]))\n",
    "        alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n",
    "        \n",
    "        # Expand dimensions to match softmax shape\n",
    "        char_prob_indices = char_prob_indices.expand(*softmax.shape, 1)\n",
    "        alphabet_indices = alphabet_indices.expand(*softmax.shape, 1)\n",
    "        \n",
    "        # Concatenate along the last dimension\n",
    "        softmax_ordered = torch.cat([char_prob_indices, softmax.unsqueeze(-1), alphabet_indices], dim=-1)\n",
    "        \n",
    "        # Sort along the probability dimension (dim=-2)\n",
    "        sorting_indices = softmax_ordered[..., 1].argsort(dim=-1, descending=True)\n",
    "        sorted_tensor = torch.gather(softmax_ordered, -2, sorting_indices.unsqueeze(-1).expand(-1, -1, -1, 3))\n",
    "        \n",
    "        x = x * 0.5\n",
    "        \n",
    "        return self.logits_tensor, softmax, sorted_tensor\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 11, 44]), torch.Size([1, 11, 44]), torch.Size([1, 11, 44, 3]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_world_model = TextReShitter(alphabet, \"hello.world\")\n",
    "\n",
    "logits, softmax, softmax_ord = hello_world_model.forward(torch.zeros(1, 1, 1, 1))\n",
    "\n",
    "logits.shape, softmax.shape, softmax_ord.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX: Save Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model saved to hello_world.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_24520\\3231084438.py:35: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 1, 1, 1)  # Example input\n",
    "\n",
    "# Define the ONNX file path\n",
    "onnx_file_path = \"hello_world.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    hello_world_model,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    input_names=[\"input\"],  # Name of the input layer\n",
    "    output_names=[\"logits\", \"softmax\", \"softmax_ordered\"],  # Names of the output layers\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"},  # Variable batch size\n",
    "                  \"logits\": {0: \"batch_size\"},\n",
    "                  \"softmax\": {0: \"batch_size\"},\n",
    "                  \"softmax_ordered\": {0: \"batch_size\"}},\n",
    "    opset_version=11  # Specify the ONNX opset version\n",
    ")\n",
    "\n",
    "print(f\"Model saved to {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Hello World Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 @ 22.23% - h\n",
      "#2 @ 2.799% - q\n",
      "#3 @ 2.752% - 0\n",
      "--\n",
      "#1 @ 4.642% - e\n",
      "#2 @ 2.745% - (\n",
      "#3 @ 2.718% - 4\n",
      "--\n",
      "#1 @ 8.379% - l\n",
      "#2 @ 2.8% - 2\n",
      "#3 @ 2.705% - 0\n",
      "--\n",
      "#1 @ 4.657% - l\n",
      "#2 @ 2.797% - 5\n",
      "#3 @ 2.751% - y\n",
      "--\n",
      "#1 @ 18.63% - o\n",
      "#2 @ 2.937% - .\n",
      "#3 @ 2.8% - y\n",
      "--\n",
      "#1 @ 3.969% - .\n",
      "#2 @ 2.592% - 2\n",
      "#3 @ 2.581% - -\n",
      "--\n",
      "#1 @ 5.144% - w\n",
      "#2 @ 2.769% - g\n",
      "#3 @ 2.763% - 7\n",
      "--\n",
      "#1 @ 4.364% - o\n",
      "#2 @ 2.585% - +\n",
      "#3 @ 2.577% - k\n",
      "--\n",
      "#1 @ 10.52% - r\n",
      "#2 @ 3.158% - 5\n",
      "#3 @ 3.148% - c\n",
      "--\n",
      "#1 @ 8.416% - l\n",
      "#2 @ 2.752% - n\n",
      "#3 @ 2.728% - )\n",
      "--\n",
      "#1 @ 13.45% - d\n",
      "#2 @ 3.052% - 2\n",
      "#3 @ 2.972% - o\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"hello_world.onnx\"\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "input_image = np.random.random(\n",
    "    (\n",
    "        1,  # batch: stack as many images as you like here\n",
    "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
    "        1,  # height: fixed to 1 for now\n",
    "        1   # width: fixed to 1 for now\n",
    "    )\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
    "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
    "\n",
    "input_name: str = inputs[0].name\n",
    "output_names: list[str] = [out.name for out in outputs]\n",
    "softmax: np.ndarray\n",
    "softmax_ordered: np.ndarray\n",
    "logits: np.ndarray\n",
    "\n",
    "logits, softmax, softmax_ordered = session.run(\n",
    "    output_names, \n",
    "    {input_name: input_image}\n",
    ")\n",
    "\n",
    "top3_preds: np.ndarray = softmax_ordered[:, :, :3, 2] # top three predictions for each character (unicode value, convert to string with chr)\n",
    "top3_pred_probs: np.ndarray = softmax_ordered[:, :, :3, 1] # top three prediction probabilities\n",
    "\n",
    "for letter_i in range(top3_preds.shape[1]):\n",
    "    for k in range(3):\n",
    "        \n",
    "        pred_percentage: float = top3_pred_probs[0, letter_i, k]*100\n",
    "        \n",
    "        print(f\"#{k+1} @ {pred_percentage:.4}% - {chr(int(top3_preds[0, letter_i, k]))}\")\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[104., 113.,  48.],\n",
       "        [101.,  40.,  52.],\n",
       "        [108.,  50.,  48.],\n",
       "        [108.,  53., 121.],\n",
       "        [111.,  46., 121.],\n",
       "        [ 46.,  50.,  45.],\n",
       "        [119., 103.,  55.],\n",
       "        [111.,  43., 107.],\n",
       "        [114.,  53.,  99.],\n",
       "        [108., 110.,  41.],\n",
       "        [100.,  50., 111.]]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
