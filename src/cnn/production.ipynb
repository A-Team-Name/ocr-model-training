{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "from einops import rearrange\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.optim import AdamW\n",
                "from torch import nn\n",
                "from typing import Any\n",
                "import glob\n",
                "import re\n",
                "from datetime import datetime\n",
                "import random\n",
                "from collections import defaultdict\n",
                "import numpy as np\n",
                "\n",
                "import torch.nn.functional as torch_func\n",
                "from torchvision.transforms.functional import rotate, affine, resize, center_crop\n",
                "from PIL import Image\n",
                "import cv2\n",
                "\n",
                "random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import onnxruntime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from models.allcnn2d import AllCNN2D, AllCNN2D_Prod\n",
                "from drawing.interactive import draw_image"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Global"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path: str = os.path.abspath(\".\")\n",
                "root_path: str = os.path.join(file_path, os.pardir, os.pardir)\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\Mongo_epoch17_trainacc0.9517_valacc0.99432_Tloss0.010203_Vloss0.002475_lr0.0007224.pkl\"\n",
                "model_name: str = \"MongoCNN_Prod\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "44"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "alphabet: list[str] = list('∀∃().0123456789:λμabcdefghijklmnopqrstuvwxyz')\n",
                "len(alphabet)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "AllCNN2D                                 [1, 44]                   --\n",
                        "├─ModuleList: 1-1                        --                        --\n",
                        "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
                        "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
                        "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
                        "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
                        "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
                        "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
                        "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
                        "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
                        "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
                        "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
                        "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
                        "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
                        "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
                        "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
                        "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
                        "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
                        "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
                        "├─Sequential: 1-2                        [1, 128]                  --\n",
                        "│    └─Flatten: 2-6                      [1, 128]                  --\n",
                        "├─ModuleList: 1-3                        --                        --\n",
                        "│    └─Sequential: 2-7                   [1, 64]                   --\n",
                        "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
                        "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
                        "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
                        "│    └─Sequential: 2-8                   [1, 44]                   --\n",
                        "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
                        "==========================================================================================\n",
                        "Total params: 83,548\n",
                        "Trainable params: 83,548\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (M): 14.05\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.02\n",
                        "Forward/backward pass size (MB): 2.18\n",
                        "Params size (MB): 0.33\n",
                        "Estimated Total Size (MB): 2.53\n",
                        "==========================================================================================\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
                "        \"fully_connected_features\": (64, len(alphabet)), \n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cuda\",\n",
                "        \"conv_dropout\": 0.075,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": model_name,\n",
                "    }\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to Mongo_LC_Prod.onnx\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Create a dummy input tensor\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = f\"Mongo_LC_Prod.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    cnn_model.to(device=DEVICE),\n",
                "    dummy_input.to(device=DEVICE),\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"logits\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class CNN_ONNX_WRAPPER(nn.Module):\n",
                "    alphabet: list[str] = list(\"∃().0123456789:λμabcdefghijklmnopqrstuvwxyz\")\n",
                "\n",
                "    model: nn.Module  # This should be set to your actual CNN model\n",
                "\n",
                "    def __init__(self, cnn_model: nn.Module):\n",
                "        super(CNN_ONNX_WRAPPER, self).__init__()\n",
                "        self.model = cnn_model\n",
                "\n",
                "    @staticmethod\n",
                "    def preprocess_im(im: torch.Tensor, threshold: float, image_dims: tuple, pad: int) -> torch.Tensor:\n",
                "        # Apply the thresholding\n",
                "        image = torch.where(\n",
                "            im.squeeze() > threshold, \n",
                "            torch.tensor(1.0), \n",
                "            torch.tensor(0.0)\n",
                "        )\n",
                "\n",
                "        # Optionally, flip the image based on the top-left corner pixel value\n",
                "        # Compare the value directly without using .item()\n",
                "        if image[0, 0] > 0.5:  # Tensor comparison (element-wise)\n",
                "            image = 1.0 - image\n",
                "\n",
                "        # Crop around black pixels (foreground)\n",
                "        mask = image > 0.5  # Assume the foreground is black\n",
                "        coords = torch.nonzero(mask)\n",
                "\n",
                "        if coords.shape[0] > 0:\n",
                "            # Get min and max coordinates directly as tensors, no need for .item()\n",
                "            min_coords = coords.min(dim=0)[0]  # Get the min values\n",
                "            max_coords = coords.max(dim=0)[0]  # Get the max values\n",
                "            \n",
                "            # Use tensor indexing for cropping\n",
                "            x_min, y_min = min_coords[0], min_coords[1]\n",
                "            x_max, y_max = max_coords[0], max_coords[1]\n",
                "            \n",
                "            # Crop the image based on coordinates\n",
                "            image = image[x_min:x_max + 1, y_min:y_max + 1]\n",
                "\n",
                "        # Resize the image using interpolate (ONNX-compatible)\n",
                "        image = F.interpolate(image.unsqueeze(0).unsqueeze(0), size=[d - pad * 2 for d in image_dims])\n",
                "\n",
                "        # Apply threshold again after resizing\n",
                "        image = (image > threshold).to(torch.float32)\n",
                "\n",
                "        # Pad the image\n",
                "        image = F.pad(image, (pad, pad, pad, pad), value=0.0)\n",
                "\n",
                "        return image\n",
                "\n",
                "    @staticmethod\n",
                "    def softmax(x: torch.Tensor) -> torch.Tensor:\n",
                "        # Softmax on the logits\n",
                "        return torch.nn.functional.softmax(x, dim=1)\n",
                "\n",
                "    def forward(self, im: torch.Tensor) -> torch.Tensor:\n",
                "        # Preprocess the input image\n",
                "        im = self.preprocess_im(im, 0.5, (64, 64), 1)\n",
                "\n",
                "        # Debugging step: Check tensor shape before model inference\n",
                "        print(f\"Processed image shape: {im.shape}\")\n",
                "\n",
                "        # Forward pass through the model\n",
                "        logits = self.model(im)  # Add batch dimension and pass through the model\n",
                "\n",
                "        # Debugging step: Check the shape of logits\n",
                "        print(f\"Logits shape: {logits.shape}\")\n",
                "\n",
                "        # Apply softmax to get the percentages\n",
                "        percentages = self.softmax(logits)\n",
                "\n",
                "        return percentages\n",
                "\n",
                "\n",
                "# Assuming cnn_model is defined somewhere (e.g., a CNN model architecture)\n",
                "\n",
                "cnn = CNN_ONNX_WRAPPER(cnn_model)\n",
                "\n",
                "# Create a dummy input tensor\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input with the shape [batch_size, channels, height, width]\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"Lake_FINAL_Prod.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    cnn.eval(),\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"prob\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}, \"prob\": {0: \"batch_size\"}},\n",
                "    opset_version=12  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load Onnx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "import onnxruntime\n",
                "import numpy as np\n",
                "import os\n",
                "from PIL import Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "onnx_file_path = f\"Lake_FINAL_Prod.onnx\"\n",
                "\n",
                "# Load the ONNX model\n",
                "model_path = onnx_file_path\n",
                "\n",
                "# Define the execution providers without TensorRT\n",
                "providers = ['CPUExecutionProvider']\n",
                "\n",
                "# Create the InferenceSession with the explicit providers\n",
                "session = onnxruntime.InferenceSession(model_path, providers=providers)\n",
                "\n",
                "# Prepare the input image\n",
                "input_image = np.random.random(\n",
                "    (\n",
                "        1,  # batch: stack as many images as you like here\n",
                "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
                "        64, # height: fixed to 64 pixels for now\n",
                "        64  # width: fixed to 64 pixels for now\n",
                "    )\n",
                ").astype(np.float32)\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processed image shape: torch.Size([1, 1, 64, 64])\n",
                        "Logits shape: torch.Size([1, 43])\n",
                        "Top indices shape: torch.Size([1, 43])\n"
                    ]
                }
            ],
            "source": [
                "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
                "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
                "\n",
                "input_name: list[str] = inputs[0].name\n",
                "output_names: list[str] = [out.name for out in outputs]\n",
                "\n",
                "test_im_path: str = r\"test_im.png\"\n",
                "test_im: Image.Image = Image.open(test_im_path).convert(\"L\")\n",
                "test_im_np: np.ndarray = np.asarray(test_im).astype(np.float32)\n",
                "\n",
                "c_prob = cnn.eval().forward(torch.tensor(test_im_np))\n",
                "prob = session.run(\n",
                "    output_names, \n",
                "    {input_name: np.expand_dims(np.expand_dims(test_im_np, 0), 0)}\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(tensor([[9.9999e-01, 5.2548e-06, 1.7248e-06, 8.1988e-07, 7.9782e-07, 5.8198e-07,\n",
                            "           3.0758e-07, 2.0259e-07, 1.6081e-07, 1.2946e-07, 2.3220e-08, 2.1835e-08,\n",
                            "           1.7244e-08, 1.7012e-08, 1.5099e-08, 1.0663e-08, 1.0223e-08, 2.6446e-09,\n",
                            "           2.5364e-09, 2.4631e-09, 2.3556e-09, 1.3106e-09, 9.8690e-10, 8.5152e-10,\n",
                            "           1.5691e-10, 8.2225e-11, 5.7212e-11, 4.9990e-11, 3.8997e-11, 1.2193e-11,\n",
                            "           2.4857e-12, 9.5809e-13, 9.4636e-13, 5.4760e-13, 1.8145e-13, 1.3831e-13,\n",
                            "           1.3262e-13, 3.5077e-14, 2.8431e-16, 1.8610e-16, 1.0641e-16, 6.3491e-17,\n",
                            "           1.4506e-17]], device='cuda:0', grad_fn=<GatherBackward0>),\n",
                            "  array([[9.9998999e-01, 5.2570049e-06, 1.7245728e-06, 8.1932870e-07,\n",
                            "          7.9734730e-07, 5.8144116e-07, 3.0757428e-07, 2.0254525e-07,\n",
                            "          1.6080422e-07, 1.2947180e-07, 2.3234454e-08, 2.1837661e-08,\n",
                            "          1.7260124e-08, 1.7012930e-08, 1.5071729e-08, 1.0674785e-08,\n",
                            "          1.0232568e-08, 2.6464866e-09, 2.5363978e-09, 2.4632330e-09,\n",
                            "          2.3562956e-09, 1.3109808e-09, 9.8771524e-10, 8.5169272e-10,\n",
                            "          1.5688881e-10, 8.2367668e-11, 5.7273134e-11, 5.0072991e-11,\n",
                            "          3.9002836e-11, 1.2206091e-11, 2.4869085e-12, 9.5866425e-13,\n",
                            "          9.4728956e-13, 5.4838718e-13, 1.8144352e-13, 1.3844312e-13,\n",
                            "          1.3272616e-13, 3.5106602e-14, 2.8464672e-16, 1.8622565e-16,\n",
                            "          1.0655152e-16, 6.3578961e-17, 1.4504423e-17]], dtype=float32)),\n",
                            " (tensor([ 956,  955,  120,  107,  121,  117,  100,   97,  108,  116,  101,  104,\n",
                            "            49,  119,   52,  122,  112,  114,   40,  118,   98,   58,  113,  102,\n",
                            "           105,  110,   55,  109,   54,   50,   57,   51, 8707,   48,   53,   56,\n",
                            "           103,   99,   46,  106,  111,   41,  115]),\n",
                            "  array([  56,  100,  116,  102,  103,  120,   98,  113,  112,  101,  109,\n",
                            "          122,   97,   54,  956,   57,  121,  107,   51, 8707,  117,   52,\n",
                            "          118,   53,  119,  108,   48,  111,  115,  104,   50,  955,   49,\n",
                            "           58,  110,   55,  105,  106,  114,   99,   40,   41,   46],\n",
                            "        dtype=int64))]"
                        ]
                    },
                    "execution_count": 135,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "list(zip([x for x in c_prob], [x for x in prob]))\n",
                "# Run inference\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from torchvision.transforms import functional as F\n",
                "\n",
                "def preprocess_im(im: str, threshold: float, image_dims: tuple, pad: int) -> np.ndarray:\n",
                "    # Load the image\n",
                "    image = Image.open(im)\n",
                "    \n",
                "    # Convert to grayscale (if not already)\n",
                "    image = image.convert(\"L\")\n",
                "    \n",
                "    # Convert the image to a tensor\n",
                "    image = F.to_tensor(image).squeeze()\n",
                "    \n",
                "    # Thresholding the image\n",
                "    image = torch.where(image > threshold, torch.tensor(1.0), torch.tensor(0.0))\n",
                "\n",
                "    \n",
                "    if image[0, 0] > 0.5:\n",
                "        image = 1.0 - image\n",
                "    \n",
                "    # Crop the image around the black pixels (foreground)\n",
                "    mask = image > 0.5  # Assuming foreground is black (0) and background is white (1)\n",
                "    coords = torch.nonzero(mask)\n",
                "    \n",
                "    if coords.shape[0] > 0:\n",
                "        # Get min and max coordinates\n",
                "        min_coords = coords.min(dim=0)[0]  # Getting the min values\n",
                "        max_coords = coords.max(dim=0)[0]  # Getting the max values\n",
                "        \n",
                "        x_min, y_min = min_coords[0].item(), min_coords[1].item()  # Convert tensor to scalar\n",
                "        x_max, y_max = max_coords[0].item(), max_coords[1].item()  # Convert tensor to scalar\n",
                "        \n",
                "        # Crop the image based on coordinates\n",
                "        image = image[x_min:x_max + 1, y_min:y_max + 1]\n",
                "    \n",
                "    # Resize to the desired dimensions\n",
                "    image = F.resize(image.unsqueeze(0), [d - pad * 2 for d in image_dims])\n",
                "    \n",
                "    # Threshold again after resize\n",
                "    image = (image > threshold).type(torch.uint8).type(torch.float32)\n",
                "\n",
                "    # Pad the image\n",
                "    image = F.pad(image, (pad, pad, pad, pad), fill=0.0)\n",
                "\n",
                "    # Convert to NumPy array\n",
                "    image = image.squeeze().cpu().numpy()  # Remove the batch dimension and convert to NumPy\n",
                "\n",
                "    return image\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJoAAACPCAAAAAAnj53jAAAADmVYSWZNTQAqAAAACAAAAAAAAADSU5MAAAI7SURBVHic7ZxbbsUgDAVx1f1vmX7ch4IKAR/8IJbno2pFWkbHEG6aBKrlVH68BcakGkKqIaQaQqohpBpCqiGkGkKqIaQaQqohpBpCqiGkGkKqIfxu/C6Vovl/po3USM6iy2ZBNfUOHmuphrAzQ4fITF2N1ITmhlZBBfQ21Q495Y6QOtcdPENxNe11KmZq6uypqd57k09NbAzCauqzIO5YUyWqmu7NcVRNfxaELagu8dQMhlrA1Eop2ucOhdTEhDG1+VA79DpUiKBq/bLSTRsLSM3irKZQUDlttbG2ryitJlhrRI3+fdM21dpvZIKm1j+l0rtJZEWAC9rrnZqG3dgAtUuXdO3++oNESYmdPX367vdc26P4f/8LWNBaJgPq1UjUBssCU6uXr21DbQ/ZQfI6tNY6buTDHgvtLJwfiitCtzQWe9vMLdDnNSrqlysf4qRm8yHyBZCa1XsdcQpqCU/NcH4GSs2UKGqmQy1MarZw1CxXqcJPzfDtQ4aacWjc1Cxf2VxXsw4tygy1ree6mnk9wxTU9pXqVTX7erJSM34PfVHNIbQg08B6X4E1NY96MlIz34xhSc0ltPXU7HewWFHzCW05NYdtPxbUnEJbTc1jr5S5mldoczXGfQJhHryG+oU2U3M0m6h5mj12rLmGNk/Nb2OqOzW3k+2LGzVns/Gt2reY40Zjo9T8zUZqB5gN1E4w64+1zUcNhBjPUG+zrprvIvDlYWvo9FkmGzYe4tLmYQU9hFRDSDWEg9X+APAMRyZZK8mFAAAAAElFTkSuQmCC",
                        "text/plain": [
                            "<PIL.Image.Image image mode=L size=154x143>"
                        ]
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_im_path: str = r\"test_im.png\"\n",
                "test_im: Image.Image = Image.open(test_im_path).convert(\"L\")\n",
                "test_im"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "test_im_np = preprocess_im(test_im_path, 0.1, (64, 64), 1)\n",
                "\n",
                "plt.imshow(test_im_np)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(array([9.94052410e-01, 1.23858731e-03, 9.81083489e-04, 6.86358311e-04,\n",
                            "        6.23539032e-04, 6.17766578e-04, 6.12980337e-04, 4.79297625e-04,\n",
                            "        1.40747419e-04, 1.03887825e-04, 1.03488579e-04, 1.00947203e-04,\n",
                            "        9.20660677e-05, 8.27235781e-05, 1.91916297e-05, 1.06729467e-05,\n",
                            "        9.47296121e-06, 8.12046710e-06, 7.98343717e-06, 7.91991897e-06,\n",
                            "        4.30528098e-06, 3.55534780e-06, 3.19691048e-06, 2.17775141e-06,\n",
                            "        1.96297742e-06, 1.89501952e-06, 1.31547233e-06, 6.72157057e-07,\n",
                            "        6.70021336e-07, 4.55316723e-07, 1.70466848e-07, 1.66958785e-07,\n",
                            "        6.93960942e-08, 3.81598682e-08, 3.45308315e-08, 2.14136531e-09,\n",
                            "        1.18820531e-09, 9.17822873e-10, 1.36637798e-10, 5.52658717e-11,\n",
                            "        5.39423055e-11, 2.60328044e-11, 1.79959415e-11], dtype=float32),\n",
                            " array([ 956,  101,  116,  107,  122,   98,  108,  120,  104,   49,  102,\n",
                            "         955,  112,  121,  100,   97,  117,   58,  113,  114,   40,  110,\n",
                            "         118,  105,   54,  119,   52,   50,   55,  109,   48,   56,   53,\n",
                            "        8707,   51,   57,  103,   99,  111,  106,   46,  115,   41]))"
                        ]
                    },
                    "execution_count": 87,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(64, 64)"
                        ]
                    },
                    "execution_count": 58,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_im_np.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "softmax: np.ndarray\n",
                "softmax_ordered: np.ndarray\n",
                "logits: np.ndarray\n",
                "\n",
                "logits = session.run(\n",
                "    output_names, \n",
                "    {input_name: np.expand_dims(np.expand_dims(test_im_np, 0), 0)}\n",
                ")[0]\n",
                "\n",
                "# logits.shape is shape (batch, character) for all character labels\n",
                "# softmax.shape is shape (batch, character) for all character labels\n",
                "# softmax_ordered is shape (batch, character, [label index, label prob, unicode character value])\n",
                "\n",
                "# character dim is 44 (there are 44 character labels)\n",
                "# label index is from 0 to 44 (corresponding to each ordered label index)\n",
                "# label prob is a softmaxed probability for this label prediction\n",
                "# unicode character value is the unicode character for this prediction\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "43"
                        ]
                    },
                    "execution_count": 61,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'μ'"
                        ]
                    },
                    "execution_count": 62,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "index = np.argmax(softmax(logits.squeeze()))\n",
                "alphabet[index]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[5.30846650e-04, 1.68099464e-03, 1.37032261e-02, 1.18300821e-04,\n",
                            "        5.17544569e-03, 1.18788116e-01, 7.00770020e-02, 2.72251527e-05,\n",
                            "        5.65034361e-06, 2.70747951e-05, 7.02182706e-06, 1.98335965e-05,\n",
                            "        2.47772950e-05, 2.26152275e-04, 2.94749648e-03, 5.39878034e-04,\n",
                            "        1.65653706e-01, 1.00289382e-01, 2.63870694e-03, 1.34053471e-05,\n",
                            "        6.00268249e-04, 2.87390407e-03, 3.22919041e-05, 2.28433162e-02,\n",
                            "        7.75476918e-04, 1.12298233e-06, 1.69513543e-04, 2.67672760e-07,\n",
                            "        3.25475936e-03, 7.96237418e-06, 1.93356220e-02, 5.77072799e-03,\n",
                            "        3.84508690e-04, 9.18507576e-03, 4.11047113e-05, 5.12989820e-04,\n",
                            "        1.17080397e-07, 9.30333436e-02, 1.01616504e-02, 3.10798088e-04,\n",
                            "        8.07399279e-04, 3.47387671e-01, 1.58225430e-05]], dtype=float32)"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "\n",
                "softmax(logits)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'softmax_ordered' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m top_character_probs: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_ordered\u001b[49m[:, :, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m top_characters: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     [\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mchr\u001b[39m(\u001b[38;5;28mint\u001b[39m(softmax_ordered[batch_i, i, \u001b[38;5;241m2\u001b[39m])) \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mrange\u001b[39m(softmax_ordered\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      9\u001b[0m ]\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'softmax_ordered' is not defined"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "top_character_probs: list[list[float]] = softmax_ordered[:, :, 1].tolist()\n",
                "\n",
                "top_characters: list[list[str]] = [\n",
                "    [\n",
                "        chr(int(softmax_ordered[batch_i, i, 2])) \n",
                "        for i in range(softmax_ordered.shape[1])\n",
                "    ] for batch_i in \n",
                "    range(softmax_ordered.shape[0])\n",
                "]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hello World"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextReShitter(nn.Module):\n",
                "    \n",
                "    def __init__(self, alphabet: list[str], text: str):\n",
                "        \n",
                "        super(TextReShitter, self).__init__()\n",
                "        \n",
                "        self.alphabet: list[str] = alphabet\n",
                "        self.text: str = text\n",
                "        \n",
                "        self.logits: list[torch.Tensor] = []\n",
                "        \n",
                "        for c in self.text:\n",
                "            c_index: int = self.alphabet.index(c)\n",
                "            \n",
                "            char_logits: torch.Tensor = torch.rand((len(alphabet),))\n",
                "            char_logits[c_index] += 2\n",
                "            char_logits *= (torch.rand(1)+0.1)\n",
                "\n",
                "            self.logits.append(char_logits)\n",
                "\n",
                "        self.logits_tensor: torch.Tensor = torch.stack(self.logits, dim=0)\n",
                "        \n",
                "        self.logits_tensor = self.logits_tensor.unsqueeze(0) # batch, chars, logit\n",
                "        \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \n",
                "        self.logits_tensor = self.logits_tensor + x.sum() * 0.0  \n",
                "\n",
                "        softmax = torch.softmax(self.logits_tensor, dim=-1)\n",
                "        \n",
                "        # Create indices for the first dimension (char_prob_i)\n",
                "        char_prob_indices = torch.arange(softmax.size(-1), device=softmax.device).reshape(1, 1, -1, 1)\n",
                "        \n",
                "        # Create indices for the third dimension (ord(self.alphabet[char_prob_i]))\n",
                "        alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n",
                "        \n",
                "        # Expand dimensions to match softmax shape\n",
                "        char_prob_indices = char_prob_indices.expand(*softmax.shape, 1)\n",
                "        alphabet_indices = alphabet_indices.expand(*softmax.shape, 1)\n",
                "        \n",
                "        # Concatenate along the last dimension\n",
                "        softmax_ordered = torch.cat([char_prob_indices, softmax.unsqueeze(-1), alphabet_indices], dim=-1)\n",
                "        \n",
                "        # Sort along the probability dimension (dim=-2)\n",
                "        sorting_indices = softmax_ordered[..., 1].argsort(dim=-1, descending=True)\n",
                "        sorted_tensor = torch.gather(softmax_ordered, -2, sorting_indices.unsqueeze(-1).expand(-1, -1, -1, 3))\n",
                "        \n",
                "        x = x * 0.5\n",
                "        \n",
                "        unicodes: torch.Tensor = sorted_tensor[0, :, :, 2]\n",
                "        probs: torch.Tensor = sorted_tensor[0, :, :, 1]\n",
                "        \n",
                "        unicodes = unicodes.to(dtype=torch.int)\n",
                "        \n",
                "        return unicodes, probs \n",
                "        \n",
                "        \n",
                "        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[104,  54,  57,  98, 121, 113, 111, 105,  50, 118,  55, 119,  51,  43,\n",
                            "          116,  40, 108, 117, 247,  97, 215, 115, 122,  99,  52,  45, 110,  53,\n",
                            "           41, 112,  48,  46, 103, 100,  49, 109,  56, 120, 955, 106, 114, 107,\n",
                            "          101, 102],\n",
                            "         [101, 103,  57, 955,  53,  97, 100, 215,  51, 114, 119,  40, 109,  48,\n",
                            "          108,  54,  46,  50, 102, 121, 247, 120, 116, 115, 122,  99,  55, 104,\n",
                            "           98,  41, 107, 110, 106, 113, 118, 105,  43, 117,  45, 111,  52, 112,\n",
                            "           56,  49],\n",
                            "         [108,  51,  50, 103, 105, 100, 113, 116, 247, 109, 119,  99,  45, 104,\n",
                            "          121, 118,  46,  43,  98,  55, 120,  52, 955,  54,  41,  49,  57, 115,\n",
                            "          102,  48,  53, 112, 101,  40, 114, 111, 215, 122,  56, 106,  97, 117,\n",
                            "          107, 110],\n",
                            "         [108,  45, 116,  99,  54,  48, 102, 121, 106,  51,  98,  56,  46, 215,\n",
                            "          115, 247, 101,  40,  52,  49,  57, 104, 111, 120, 955,  55, 110, 122,\n",
                            "          119, 113, 100,  41, 109,  50,  97, 117,  43, 118, 103, 114, 107, 105,\n",
                            "           53, 112],\n",
                            "         [111, 101,  57,  48,  99, 118, 112, 215,  49, 104, 114, 122, 105,  98,\n",
                            "           46, 103, 110, 120, 115,  97,  54,  45,  53,  51, 116,  40,  50, 108,\n",
                            "           56, 117, 113, 109, 119,  43, 121, 107, 100,  41,  52,  55, 955, 102,\n",
                            "          247, 106],\n",
                            "         [ 46, 112, 117, 106, 111, 115, 247, 119,  54, 103, 109, 113, 107, 101,\n",
                            "           97, 122, 110, 121, 114,  43,  40, 116, 118, 105,  49, 215, 100,  57,\n",
                            "           48,  51,  52,  56,  55,  41, 104, 955,  53,  98, 108,  99,  45, 120,\n",
                            "          102,  50],\n",
                            "         [119, 112,  55,  45,  51, 111,  54,  98, 102,  52, 247,  57, 955,  40,\n",
                            "          118, 115, 100,  97, 103, 101, 110, 107, 117, 106, 121, 109,  41,  53,\n",
                            "          120, 105,  49,  46,  56, 114, 104,  48, 108,  43, 215, 122, 116,  99,\n",
                            "          113,  50],\n",
                            "         [111,  55, 107, 120, 100,  98,  57,  41, 113,  51,  99, 103,  50, 108,\n",
                            "          105, 116,  48, 215, 112, 106,  43,  56, 122,  46,  52, 119,  40, 109,\n",
                            "           45, 102,  97, 104, 101, 118,  49, 110, 114,  53, 247, 121,  54, 115,\n",
                            "          955, 117],\n",
                            "         [114,  56, 104,  97, 103,  55, 116,  41, 105, 215,  49,  48, 101, 118,\n",
                            "           57, 107, 119, 109, 247, 955,  40, 111,  52,  98, 108,  51,  54, 120,\n",
                            "          112, 115,  53,  45, 117, 121, 100,  46, 102,  99,  43,  50, 106, 110,\n",
                            "          122, 113],\n",
                            "         [108,  54,  43, 118, 115,  51, 102,  41,  55, 117,  98, 101, 122, 215,\n",
                            "          119, 100,  56, 116, 107,  99,  48, 109,  49, 106,  45, 121,  46, 105,\n",
                            "           53,  52,  97, 112, 955, 104, 113, 111, 114,  50, 247,  40, 103, 120,\n",
                            "           57, 110],\n",
                            "         [100,  46, 103, 105, 215, 117, 110, 104, 113, 119, 118, 247,  54,  48,\n",
                            "          121, 112, 122, 114,  49, 106,  57,  56, 120,  98, 102, 955,  52, 101,\n",
                            "          108, 107,  40, 115, 109,  43,  51,  99,  50, 111,  55,  97, 116,  45,\n",
                            "           53,  41]], dtype=torch.int32),\n",
                            " tensor([[0.1105, 0.0316, 0.0312, 0.0310, 0.0306, 0.0294, 0.0279, 0.0268, 0.0261,\n",
                            "          0.0260, 0.0260, 0.0257, 0.0255, 0.0242, 0.0237, 0.0235, 0.0226, 0.0226,\n",
                            "          0.0222, 0.0219, 0.0219, 0.0214, 0.0208, 0.0202, 0.0198, 0.0196, 0.0191,\n",
                            "          0.0187, 0.0187, 0.0170, 0.0165, 0.0162, 0.0154, 0.0151, 0.0150, 0.0145,\n",
                            "          0.0142, 0.0142, 0.0136, 0.0125, 0.0122, 0.0114, 0.0114, 0.0113],\n",
                            "         [0.0369, 0.0249, 0.0248, 0.0248, 0.0248, 0.0248, 0.0246, 0.0245, 0.0243,\n",
                            "          0.0243, 0.0241, 0.0240, 0.0239, 0.0239, 0.0238, 0.0238, 0.0236, 0.0233,\n",
                            "          0.0231, 0.0231, 0.0229, 0.0229, 0.0227, 0.0226, 0.0224, 0.0224, 0.0223,\n",
                            "          0.0219, 0.0219, 0.0217, 0.0216, 0.0212, 0.0212, 0.0210, 0.0208, 0.0201,\n",
                            "          0.0201, 0.0199, 0.0196, 0.0193, 0.0193, 0.0192, 0.0190, 0.0190],\n",
                            "         [0.0446, 0.0253, 0.0252, 0.0250, 0.0250, 0.0248, 0.0247, 0.0247, 0.0240,\n",
                            "          0.0240, 0.0240, 0.0239, 0.0237, 0.0235, 0.0235, 0.0234, 0.0232, 0.0229,\n",
                            "          0.0227, 0.0227, 0.0223, 0.0221, 0.0219, 0.0219, 0.0216, 0.0215, 0.0214,\n",
                            "          0.0211, 0.0210, 0.0210, 0.0208, 0.0206, 0.0206, 0.0205, 0.0205, 0.0205,\n",
                            "          0.0203, 0.0203, 0.0202, 0.0202, 0.0201, 0.0199, 0.0196, 0.0194],\n",
                            "         [0.0365, 0.0246, 0.0246, 0.0244, 0.0243, 0.0242, 0.0241, 0.0241, 0.0240,\n",
                            "          0.0239, 0.0239, 0.0237, 0.0237, 0.0234, 0.0234, 0.0234, 0.0230, 0.0228,\n",
                            "          0.0227, 0.0226, 0.0225, 0.0225, 0.0225, 0.0222, 0.0221, 0.0220, 0.0220,\n",
                            "          0.0220, 0.0219, 0.0218, 0.0217, 0.0215, 0.0213, 0.0212, 0.0210, 0.0208,\n",
                            "          0.0208, 0.0208, 0.0206, 0.0205, 0.0204, 0.0203, 0.0203, 0.0202],\n",
                            "         [0.1248, 0.0297, 0.0292, 0.0291, 0.0290, 0.0290, 0.0286, 0.0275, 0.0272,\n",
                            "          0.0262, 0.0250, 0.0244, 0.0243, 0.0241, 0.0234, 0.0230, 0.0226, 0.0218,\n",
                            "          0.0217, 0.0214, 0.0207, 0.0202, 0.0202, 0.0192, 0.0190, 0.0188, 0.0184,\n",
                            "          0.0184, 0.0182, 0.0176, 0.0174, 0.0168, 0.0154, 0.0148, 0.0144, 0.0142,\n",
                            "          0.0135, 0.0133, 0.0133, 0.0132, 0.0132, 0.0130, 0.0129, 0.0121],\n",
                            "         [0.0831, 0.0307, 0.0302, 0.0302, 0.0297, 0.0293, 0.0292, 0.0292, 0.0285,\n",
                            "          0.0281, 0.0271, 0.0266, 0.0250, 0.0245, 0.0234, 0.0233, 0.0230, 0.0228,\n",
                            "          0.0226, 0.0214, 0.0211, 0.0208, 0.0207, 0.0204, 0.0200, 0.0194, 0.0189,\n",
                            "          0.0182, 0.0177, 0.0177, 0.0177, 0.0168, 0.0166, 0.0164, 0.0163, 0.0160,\n",
                            "          0.0156, 0.0155, 0.0150, 0.0147, 0.0142, 0.0141, 0.0141, 0.0140],\n",
                            "         [0.0618, 0.0272, 0.0269, 0.0265, 0.0264, 0.0263, 0.0259, 0.0255, 0.0254,\n",
                            "          0.0251, 0.0251, 0.0249, 0.0246, 0.0242, 0.0237, 0.0234, 0.0233, 0.0232,\n",
                            "          0.0227, 0.0227, 0.0224, 0.0223, 0.0221, 0.0203, 0.0202, 0.0200, 0.0199,\n",
                            "          0.0199, 0.0199, 0.0196, 0.0194, 0.0193, 0.0193, 0.0191, 0.0190, 0.0189,\n",
                            "          0.0183, 0.0181, 0.0181, 0.0180, 0.0178, 0.0178, 0.0178, 0.0178],\n",
                            "         [0.0979, 0.0313, 0.0303, 0.0293, 0.0291, 0.0289, 0.0271, 0.0265, 0.0259,\n",
                            "          0.0257, 0.0252, 0.0244, 0.0241, 0.0236, 0.0231, 0.0230, 0.0216, 0.0215,\n",
                            "          0.0213, 0.0207, 0.0207, 0.0205, 0.0204, 0.0203, 0.0198, 0.0195, 0.0189,\n",
                            "          0.0188, 0.0184, 0.0183, 0.0180, 0.0180, 0.0169, 0.0168, 0.0164, 0.0163,\n",
                            "          0.0161, 0.0157, 0.0156, 0.0156, 0.0151, 0.0147, 0.0143, 0.0142],\n",
                            "         [0.0317, 0.0248, 0.0247, 0.0247, 0.0246, 0.0245, 0.0241, 0.0241, 0.0239,\n",
                            "          0.0238, 0.0236, 0.0236, 0.0233, 0.0233, 0.0233, 0.0232, 0.0232, 0.0228,\n",
                            "          0.0227, 0.0226, 0.0225, 0.0225, 0.0224, 0.0224, 0.0224, 0.0220, 0.0219,\n",
                            "          0.0219, 0.0218, 0.0218, 0.0218, 0.0216, 0.0215, 0.0214, 0.0212, 0.0212,\n",
                            "          0.0212, 0.0212, 0.0211, 0.0210, 0.0209, 0.0207, 0.0206, 0.0205],\n",
                            "         [0.0449, 0.0259, 0.0256, 0.0250, 0.0250, 0.0250, 0.0250, 0.0248, 0.0246,\n",
                            "          0.0245, 0.0243, 0.0238, 0.0237, 0.0236, 0.0233, 0.0233, 0.0232, 0.0231,\n",
                            "          0.0230, 0.0230, 0.0230, 0.0226, 0.0220, 0.0220, 0.0218, 0.0218, 0.0217,\n",
                            "          0.0213, 0.0212, 0.0209, 0.0207, 0.0207, 0.0203, 0.0202, 0.0201, 0.0200,\n",
                            "          0.0200, 0.0197, 0.0195, 0.0193, 0.0193, 0.0192, 0.0191, 0.0190],\n",
                            "         [0.0325, 0.0244, 0.0240, 0.0239, 0.0239, 0.0237, 0.0236, 0.0236, 0.0236,\n",
                            "          0.0236, 0.0233, 0.0233, 0.0230, 0.0229, 0.0229, 0.0228, 0.0228, 0.0228,\n",
                            "          0.0228, 0.0228, 0.0228, 0.0227, 0.0227, 0.0227, 0.0226, 0.0226, 0.0226,\n",
                            "          0.0225, 0.0223, 0.0223, 0.0223, 0.0222, 0.0221, 0.0215, 0.0214, 0.0214,\n",
                            "          0.0212, 0.0211, 0.0211, 0.0209, 0.0208, 0.0207, 0.0206, 0.0204]]))"
                        ]
                    },
                    "execution_count": 133,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "hello_world_model = TextReShitter(alphabet, \"hello.world\")\n",
                "\n",
                "hello_world_model.forward(torch.zeros(1, 1, 1, 1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ONNX: Save Hello World"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to hello_world.onnx\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_1824\\427549207.py:35: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  alphabet_indices = torch.tensor([ord(c) for c in self.alphabet], device=softmax.device).reshape(1, 1, -1, 1)\n"
                    ]
                }
            ],
            "source": [
                "# Create a dummy input tensor\n",
                "dummy_input = torch.randn(1, 1, 100, 100)  # Example input\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"hello_world.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    hello_world_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load Hello World Onnx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['h', 'e', 'l', 'l', 'o', '.', 'w', 'o', 'r', 'l', 'd']"
                        ]
                    },
                    "execution_count": 135,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "\n",
                "# Load the ONNX model\n",
                "model_path = \"hello_world.onnx\"\n",
                "session = onnxruntime.InferenceSession(model_path)\n",
                "\n",
                "input_image = np.random.random(\n",
                "    (\n",
                "        1,  # batch: stack as many images as you like here\n",
                "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
                "        2,  # height: fixed to 1 for now\n",
                "        1   # width: fixed to 1 for now\n",
                "    )\n",
                ").astype(np.float32)\n",
                "\n",
                "# Run inference\n",
                "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
                "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
                "\n",
                "input_name: str = inputs[0].name\n",
                "output_names: list[str] = [out.name for out in outputs]\n",
                "softmax: np.ndarray\n",
                "softmax_ordered: np.ndarray\n",
                "logits: np.ndarray\n",
                "\n",
                "unicodes, probs = session.run(\n",
                "    output_names, \n",
                "    {input_name: input_image}\n",
                ")\n",
                "\n",
                "list(map(chr, unicodes[:, 0].tolist()))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create CNN Production"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "class KrudModel(nn.Module):\n",
                "    \n",
                "    def __init__(\n",
                "        self, \n",
                "        alphabet: list[str], \n",
                "        target_width: int = 64, \n",
                "        target_height: int = 64, \n",
                "        padding: int = 1,\n",
                "        model: torch.nn.Module = None\n",
                "    ):\n",
                "        \n",
                "        super(KrudModel, self).__init__()\n",
                "        \n",
                "        self.alphabet: list[str] = alphabet\n",
                "        self.alphabet_float: torch.Tensor = torch.tensor([ord(a) for a in self.alphabet], dtype=torch.float32) \n",
                "        self.target_width: int = target_width\n",
                "        self.target_height: int = target_height\n",
                "        self.padding: int = padding\n",
                "        self._unpadded_dims: tuple[int] = (\n",
                "            self.target_height-self.padding*2,\n",
                "            self.target_width-self.padding*2\n",
                "        )\n",
                "        self.model: torch.nn.Module = model.eval()\n",
                "    \n",
                "        \n",
                "    def preprocess_image(\n",
                "        self, \n",
                "        im: torch.Tensor\n",
                "    ) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Preprocesses the input image by cropping, resizing, binarizing, and padding it.\n",
                "\n",
                "        Args:\n",
                "            im (torch.Tensor): Input image tensor of shape (C, H, W) or (B, C, H, W).\n",
                "\n",
                "        Returns:\n",
                "            torch.Tensor: Preprocessed image tensor.\n",
                "        \"\"\"\n",
                "        # Step 1: Binarize the image (foreground = 0, background = 1)\n",
                "        mask = im > 0.5\n",
                "\n",
                "        # Step 2: Find bounding box coordinates of the foreground\n",
                "        coords = torch.nonzero(mask)  # Get coordinates of non-zero (foreground) pixels\n",
                "\n",
                "        # Step 3: Check if there are any foreground pixels (traceable)\n",
                "        has_foreground = torch.any(mask)\n",
                "\n",
                "        # Step 4: Compute min and max coordinates for cropping (if foreground exists)\n",
                "        min_coords = torch.where(\n",
                "            has_foreground,\n",
                "            coords.min(dim=0)[0],\n",
                "            torch.tensor([0, 0, 0, 0], device=im.device)  # Default value if no foreground\n",
                "        )\n",
                "        max_coords = torch.where(\n",
                "            has_foreground,\n",
                "            coords.max(dim=0)[0],\n",
                "            torch.tensor([0, 0, im.size(2) - 1, im.size(3) - 1], device=im.device)  # Default value if no foreground\n",
                "        )\n",
                "\n",
                "        # Step 5: Extract min and max coordinates for height and width (assuming input is 4D: B, C, H, W)\n",
                "        min_x, min_y = min_coords[2], min_coords[3]\n",
                "        max_x, max_y = max_coords[2], max_coords[3]\n",
                "\n",
                "        # Step 6: Crop the image (use slicing, which is traceable)\n",
                "        im = im[:, :, min_x:max_x + 1, min_y:max_y + 1]\n",
                "\n",
                "        # Step 7: Resize the image to the desired dimensions\n",
                "        im = torch_func.interpolate(\n",
                "            im, \n",
                "            size=self._unpadded_dims, \n",
                "            mode=\"bilinear\", \n",
                "            align_corners=False\n",
                "        )\n",
                "\n",
                "        # Step 8: Binarize the image again (optional, depending on your use case)\n",
                "        im = (im > 0.5).type(torch.uint8).type(torch.float32)\n",
                "\n",
                "        # Step 9: Pad the image\n",
                "        im = torch_func.pad(\n",
                "            im,\n",
                "            (self.padding, self.padding, self.padding, self.padding),\n",
                "            mode='constant',\n",
                "            value=0.0\n",
                "        )\n",
                "\n",
                "        return im\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        x = self.preprocess_image(x)\n",
                "        \n",
                "        #plt.imshow(x.squeeze(), cmap=\"gray\")\n",
                "        #plt.show()\n",
                "        \n",
                "        y_hat: torch.Tensor = self.model(x)\n",
                "        y_hat = y_hat.squeeze().squeeze()\n",
                "        \n",
                "        softmax = torch.softmax(y_hat, dim=-1)\n",
                "        \n",
                "        stacked = torch.stack([self.alphabet_float, softmax], dim=1)\n",
                "        sorted_indices = torch.argsort(stacked[:, 1], descending=True)\n",
                "        sorted_tensor = stacked[sorted_indices]\n",
                "\n",
                "        unicodes: torch.Tensor = sorted_tensor[:, 0]\n",
                "        probabilities: torch.Tensor = sorted_tensor[:, 1]\n",
                "\n",
                "        unicodes = unicodes.to(dtype=torch.int)\n",
                "        probabilities = probabilities.detach()\n",
                "\n",
                "        return unicodes.unsqueeze(0), probabilities.unsqueeze(0)\n",
                "        \n",
                "        \n",
                "       \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "cnn_sara"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "AllCNN2D                                 [1, 44]                   --\n",
                        "├─ModuleList: 1-1                        --                        --\n",
                        "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
                        "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
                        "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
                        "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
                        "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
                        "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
                        "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
                        "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
                        "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
                        "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
                        "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
                        "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
                        "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
                        "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
                        "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
                        "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
                        "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
                        "├─Sequential: 1-2                        [1, 128]                  --\n",
                        "│    └─Flatten: 2-6                      [1, 128]                  --\n",
                        "├─ModuleList: 1-3                        --                        --\n",
                        "│    └─Sequential: 2-7                   [1, 64]                   --\n",
                        "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
                        "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
                        "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
                        "│    └─Sequential: 2-8                   [1, 44]                   --\n",
                        "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
                        "==========================================================================================\n",
                        "Total params: 83,548\n",
                        "Trainable params: 83,548\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (M): 14.05\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.02\n",
                        "Forward/backward pass size (MB): 2.18\n",
                        "Params size (MB): 0.33\n",
                        "Estimated Total Size (MB): 2.53\n",
                        "==========================================================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_18944\\1877883219.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, 0, 0], device=im.device)  # Default value if no foreground\n",
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_18944\\1877883219.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, im.size(2) - 1, im.size(3) - 1], device=im.device)  # Default value if no foreground\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:657: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:1127: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to CNN_SARA_LC.onnx\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "alphabet: list[str] = list('∀∃().0123456789:λμabcdefghijklmnopqrstuvwxyz')\n",
                "\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\Mongo_epoch17_trainacc0.9517_valacc0.99432_Tloss0.010203_Vloss0.002475_lr0.0007224.pkl\"\n",
                "model_name: str = \"MongoCNN_Prod\"\n",
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
                "        \"fully_connected_features\": (64, len(alphabet)), \n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cpu\",\n",
                "        \"conv_dropout\": 0.075,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": model_name,\n",
                "    }\n",
                ").eval()\n",
                "\n",
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
                "\n",
                "krud_model: KrudModel = KrudModel(\n",
                "    alphabet,\n",
                "    model=cnn_model\n",
                ")\n",
                "\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "krud_model.forward(dummy_input)\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"CNN_SARA_LC.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    krud_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}, \"unicode\": {0: \"batch\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(1, 44)"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_path = onnx_file_path\n",
                "session = onnxruntime.InferenceSession(model_path)\n",
                "\n",
                "input_image = np.random.random(\n",
                "    (\n",
                "        1,  # batch: stack as many images as you like here\n",
                "        1,  # channels: needs to be 1 (grayscale), pixels are 1.0 or 0.0\n",
                "        64,  # height: fixed to 1 for now\n",
                "        64   # width: fixed to 1 for now\n",
                "    )\n",
                ").astype(np.float32)\n",
                "\n",
                "# Run inference\n",
                "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
                "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
                "\n",
                "input_name: str = inputs[0].name\n",
                "output_names: list[str] = [out.name for out in outputs]\n",
                "softmax: np.ndarray\n",
                "softmax_ordered: np.ndarray\n",
                "logits: np.ndarray\n",
                "\n",
                "unicodes, probs = session.run(\n",
                "    output_names, \n",
                "    {input_name: input_image}\n",
                ")\n",
                "\n",
                "unicodes[0, :3]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "cnn_py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded: encoder_conv_blocks.0.0.weight\n",
                        "Loaded: encoder_conv_blocks.0.0.bias\n",
                        "Loaded: encoder_conv_blocks.0.2.weight\n",
                        "Loaded: encoder_conv_blocks.0.2.bias\n",
                        "Loaded: encoder_conv_blocks.0.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.0.2.running_var\n",
                        "Loaded: encoder_conv_blocks.0.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.0.4.weight\n",
                        "Loaded: encoder_conv_blocks.0.4.bias\n",
                        "Loaded: encoder_conv_blocks.0.6.weight\n",
                        "Loaded: encoder_conv_blocks.0.6.bias\n",
                        "Loaded: encoder_conv_blocks.0.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.0.6.running_var\n",
                        "Loaded: encoder_conv_blocks.0.6.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.1.0.weight\n",
                        "Loaded: encoder_conv_blocks.1.0.bias\n",
                        "Loaded: encoder_conv_blocks.1.2.weight\n",
                        "Loaded: encoder_conv_blocks.1.2.bias\n",
                        "Loaded: encoder_conv_blocks.1.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.1.2.running_var\n",
                        "Loaded: encoder_conv_blocks.1.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.1.4.weight\n",
                        "Loaded: encoder_conv_blocks.1.4.bias\n",
                        "Loaded: encoder_conv_blocks.1.6.weight\n",
                        "Loaded: encoder_conv_blocks.1.6.bias\n",
                        "Loaded: encoder_conv_blocks.1.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.1.6.running_var\n",
                        "Loaded: encoder_conv_blocks.1.6.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.2.0.weight\n",
                        "Loaded: encoder_conv_blocks.2.0.bias\n",
                        "Loaded: encoder_conv_blocks.2.2.weight\n",
                        "Loaded: encoder_conv_blocks.2.2.bias\n",
                        "Loaded: encoder_conv_blocks.2.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.2.2.running_var\n",
                        "Loaded: encoder_conv_blocks.2.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.2.4.weight\n",
                        "Loaded: encoder_conv_blocks.2.4.bias\n",
                        "Loaded: encoder_conv_blocks.2.6.weight\n",
                        "Loaded: encoder_conv_blocks.2.6.bias\n",
                        "Loaded: encoder_conv_blocks.2.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.2.6.running_var\n",
                        "Loaded: encoder_conv_blocks.2.6.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.3.0.weight\n",
                        "Loaded: encoder_conv_blocks.3.0.bias\n",
                        "Loaded: encoder_conv_blocks.3.2.weight\n",
                        "Loaded: encoder_conv_blocks.3.2.bias\n",
                        "Loaded: encoder_conv_blocks.3.2.running_mean\n",
                        "Loaded: encoder_conv_blocks.3.2.running_var\n",
                        "Loaded: encoder_conv_blocks.3.2.num_batches_tracked\n",
                        "Loaded: encoder_conv_blocks.3.4.weight\n",
                        "Loaded: encoder_conv_blocks.3.4.bias\n",
                        "Loaded: encoder_conv_blocks.3.6.weight\n",
                        "Loaded: encoder_conv_blocks.3.6.bias\n",
                        "Loaded: encoder_conv_blocks.3.6.running_mean\n",
                        "Loaded: encoder_conv_blocks.3.6.running_var\n",
                        "Loaded: encoder_conv_blocks.3.6.num_batches_tracked\n",
                        "Loaded: fully_connected_blocks.0.0.weight\n",
                        "Loaded: fully_connected_blocks.0.0.bias\n",
                        "Loaded: fully_connected_blocks.1.0.weight\n",
                        "Loaded: fully_connected_blocks.1.0.bias\n",
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "AllCNN2D                                 [1, 94]                   --\n",
                        "├─ModuleList: 1-1                        --                        --\n",
                        "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
                        "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
                        "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
                        "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
                        "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
                        "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
                        "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
                        "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
                        "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
                        "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
                        "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
                        "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
                        "├─Sequential: 1-2                        [1, 512]                  --\n",
                        "│    └─Flatten: 2-5                      [1, 512]                  --\n",
                        "├─ModuleList: 1-3                        --                        --\n",
                        "│    └─Sequential: 2-6                   [1, 128]                  --\n",
                        "│    │    └─Linear: 3-33                 [1, 128]                  65,664\n",
                        "│    │    └─Dropout: 3-34                [1, 128]                  --\n",
                        "│    │    └─LeakyReLU: 3-35              [1, 128]                  --\n",
                        "│    └─Sequential: 2-7                   [1, 94]                   --\n",
                        "│    │    └─Linear: 3-36                 [1, 94]                   12,126\n",
                        "==========================================================================================\n",
                        "Total params: 131,598\n",
                        "Trainable params: 131,598\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (M): 13.93\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.02\n",
                        "Forward/backward pass size (MB): 2.17\n",
                        "Params size (MB): 0.53\n",
                        "Estimated Total Size (MB): 2.72\n",
                        "==========================================================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_28708\\77278072.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, 0, 0], device=im.device)  # Default value if no foreground\n",
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_28708\\77278072.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, im.size(2) - 1, im.size(3) - 1], device=im.device)  # Default value if no foreground\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:657: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
                        "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\utils.py:1127: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
                        "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to CNN_APPLE_PY.onnx\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "alphabet = list('!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~')\n",
                "\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\z_ApplePie_epoch185_trainacc0.71586_valacc0.74578_Tloss0.063201_Vloss0.060233_lr0.0001673200819949859.pkl\"\n",
                "model_name: str = \"MongoCNN_Prod\"\n",
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32),\n",
                "        \"fully_connected_features\": (128, len(alphabet)),\n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cpu\",\n",
                "        \"conv_dropout\": 0.15,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": \"CNN_py_Apple\",\n",
                "        \"checkpoint_path\": checkpoint_path\n",
                "    }\n",
                ").eval()\n",
                "\n",
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
                "\n",
                "krud_model: KrudModel = KrudModel(\n",
                "    alphabet,\n",
                "    model=cnn_model\n",
                ")\n",
                "\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"CNN_APPLE_PY.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    krud_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "cnn_apl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "AllCNN2D                                 [1, 256]                  --\n",
                        "├─ModuleList: 1-1                        --                        --\n",
                        "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
                        "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
                        "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
                        "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
                        "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
                        "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
                        "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
                        "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
                        "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
                        "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
                        "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
                        "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
                        "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
                        "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
                        "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
                        "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
                        "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
                        "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
                        "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
                        "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
                        "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
                        "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
                        "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
                        "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
                        "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
                        "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
                        "├─Sequential: 1-2                        [1, 512]                  --\n",
                        "│    └─Flatten: 2-5                      [1, 512]                  --\n",
                        "├─ModuleList: 1-3                        --                        --\n",
                        "│    └─Sequential: 2-6                   [1, 256]                  --\n",
                        "│    │    └─Linear: 3-33                 [1, 256]                  131,328\n",
                        "│    │    └─Dropout: 3-34                [1, 256]                  --\n",
                        "│    │    └─LeakyReLU: 3-35              [1, 256]                  --\n",
                        "│    └─Sequential: 2-7                   [1, 256]                  --\n",
                        "│    │    └─Linear: 3-36                 [1, 256]                  65,792\n",
                        "==========================================================================================\n",
                        "Total params: 250,928\n",
                        "Trainable params: 250,928\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (M): 14.05\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.02\n",
                        "Forward/backward pass size (MB): 2.17\n",
                        "Params size (MB): 1.00\n",
                        "Estimated Total Size (MB): 3.20\n",
                        "==========================================================================================\n",
                        "Model saved to CNN_APL_BEST_done.onnx\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_25308\\1877883219.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, 0, 0], device=im.device)  # Default value if no foreground\n",
                        "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_25308\\1877883219.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  torch.tensor([0, 0, im.size(2) - 1, im.size(3) - 1], device=im.device)  # Default value if no foreground\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "alphabet: list[str] = list('!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~£¨¯´·×÷←↑→↓∆∇∊∘√∣∧∨∩∪≠≡≢≤≥⊂⊃⊆⊇⊖⊢⊣⊤⊥⋄⌈⌊⌶⌷⌸⌹⌺⌽⌿⍀⍉⍋⍎⍒⍕⍙⍛⍝⍞⍟⍠⍢⍣⍤⍥⍨⍪⍫⍬⍱⍲⍳⍴⍵⍷⍸⍺⎕○')\n",
                "alphabet = alphabet + [\" \"] * (256 - len(alphabet))\n",
                "\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\20250421_153213__BEST_APL_MODEL_V5__Epoch13_tLoss1.75856_tL153.07858_tMSE38.59373_tAcc0.57354_vLoss1.04812_vL154.07807_vMSE41.83569_vAcc0.70611.pt\"\n",
                "model_name: str = \"APL_Prod_Best\"\n",
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32),\n",
                "        \"fully_connected_features\": (256, 256),\n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cpu\",\n",
                "        \"conv_dropout\": 0.05,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": model_name,\n",
                "        \"frozen_layer_prefixes\": [],\n",
                "    }\n",
                ").eval()\n",
                "\n",
                "\n",
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
                "\n",
                "krud_model: KrudModel = KrudModel(\n",
                "    alphabet,\n",
                "    model=cnn_model\n",
                ")\n",
                "\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "krud_model.forward(dummy_input)\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"CNN_APL_BEST_done.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    krud_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}, \"unicode\": {0: \"batch\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "alphabet: list[str] = list('!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~£¨¯´·×÷←↑→↓∆∇∊∘√∣∧∨∩∪≠≡≢≤≥⊂⊃⊆⊇⊖⊢⊣⊤⊥⋄⌈⌊⌶⌷⌸⌹⌺⌽⌿⍀⍉⍋⍎⍒⍕⍙⍛⍝⍞⍟⍠⍢⍣⍤⍥⍨⍪⍫⍬⍱⍲⍳⍴⍵⍷⍸⍺⎕○')\n",
                "alphabet = alphabet + [\" \"] * (256 - len(alphabet))\n",
                "\n",
                "checkpoint_path: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\20250421_153213__BEST_APL_MODEL_V5__Epoch13_tLoss1.75856_tL153.07858_tMSE38.59373_tAcc0.57354_vLoss1.04812_vL154.07807_vMSE41.83569_vAcc0.70611.pt\"\n",
                "model_name: str = \"APL_Prod_Best\"\n",
                "\n",
                "cnn_model: nn.Module = AllCNN2D(\n",
                "    **{\n",
                "        \"conv_features\": (1, 16, 32, 32, 32),\n",
                "        \"fully_connected_features\": (256, 256),\n",
                "        \"expected_input_size\": (64, 64),\n",
                "        \"device\": \"cpu\",\n",
                "        \"conv_dropout\": 0.05,\n",
                "        \"verbose\": True,\n",
                "        \"name_prefix\": model_name,\n",
                "        \"frozen_layer_prefixes\": [],\n",
                "    }\n",
                ").eval()\n",
                "\n",
                "\n",
                "cnn_model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
                "\n",
                "krud_model: KrudModel = KrudModel(\n",
                "    alphabet,\n",
                "    model=cnn_model\n",
                ")\n",
                "\n",
                "dummy_input = torch.randn(1, 1, 64, 64)  # Example input\n",
                "\n",
                "krud_model.forward(dummy_input)\n",
                "\n",
                "# Define the ONNX file path\n",
                "onnx_file_path = \"CNN_APL_BEST_done.onnx\"\n",
                "\n",
                "# Export the model\n",
                "torch.onnx.export(\n",
                "    krud_model,\n",
                "    dummy_input,\n",
                "    onnx_file_path,\n",
                "    input_names=[\"input\"],  # Name of the input layer\n",
                "    output_names=[\"unicode\", \"probability\"],  # Names of the output layers\n",
                "    dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}, \"unicode\": {0: \"batch\"}},\n",
                "    opset_version=11  # Specify the ONNX opset version\n",
                ")\n",
                "\n",
                "print(f\"Model saved to {onnx_file_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "cnn_onnx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "tuple indices must be integers or slices, not tuple",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m im \u001b[38;5;241m=\u001b[39m (im[:, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      9\u001b[0m im_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(im, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[43mkrud_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem())))\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(im)\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
                        "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
                    ]
                }
            ],
            "source": [
                "dataset_dirpath: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\data\\apl_lora\"\n",
                "for im_filename in os.listdir(dataset_dirpath):\n",
                "    fullpath: str = os.path.join(dataset_dirpath, im_filename)\n",
                "\n",
                "    im = np.asarray(Image.open(fullpath))\n",
                "    \n",
                "    im = (im[:, :, 3] > 0.5).astype(np.uint8).astype(np.float32)\n",
                "    \n",
                "    im_tensor = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
                "\n",
                "    print(chr(int(krud_model.forward(im_tensor)[0, 0].item())))\n",
                "    \n",
                "    plt.imshow(im)\n",
                "    plt.show()\n",
                "\n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Test Krud ONNX"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt \n",
                "import onnxruntime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "input_image = Image.open(\"test_im.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "test_image = np.asarray(input_image)\n",
                "test_image = ((1.0-test_image[:, :, 0]/255) > 0.5).astype(np.uint8)*1.0\n",
                "\n",
                "plt.imshow(test_image, cmap=\"gray\")\n",
                "plt.show()\n",
                "\n",
                "test_image = np.expand_dims(np.expand_dims(test_image, 0), 0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(1, 1, 143, 154)"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_image.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         ...,\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.],\n",
                            "         [0., 0., 0., ..., 0., 0., 0.]]]])"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "⌸ 0.28821778\n",
                        "⌹ 0.25435978\n",
                        "⊖ 0.15252703\n",
                        "@ 0.14573595\n",
                        "⍬ 0.058862936\n",
                        "⍠ 0.039472453\n",
                        "⍟ 0.023810962\n",
                        "⍙ 0.017362919\n",
                        "⌺ 0.015063271\n",
                        "Q 0.0016960067\n",
                        "B 0.0008566621\n",
                        "⍵ 0.0005646281\n",
                        "⊇ 0.0002887322\n",
                        "G 0.00023409257\n",
                        "⍉ 0.00022629506\n",
                        "8 0.0001647064\n",
                        "g 9.368369e-05\n",
                        "& 8.975658e-05\n",
                        "⍫ 6.020118e-05\n",
                        "∆ 5.0487546e-05\n",
                        "a 3.1156014e-05\n",
                        "⍝ 2.2122123e-05\n",
                        "⊆ 2.086064e-05\n",
                        "⍞ 1.8243521e-05\n",
                        "⍋ 1.397234e-05\n",
                        "⍷ 1.3026697e-05\n",
                        "3 1.2116537e-05\n",
                        "s 1.1942984e-05\n",
                        "∊ 1.1737614e-05\n",
                        "⎕ 1.1621817e-05\n",
                        "S 1.0351169e-05\n",
                        "w 9.390529e-06\n",
                        "e 8.838984e-06\n",
                        "d 5.7626726e-06\n",
                        "$ 4.940719e-06\n",
                        "⌽ 4.787412e-06\n",
                        "o 4.738636e-06\n",
                        "6 3.955339e-06\n",
                        "2 3.609199e-06\n",
                        "q 3.5381015e-06\n",
                        "W 3.5288078e-06\n",
                        "⍱ 3.1794166e-06\n",
                        "b 2.7211154e-06\n",
                        "z 2.5932843e-06\n",
                        "5 2.5258655e-06\n",
                        "⌷ 2.1485453e-06\n",
                        "} 1.9275826e-06\n",
                        "⍸ 1.720294e-06\n",
                        "⋄ 1.6161869e-06\n",
                        "E 1.3485228e-06\n",
                        "⍛ 1.3265477e-06\n",
                        "⍢ 1.3077039e-06\n",
                        "4 9.727377e-07\n",
                        "≥ 7.711606e-07\n",
                        "A 7.6530023e-07\n",
                        "0 7.27683e-07\n",
                        "9 7.132206e-07\n",
                        "H 5.872283e-07\n",
                        "R 5.458167e-07\n",
                        "→ 5.107469e-07\n",
                        "⍺ 4.2842666e-07\n",
                        "∘ 2.9059322e-07\n",
                        "P 2.7580458e-07\n",
                        "Z 2.4335517e-07\n",
                        "% 1.9048957e-07\n",
                        "≡ 1.6842804e-07\n",
                        "# 1.4850333e-07\n",
                        "y 1.4227184e-07\n",
                        "≤ 1.3371094e-07\n",
                        "⍥ 1.2664171e-07\n",
                        "∇ 1.1938428e-07\n",
                        "⍴ 1.1197866e-07\n",
                        "⊃ 1.0663711e-07\n",
                        "○ 1.0194305e-07\n",
                        "u 9.830375e-08\n",
                        "← 8.6383224e-08\n",
                        "⍎ 8.084634e-08\n",
                        "D 6.7984786e-08\n",
                        "p 6.618395e-08\n",
                        "⍨ 5.2429566e-08\n",
                        "N 5.0676515e-08\n",
                        "⍤ 4.702728e-08\n",
                        ". 4.6371987e-08\n",
                        "] 2.9736176e-08\n",
                        "h 2.1004903e-08\n",
                        "{ 1.9303805e-08\n",
                        "⍒ 1.7856303e-08\n",
                        "f 1.7645005e-08\n",
                        "; 1.6116662e-08\n",
                        ", 1.3816347e-08\n",
                        "O 1.33103875e-08\n",
                        "j 1.2897941e-08\n",
                        "7 1.214412e-08\n",
                        "~ 9.8091295e-09\n",
                        "⍕ 9.803444e-09\n",
                        "£ 9.768038e-09\n",
                        "≢ 6.5241275e-09\n",
                        "' 4.846572e-09\n",
                        "k 3.01753e-09\n",
                        "? 1.3784148e-09\n",
                        "m 1.238862e-09\n",
                        "> 1.1892973e-09\n",
                        "⍳ 1.1610495e-09\n",
                        "F 1.1270418e-09\n",
                        "= 1.0698353e-09\n",
                        "J 9.771275e-10\n",
                        "i 9.1269065e-10\n",
                        "c 8.1498297e-10\n",
                        "M 7.378292e-10\n",
                        "K 6.1712524e-10\n",
                        "↓ 3.7328585e-10\n",
                        "⍲ 2.2604643e-10\n",
                        "⊣ 2.0897026e-10\n",
                        "` 2.0531457e-10\n",
                        "U 1.9530365e-10\n",
                        "l 1.7846138e-10\n",
                        "⊂ 1.7615227e-10\n",
                        "· 1.4545438e-10\n",
                        "1 1.0493525e-10\n",
                        "v 9.851119e-11\n",
                        "C 9.5091386e-11\n",
                        "n 8.705148e-11\n",
                        "L 8.6003815e-11\n",
                        "≠ 8.210143e-11\n",
                        "- 5.9713484e-11\n",
                        "⍪ 4.492116e-11\n",
                        ") 4.315628e-11\n",
                        "⍣ 4.0605887e-11\n",
                        "! 3.6888045e-11\n",
                        "I 3.3425773e-11\n",
                        "^ 2.8137123e-11\n",
                        "∪ 2.5138482e-11\n",
                        "* 1.6601236e-11\n",
                        "Y 1.6040858e-11\n",
                        "´ 1.58642e-11\n",
                        "⌶ 1.519651e-11\n",
                        "\" 1.4330963e-11\n",
                        "÷ 1.0760268e-11\n",
                        "t 1.0475961e-11\n",
                        "[ 9.840692e-12\n",
                        "V 8.98048e-12\n",
                        "_ 5.7595014e-12\n",
                        "x 5.307369e-12\n",
                        "+ 1.3821406e-12\n",
                        "< 9.34178e-13\n",
                        "| 8.6805834e-13\n",
                        "r 8.62982e-13\n",
                        ": 7.1705467e-13\n",
                        "∨ 3.5812323e-13\n",
                        "↑ 9.220023e-14\n",
                        "( 8.936421e-14\n",
                        "⊢ 7.0249355e-14\n",
                        "X 6.293053e-14\n",
                        "√ 2.6901695e-14\n",
                        "⍀ 2.6491392e-14\n",
                        "⊥ 2.6303892e-14\n",
                        "⌿ 9.6563425e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "  9.0695985e-15\n",
                        "¯ 8.677776e-15\n",
                        "× 6.4424568e-15\n",
                        "T 4.519746e-15\n",
                        "\\ 2.1525756e-15\n",
                        "∩ 2.1378946e-15\n",
                        "⌈ 6.5227233e-16\n",
                        "⌊ 6.296168e-16\n",
                        "/ 5.659198e-16\n",
                        "⊤ 4.111508e-16\n",
                        "∣ 2.5485146e-16\n",
                        "∧ 2.3020487e-16\n",
                        "¨ 6.6118035e-17\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "\n",
                "# Load the ONNX model\n",
                "model_path = \"cnn_apl_final.onnx\"\n",
                "session = onnxruntime.InferenceSession(model_path)\n",
                "\n",
                "# Run inference\n",
                "inputs: list[onnxruntime.NodeArg] = session.get_inputs()\n",
                "outputs: list[onnxruntime.NodeArg] = session.get_outputs()\n",
                "\n",
                "input_name: str = inputs[0].name            # Only one input \n",
                "output_names: list[str] = [output.name for output in outputs]    # Only one output [unicode, prob] pairs for a single character\n",
                "\n",
                "unicodes: torch.Tensor\n",
                "probabilities: torch.Tensor\n",
                "unicodes, probabilities = session.run(\n",
                "    output_names, \n",
                "    {input_name: test_image.astype(np.float32)} # Must be float32 image of shape (batch, channels, height, width) -> (1, 1, Any, Any)\n",
                ")\n",
                "\n",
                "\n",
                "for char, prob in zip(unicodes[0], probabilities[0]):\n",
                "    print(chr(int(char)), prob)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[array([ 48, 112, 100,  54, 116,  56,  53, 111, 113, 102,  98,  40, 119,\n",
                            "        109, 107, 106, 104, 110,  50,  99, 120, 114, 215,  51,  45,  49,\n",
                            "        118,  57, 103, 122, 105, 101,  41, 115, 117, 121,  55,  97,  46,\n",
                            "         52, 108, 247, 955,  43], dtype=int32),\n",
                            " array([6.8143886e-01, 3.1727558e-01, 9.8913838e-04, 1.6000890e-04,\n",
                            "        5.1013758e-05, 2.8218346e-05, 2.4816827e-05, 8.7805711e-06,\n",
                            "        5.5376886e-06, 3.6561419e-06, 3.0133053e-06, 2.6766511e-06,\n",
                            "        2.4739402e-06, 1.5798223e-06, 1.5708143e-06, 8.6630882e-07,\n",
                            "        6.8614111e-07, 5.5757562e-07, 4.7443012e-07, 3.3036901e-07,\n",
                            "        6.3471326e-08, 2.9459986e-08, 2.2759442e-08, 1.8600712e-08,\n",
                            "        1.8000391e-08, 1.2019766e-08, 3.3489278e-09, 3.0067731e-09,\n",
                            "        2.6663645e-09, 2.0556845e-09, 1.1345208e-09, 3.8724163e-10,\n",
                            "        2.8197075e-10, 1.6642417e-10, 3.9026893e-11, 1.4455865e-11,\n",
                            "        1.0275350e-11, 8.5388979e-12, 2.8815811e-13, 2.1714051e-13,\n",
                            "        4.0094240e-14, 2.7764098e-14, 2.0496294e-15, 9.2305130e-16],\n",
                            "       dtype=float32)]"
                        ]
                    },
                    "execution_count": 68,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "tuple indices must be integers or slices, not tuple",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mkrud_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m [\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;28mint\u001b[39m(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m preds]\n",
                        "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
                    ]
                }
            ],
            "source": [
                "preds = krud_model.eval().forward(torch.tensor(test_image.astype(np.float32)))[:, 0].tolist()\n",
                "\n",
                "[chr(int(x)) for x in preds]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
