{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.train import EpochLogs, grid_search, LogPoint\n",
    "from dataset.character_dataset import CharImageDataset\n",
    "from models.allcnn2d import AllCNN2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir: str = os.path.abspath(\".\")\n",
    "data_dir: str = os.path.join(\n",
    "    notebook_dir,\n",
    "    os.pardir,\n",
    "    os.pardir,\n",
    "    \"data\",\n",
    "    \"lambda\"\n",
    ")\n",
    "\n",
    "file_names: list[str] = [\n",
    "    \"u3bb-1736974468605.png\",\n",
    "    \"u2e-1736974503521.png\",\n",
    "    \"u31-1736974480810.png\"\n",
    "]\n",
    "\n",
    "labels: list[str] = [\n",
    "    \"u3bb\",\n",
    "    \"u2e\",\n",
    "    \"u31\"\n",
    "]\n",
    "\n",
    "file_paths: list[str] = [\n",
    "    os.path.join(\n",
    "        data_dir,\n",
    "        file_name\n",
    "    ) for file_name in\n",
    "    file_names\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "char_dataset: CharImageDataset = CharImageDataset(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    rotation_limit=0.1,\n",
    "    translation_limit=0.2,\n",
    "    skew_limit=0.05,\n",
    "    zoom_change=0.3,\n",
    "    image_dims=(64, 64)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for im, lab in char_dataset:\n",
    "    plt.imshow(rearrange(im, \"1 h w -> h w\")*255, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(lab)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader: DataLoader = DataLoader(\n",
    "    char_dataset,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_model_parameters: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"conv_features\": (1, 16, 16),\n",
    "        \"fully_connected_features\": (3,),\n",
    "        \"expected_input_size\": (64, 64),\n",
    "        \"device\": \"cuda\",\n",
    "        \"verbose\": False\n",
    "    }\n",
    "]\n",
    "\n",
    "all_optim_parameters: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"lr\": 0.000001,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 17.32it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 16.85it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 17.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 16.17it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 17.01it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 19.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 17.96it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 18.29it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 18.06it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 17.75it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 17.14it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 17.54it/s]\n",
      "Validating Model...: 100%|██████████| 3/3 [00:00<00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 3/3 [00:00<00:00, 17.44it/s]\n",
      "Validating Model...:  33%|███▎      | 1/3 [00:00<00:00, 13.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      3\u001b[0m     log: EpochLogs\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m grid_search(\n\u001b[0;32m      5\u001b[0m         model_factor\u001b[38;5;241m=\u001b[39mAllCNN2D,\n\u001b[0;32m      6\u001b[0m         all_model_parameters\u001b[38;5;241m=\u001b[39mall_model_parameters,\n\u001b[0;32m      7\u001b[0m         optim_factory\u001b[38;5;241m=\u001b[39mAdamW,\n\u001b[0;32m      8\u001b[0m         all_optim_params\u001b[38;5;241m=\u001b[39mall_optim_parameters,\n\u001b[0;32m      9\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     10\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),\n\u001b[0;32m     11\u001b[0m         train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     12\u001b[0m         val_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     13\u001b[0m         lr_decay_window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     14\u001b[0m         lr_decay_minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     15\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m     ):\n\u001b[0;32m     17\u001b[0m         train_logpoints: \u001b[38;5;28mlist\u001b[39m[LogPoint] \u001b[38;5;241m=\u001b[39m log\u001b[38;5;241m.\u001b[39mtrain_logs\n\u001b[0;32m     19\u001b[0m         log_point: LogPoint \u001b[38;5;241m=\u001b[39m train_logpoints[\u001b[38;5;241m0\u001b[39m] \n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\training\\train.py:367\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(model_factor, all_model_parameters, optim_factory, all_optim_params, epochs, criterion, train_dataloader, val_dataloader, lr_decay_window_size, lr_decay_minimum, scheduler_scale, device, compile_model)\u001b[0m\n\u001b[0;32m    364\u001b[0m     loss_window\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    366\u001b[0m val_log: LogPoint\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_log \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    368\u001b[0m     iterable\u001b[38;5;241m=\u001b[39mevaluate(\n\u001b[0;32m    369\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    370\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m    371\u001b[0m         eval_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[0;32m    372\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    373\u001b[0m     ),\n\u001b[0;32m    374\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    375\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_dataloader)\n\u001b[0;32m    376\u001b[0m ):\n\u001b[0;32m    377\u001b[0m     epoch_val_logs\u001b[38;5;241m.\u001b[39mappend(val_log)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m EpochLogs(\n\u001b[0;32m    380\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    381\u001b[0m     optimiser\u001b[38;5;241m=\u001b[39mmodel_optimiser,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m     val_logs\u001b[38;5;241m=\u001b[39mepoch_val_logs\n\u001b[0;32m    385\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\training\\train.py:222\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, criterion, eval_dataloader, device)\u001b[0m\n\u001b[0;32m    219\u001b[0m X: Tensor\n\u001b[0;32m    220\u001b[0m y: Tensor\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m eval_dataloader:\n\u001b[0;32m    224\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    225\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\dataset\\character_dataset.py:88\u001b[0m, in \u001b[0;36mCharImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     85\u001b[0m image \u001b[38;5;241m=\u001b[39m reduce(image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc h w -> 1 h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Apply random transformations\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_random_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# One-hot encode the label\u001b[39;00m\n\u001b[0;32m     93\u001b[0m one_hot_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels),\n\u001b[0;32m     95\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m     96\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\dataset\\character_dataset.py:119\u001b[0m, in \u001b[0;36mCharImageDataset._apply_random_transformations\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Random rotation\u001b[39;00m\n\u001b[0;32m    115\u001b[0m rotation_angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_limit\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m365\u001b[39m,\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_limit\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m365\u001b[39m\n\u001b[0;32m    118\u001b[0m )\n\u001b[1;32m--> 119\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrotation_angle\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Random translation\u001b[39;00m\n\u001b[0;32m    125\u001b[0m max_dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation_limit \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1140\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# we need to set -angle.\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m matrix \u001b[38;5;241m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[38;5;241m-\u001b[39mangle, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], \u001b[38;5;241m1.0\u001b[39m, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m])\n\u001b[1;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:669\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[0;32m    667\u001b[0m grid \u001b[38;5;241m=\u001b[39m _gen_affine_grid(theta, w\u001b[38;5;241m=\u001b[39mw, h\u001b[38;5;241m=\u001b[39mh, ow\u001b[38;5;241m=\u001b[39mow, oh\u001b[38;5;241m=\u001b[39moh)\n\u001b[1;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_grid_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:560\u001b[0m, in \u001b[0;36m_apply_grid_transform\u001b[1;34m(img, grid, mode, fill)\u001b[0m\n\u001b[0;32m    557\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    558\u001b[0m     img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((img, mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 560\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzeros\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Fill with required color\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:4304\u001b[0m, in \u001b[0;36mgrid_sample\u001b[1;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[0;32m   4296\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   4297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign_corners=True if the old behavior is desired. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee the documentation of grid_sample for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4301\u001b[0m     )\n\u001b[0;32m   4302\u001b[0m     align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "\n",
    "    log: EpochLogs\n",
    "    for log in grid_search(\n",
    "        model_factor=AllCNN2D,\n",
    "        all_model_parameters=all_model_parameters,\n",
    "        optim_factory=AdamW,\n",
    "        all_optim_params=all_optim_parameters,\n",
    "        epochs=100,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=train_dataloader,\n",
    "        lr_decay_window_size=10,\n",
    "        lr_decay_minimum=0.0,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        train_logpoints: list[LogPoint] = log.train_logs\n",
    "        \n",
    "        log_point: LogPoint = train_logpoints[0] \n",
    "        \n",
    "        y_hat_pred: torch.Tensor = torch.argmax(log_point.y_hat)\n",
    "        y_pred: torch.Tensor = torch.argmax(log_point.y)\n",
    "        \n",
    "        print(y_hat_pred.detach().cpu().item(), y_pred.detach().cpu().item())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
