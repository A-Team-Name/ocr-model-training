{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from typing import Any\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.train import EpochLogs, grid_search, LogPoint\n",
    "from dataset.character_dataset import CharImageDataset\n",
    "from models.allcnn2d import AllCNN2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SPLIT: float = 0.8\n",
    "MODEL_NAME: str = \"Krud\"\n",
    "LOAD_CHECKPOINT: str = r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\checkpoints\\Jigsaw_epoch12_trainacc0.92045_valacc0.96591_Tloss0.015995_Vloss0.0063332_lr0.0007224.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths\n",
    "Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir: str = os.path.abspath(\".\")\n",
    "root_dir: str = os.path.join(\n",
    "    notebook_dir,\n",
    "    os.pardir,\n",
    "    os.pardir\n",
    ")\n",
    "\n",
    "data_dir: str = os.path.join(\n",
    "    root_dir,\n",
    "    \"data\",\n",
    "    \"lambda\"\n",
    ")\n",
    "\n",
    "model_save_dirpath: str = os.path.join(\n",
    "    root_dir,\n",
    "    \"models\",\n",
    "    \"allcnn\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "images_path_glob: str = os.path.join(\n",
    "    data_dir,\n",
    "    \"u*.png\"\n",
    ")\n",
    "images_path_regex: str = r\"(u[0-9a-f]+)-([0-9]+)\\.png\"\n",
    "\n",
    "image_paths: list[str] = glob.glob(images_path_glob)\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time() -> str:\n",
    "    return datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging_path: str = f\"{MODEL_NAME}_log_{get_current_time()}.csv\"\n",
    "\n",
    "with open(logging_path, \"w\") as f:\n",
    "    f.write(\"TIME,EPOCH,TRAIN_ACC,VAL_ACC,TRAIN_LOSS,VAL_LOSS,LR\\n\")\n",
    "\n",
    "def log(\n",
    "    epoch: int,\n",
    "    train_acc: float, \n",
    "    train_loss: float, \n",
    "    val_acc: float, \n",
    "    val_loss, \n",
    "    lr: float\n",
    ")-> None:\n",
    "    with open(logging_path, \"a\") as f:\n",
    "        f.write(f\"{get_current_time()},{epoch},{train_acc},{val_acc},{train_loss},{val_loss},{lr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Labels From File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1104, 1104)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "labeled_image_paths: list[tuple[str, str]] = []\n",
    "\n",
    "class_counts: dict[str, int] = defaultdict(lambda: 0)\n",
    "\n",
    "file_path: str\n",
    "for file_path in image_paths:\n",
    "    match = re.search(images_path_regex, file_path)\n",
    "    \n",
    "    if not match:\n",
    "        continue\n",
    "    \n",
    "    u_hexvalue: str = match.group(1)\n",
    "    \n",
    "    class_counts[u_hexvalue] += 1\n",
    "    \n",
    "    \n",
    "    labeled_image_paths.append((u_hexvalue, file_path))\n",
    "\n",
    "labels: list[str]\n",
    "image_paths: list[str]\n",
    "labels, image_paths =  list(zip(*labeled_image_paths))\n",
    "\n",
    "len(labels), len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'u61': 54,\n",
       "         'u2b': 54,\n",
       "         'u31': 54,\n",
       "         'u3bb': 49,\n",
       "         'u2e': 49,\n",
       "         'u28': 48,\n",
       "         'u29': 48,\n",
       "         'u2d': 23,\n",
       "         'uf7': 23,\n",
       "         'ud7': 22,\n",
       "         'u6f': 20,\n",
       "         'u30': 20,\n",
       "         'u73': 20,\n",
       "         'u69': 20,\n",
       "         'u63': 20,\n",
       "         'u6a': 20,\n",
       "         'u67': 20,\n",
       "         'u65': 20,\n",
       "         'u77': 20,\n",
       "         'u6c': 20,\n",
       "         'u6e': 20,\n",
       "         'u64': 20,\n",
       "         'u36': 20,\n",
       "         'u76': 20,\n",
       "         'u70': 20,\n",
       "         'u6d': 20,\n",
       "         'u79': 20,\n",
       "         'u74': 20,\n",
       "         'u33': 20,\n",
       "         'u35': 20,\n",
       "         'u71': 20,\n",
       "         'u34': 20,\n",
       "         'u78': 20,\n",
       "         'u6b': 20,\n",
       "         'u68': 20,\n",
       "         'u38': 20,\n",
       "         'u32': 20,\n",
       "         'u75': 20,\n",
       "         'u39': 20,\n",
       "         'u72': 20,\n",
       "         'u7a': 20,\n",
       "         'u62': 20,\n",
       "         'u37': 20,\n",
       "         'u66': 20})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Classes Using Oversample/Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_counts: list[tuple[str, int]] = sorted(\n",
    "    class_counts.items(), \n",
    "    key=lambda x: x[0]\n",
    ")\n",
    "\n",
    "counts: list[int] = [pair[1] for pair in sorted_counts]\n",
    "\n",
    "max_count: int = max(counts)\n",
    "min_count: int = min(counts)\n",
    "\n",
    "to_add_counts: dict[str, int] = {\n",
    "    uid: max_count - count \n",
    "    for uid, count in \n",
    "    sorted_counts\n",
    "}\n",
    "\n",
    "to_undersample_counts: dict[str, int] = {\n",
    "    uid: min_count\n",
    "    for uid, count in \n",
    "    sorted_counts\n",
    "}\n",
    "\n",
    "total_items = sum(x[1] for x in sorted_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 54, 1104)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count, max_count, total_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(*to_add_counts.items())\n",
    "#print(sorted([(chr(int(pair[0][1:], 16)), pair[1]) for pair in to_remove_counts.items()], key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_add_labels: list[str] = []\n",
    "#to_add_file_paths: list[str] = []\n",
    "#\n",
    "#while True in [to_add_count>0 for to_add_count in to_add_counts.values()]:  \n",
    "#    for label, image_path in zip(labels, image_paths):\n",
    "#        remaining: int = to_add_counts[label]\n",
    "#        \n",
    "#        if remaining > 0:\n",
    "#            to_add_labels.append(label)\n",
    "#            to_add_file_paths.append(image_path)\n",
    "#            to_add_counts[label] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep_labels: list[str] = []\n",
    "to_keep_file_paths: list[str] = []\n",
    "\n",
    "while True in [to_add_count>0 for to_add_count in to_undersample_counts.values()]:  \n",
    "    for label, image_path in zip(labels, image_paths):\n",
    "        remaining: int = to_undersample_counts[label]\n",
    "        \n",
    "        if remaining > 0:\n",
    "            to_keep_labels.append(label)\n",
    "            to_keep_file_paths.append(image_path)\n",
    "            to_undersample_counts[label] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'u3bb': 20,\n",
       "         'u6f': 20,\n",
       "         'u30': 20,\n",
       "         'u73': 20,\n",
       "         'u61': 20,\n",
       "         'u69': 20,\n",
       "         'u63': 20,\n",
       "         'u6a': 20,\n",
       "         'u67': 20,\n",
       "         'u28': 20,\n",
       "         'u65': 20,\n",
       "         'ud7': 20,\n",
       "         'u77': 20,\n",
       "         'u6c': 20,\n",
       "         'u6e': 20,\n",
       "         'u2e': 20,\n",
       "         'u64': 20,\n",
       "         'u36': 20,\n",
       "         'u2d': 20,\n",
       "         'u76': 20,\n",
       "         'u70': 20,\n",
       "         'u6d': 20,\n",
       "         'u79': 20,\n",
       "         'u74': 20,\n",
       "         'u33': 20,\n",
       "         'u35': 20,\n",
       "         'u71': 20,\n",
       "         'u34': 20,\n",
       "         'u78': 20,\n",
       "         'u6b': 20,\n",
       "         'uf7': 20,\n",
       "         'u68': 20,\n",
       "         'u38': 20,\n",
       "         'u2b': 20,\n",
       "         'u32': 20,\n",
       "         'u31': 20,\n",
       "         'u29': 20,\n",
       "         'u75': 20,\n",
       "         'u39': 20,\n",
       "         'u72': 20,\n",
       "         'u7a': 20,\n",
       "         'u62': 20,\n",
       "         'u37': 20,\n",
       "         'u66': 20})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Counter(to_keep_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880, 880)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths = to_keep_file_paths\n",
    "labels = to_keep_labels\n",
    "len(image_paths), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_index: int = int(len(image_paths)*DATASET_SPLIT)\n",
    "all_label_classes: list[str] = sorted(list(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_char_dataset: CharImageDataset = CharImageDataset(\n",
    "    file_paths=image_paths[:split_index], #list(image_paths[:split_index]) + to_add_file_paths,\n",
    "    labels=labels[:split_index], #list(labels[:split_index]) + to_add_labels,\n",
    "    all_label_classes=all_label_classes,\n",
    "    rotation_limit=0.05,\n",
    "    translation_limit=0.2,\n",
    "    skew_limit=0.02,\n",
    "    zoom_change=1.2,\n",
    "    min_zoom=0.8,\n",
    "    thicken_sigma=-4.9,\n",
    "    image_dims=(64, 64)\n",
    ")\n",
    "\n",
    "val_char_dataset: CharImageDataset = CharImageDataset(\n",
    "    file_paths=image_paths[split_index:],\n",
    "    labels=labels[split_index:],\n",
    "    all_label_classes=all_label_classes,\n",
    "    rotation_limit=0.0,\n",
    "    translation_limit=0.0,\n",
    "    skew_limit=0.00,\n",
    "    zoom_change=0.0,\n",
    "    min_zoom=1.0,\n",
    "    thicken_sigma=0.0,\n",
    "    image_dims=(64, 64)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for im, lab in train_char_dataset:\n",
    "    plt.imshow(\n",
    "        rearrange(im, \"1 h w -> h w\")*255, \n",
    "        cmap=\"gray\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(lab)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for im, lab in val_char_dataset:\n",
    "    print(im.shape)\n",
    "    plt.imshow(rearrange(im, \"1 h w -> h w\")*255, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(lab)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader: DataLoader = DataLoader(\n",
    "    train_char_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "val_dataloader: DataLoader = DataLoader(\n",
    "    val_char_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_counts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_model_parameters: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
    "        \"fully_connected_features\": (64, 44),\n",
    "        \"expected_input_size\": (64, 64),\n",
    "        \"device\": \"cuda\",\n",
    "        \"conv_dropout\": 0.075,\n",
    "        \"verbose\": True,\n",
    "        \"name_prefix\": MODEL_NAME,\n",
    "        \"checkpoint_path\": LOAD_CHECKPOINT\n",
    "    }\n",
    "]\n",
    "\n",
    "all_optim_parameters: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"lr\": 0.0007224,\n",
    "        \"weight_decay\": 0.000001\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: encoder_conv_blocks.0.0.weight\n",
      "Loaded: encoder_conv_blocks.0.0.bias\n",
      "Loaded: encoder_conv_blocks.0.2.weight\n",
      "Loaded: encoder_conv_blocks.0.2.bias\n",
      "Loaded: encoder_conv_blocks.0.2.running_mean\n",
      "Loaded: encoder_conv_blocks.0.2.running_var\n",
      "Loaded: encoder_conv_blocks.0.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.0.4.weight\n",
      "Loaded: encoder_conv_blocks.0.4.bias\n",
      "Loaded: encoder_conv_blocks.0.6.weight\n",
      "Loaded: encoder_conv_blocks.0.6.bias\n",
      "Loaded: encoder_conv_blocks.0.6.running_mean\n",
      "Loaded: encoder_conv_blocks.0.6.running_var\n",
      "Loaded: encoder_conv_blocks.0.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.1.0.weight\n",
      "Loaded: encoder_conv_blocks.1.0.bias\n",
      "Loaded: encoder_conv_blocks.1.2.weight\n",
      "Loaded: encoder_conv_blocks.1.2.bias\n",
      "Loaded: encoder_conv_blocks.1.2.running_mean\n",
      "Loaded: encoder_conv_blocks.1.2.running_var\n",
      "Loaded: encoder_conv_blocks.1.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.1.4.weight\n",
      "Loaded: encoder_conv_blocks.1.4.bias\n",
      "Loaded: encoder_conv_blocks.1.6.weight\n",
      "Loaded: encoder_conv_blocks.1.6.bias\n",
      "Loaded: encoder_conv_blocks.1.6.running_mean\n",
      "Loaded: encoder_conv_blocks.1.6.running_var\n",
      "Loaded: encoder_conv_blocks.1.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.2.0.weight\n",
      "Loaded: encoder_conv_blocks.2.0.bias\n",
      "Loaded: encoder_conv_blocks.2.2.weight\n",
      "Loaded: encoder_conv_blocks.2.2.bias\n",
      "Loaded: encoder_conv_blocks.2.2.running_mean\n",
      "Loaded: encoder_conv_blocks.2.2.running_var\n",
      "Loaded: encoder_conv_blocks.2.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.2.4.weight\n",
      "Loaded: encoder_conv_blocks.2.4.bias\n",
      "Loaded: encoder_conv_blocks.2.6.weight\n",
      "Loaded: encoder_conv_blocks.2.6.bias\n",
      "Loaded: encoder_conv_blocks.2.6.running_mean\n",
      "Loaded: encoder_conv_blocks.2.6.running_var\n",
      "Loaded: encoder_conv_blocks.2.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.3.0.weight\n",
      "Loaded: encoder_conv_blocks.3.0.bias\n",
      "Loaded: encoder_conv_blocks.3.2.weight\n",
      "Loaded: encoder_conv_blocks.3.2.bias\n",
      "Loaded: encoder_conv_blocks.3.2.running_mean\n",
      "Loaded: encoder_conv_blocks.3.2.running_var\n",
      "Loaded: encoder_conv_blocks.3.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.3.4.weight\n",
      "Loaded: encoder_conv_blocks.3.4.bias\n",
      "Loaded: encoder_conv_blocks.3.6.weight\n",
      "Loaded: encoder_conv_blocks.3.6.bias\n",
      "Loaded: encoder_conv_blocks.3.6.running_mean\n",
      "Loaded: encoder_conv_blocks.3.6.running_var\n",
      "Loaded: encoder_conv_blocks.3.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.4.0.weight\n",
      "Loaded: encoder_conv_blocks.4.0.bias\n",
      "Loaded: encoder_conv_blocks.4.2.weight\n",
      "Loaded: encoder_conv_blocks.4.2.bias\n",
      "Loaded: encoder_conv_blocks.4.2.running_mean\n",
      "Loaded: encoder_conv_blocks.4.2.running_var\n",
      "Loaded: encoder_conv_blocks.4.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.4.4.weight\n",
      "Loaded: encoder_conv_blocks.4.4.bias\n",
      "Loaded: encoder_conv_blocks.4.6.weight\n",
      "Loaded: encoder_conv_blocks.4.6.bias\n",
      "Loaded: encoder_conv_blocks.4.6.running_mean\n",
      "Loaded: encoder_conv_blocks.4.6.running_var\n",
      "Loaded: encoder_conv_blocks.4.6.num_batches_tracked\n",
      "Loaded: fully_connected_blocks.0.0.weight\n",
      "Loaded: fully_connected_blocks.0.0.bias\n",
      "Loaded: fully_connected_blocks.1.0.weight\n",
      "Loaded: fully_connected_blocks.1.0.bias\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AllCNN2D                                 [1, 44]                   --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
      "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
      "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
      "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
      "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
      "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
      "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
      "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
      "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
      "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
      "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
      "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
      "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
      "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
      "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
      "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
      "├─Sequential: 1-2                        [1, 128]                  --\n",
      "│    └─Flatten: 2-6                      [1, 128]                  --\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─Sequential: 2-7                   [1, 64]                   --\n",
      "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
      "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
      "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
      "│    └─Sequential: 2-8                   [1, 44]                   --\n",
      "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
      "==========================================================================================\n",
      "Total params: 83,548\n",
      "Trainable params: 83,548\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 14.05\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.18\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 2.53\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:50<00:00,  3.87s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 42, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.8707386255264282\n",
      "Val Accuracy        : 0.9545454382896423\n",
      "Loss                : 0.028944937512278557\n",
      "Val Loss            : 0.008107868023216724\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:51<00:00,  3.90s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13,  0, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 42, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41,  5, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28,  0, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 42, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.8778409361839294\n",
      "Val Accuracy        : 0.9318181872367859\n",
      "Loss                : 0.024190621450543404\n",
      "Val Loss            : 0.010419491678476334\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:51<00:00,  3.89s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13,  0, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 41, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9048295617103577\n",
      "Val Accuracy        : 0.9602272510528564\n",
      "Loss                : 0.019855527207255363\n",
      "Val Loss            : 0.008727794513106346\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:54<00:00,  3.97s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9, 30, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.8991477489471436\n",
      "Val Accuracy        : 0.9545454382896423\n",
      "Loss                : 0.021736083552241325\n",
      "Val Loss            : 0.0073296548798680305\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:51<00:00,  3.90s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 12,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14,  0,  0, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 12, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.8977272510528564\n",
      "Val Accuracy        : 0.9659090638160706\n",
      "Loss                : 0.023282861337065697\n",
      "Val Loss            : 0.008766192942857742\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:52<00:00,  3.91s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32, 30, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38, 30, 14, 42, 22, 23, 39,  8, 25, 11, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41, 30, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 41, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.8920454382896423\n",
      "Val Accuracy        : 0.9261363744735718\n",
      "Loss                : 0.02165883034467697\n",
      "Val Loss            : 0.01104749459773302\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:52<00:00,  3.93s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  4, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  4, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28,  0, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 41, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.8934659361839294\n",
      "Val Accuracy        : 0.9431818127632141\n",
      "Loss                : 0.022655045613646507\n",
      "Val Loss            : 0.009322918951511383\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:58<00:00,  4.05s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 37, 13, 35, 10, 14, 22, 13, 11, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  4, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  4, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 41, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9076704382896423\n",
      "Val Accuracy        : 0.9375\n",
      "Loss                : 0.01914723962545395\n",
      "Val Loss            : 0.010010840371251106\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:47<00:00,  3.82s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41,  5, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 43,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 41, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9076704382896423\n",
      "Val Accuracy        : 0.9431818127632141\n",
      "Loss                : 0.017007101327180862\n",
      "Val Loss            : 0.008886895142495632\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:53<00:00,  3.94s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9076704382896423\n",
      "Val Accuracy        : 0.9715909361839294\n",
      "Loss                : 0.01655096560716629\n",
      "Val Loss            : 0.005544905085116625\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:48<00:00,  3.83s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41,  5, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 42,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9161931872367859\n",
      "Val Accuracy        : 0.9488636255264282\n",
      "Loss                : 0.01730070821940899\n",
      "Val Loss            : 0.007293393835425377\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:53<00:00,  3.94s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38, 30, 14, 42, 22, 23, 39,  8, 25, 11, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  4, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 42,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9133522510528564\n",
      "Val Accuracy        : 0.9431818127632141\n",
      "Loss                : 0.01802901178598404\n",
      "Val Loss            : 0.007067121099680662\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:50<00:00,  3.87s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([37, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41,  5, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9048295617103577\n",
      "Val Accuracy        : 0.9545454382896423\n",
      "Loss                : 0.021504543721675873\n",
      "Val Loss            : 0.0073272972367703915\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:53<00:00,  3.94s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13,  0, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38, 30, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9190340638160706\n",
      "Val Accuracy        : 0.9545454382896423\n",
      "Loss                : 0.018034351989626884\n",
      "Val Loss            : 0.006022978108376265\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:58<00:00,  4.05s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:30<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41,  5, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9190340638160706\n",
      "Val Accuracy        : 0.9602272510528564\n",
      "Loss                : 0.01606503315269947\n",
      "Val Loss            : 0.007405856624245644\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:53<00:00,  3.95s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13,  0, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41,  5, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28,  0, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14,  0,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9303977489471436\n",
      "Val Accuracy        : 0.9488636255264282\n",
      "Loss                : 0.015561624430119991\n",
      "Val Loss            : 0.007845974527299404\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:03<00:00,  4.17s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:30<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  4, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.921875\n",
      "Val Accuracy        : 0.9602272510528564\n",
      "Loss                : 0.01590109057724476\n",
      "Val Loss            : 0.006143011152744293\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:58<00:00,  4.05s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:30<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 11, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9289772510528564\n",
      "Val Accuracy        : 0.9715909361839294\n",
      "Loss                : 0.013525838032364845\n",
      "Val Loss            : 0.0040459465235471725\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:53<00:00,  3.94s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28,  0, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14,  0,  0, 17,  0, 42, 41, 40, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9275568127632141\n",
      "Val Accuracy        : 0.9659090638160706\n",
      "Loss                : 0.012581967748701572\n",
      "Val Loss            : 0.005751353222876787\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:52<00:00,  3.92s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9360795617103577\n",
      "Val Accuracy        : 0.9659090638160706\n",
      "Loss                : 0.01099281944334507\n",
      "Val Loss            : 0.005015610251575708\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:51<00:00,  3.89s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9318181872367859\n",
      "Val Accuracy        : 0.9659090638160706\n",
      "Loss                : 0.013400179333984852\n",
      "Val Loss            : 0.004863290581852198\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:53<00:00,  3.95s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 42,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9247159361839294\n",
      "Val Accuracy        : 0.9715909361839294\n",
      "Loss                : 0.014694317243993282\n",
      "Val Loss            : 0.0047571538016200066\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:44<00:00,  3.74s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38, 30, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9488636255264282\n",
      "Val Accuracy        : 0.9488636255264282\n",
      "Loss                : 0.00961695984005928\n",
      "Val Loss            : 0.006585988681763411\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:48<00:00,  3.83s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:28<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9488636255264282\n",
      "Val Accuracy        : 0.9659090638160706\n",
      "Loss                : 0.011712751351296902\n",
      "Val Loss            : 0.005306145176291466\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:52<00:00,  3.92s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9389204382896423\n",
      "Val Accuracy        : 0.9659090638160706\n",
      "Loss                : 0.013279104605317116\n",
      "Val Loss            : 0.005163948517292738\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [02:54<00:00,  3.97s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:30<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28,  2,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28,  0, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9261363744735718\n",
      "Val Accuracy        : 0.9602272510528564\n",
      "Loss                : 0.015176245011389256\n",
      "Val Loss            : 0.005367562640458345\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:02<00:00,  4.16s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:29<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 42,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9375\n",
      "Val Accuracy        : 0.9659090638160706\n",
      "Loss                : 0.013403750024735928\n",
      "Val Loss            : 0.004974705632776022\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:04<00:00,  4.19s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:31<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9275568127632141\n",
      "Val Accuracy        : 0.9829545617103577\n",
      "Loss                : 0.012193783186376095\n",
      "Val Loss            : 0.00397457042708993\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:06<00:00,  4.25s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:31<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 11, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 11, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9275568127632141\n",
      "Val Accuracy        : 0.9602272510528564\n",
      "Loss                : 0.01511025708168745\n",
      "Val Loss            : 0.005999712739139795\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:08<00:00,  4.29s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:31<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25, 30, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 12,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9346590638160706\n",
      "Val Accuracy        : 0.9715909361839294\n",
      "Loss                : 0.01313154399394989\n",
      "Val Loss            : 0.005667677149176598\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:02<00:00,  4.15s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:32<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 42,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17, 30]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9346590638160706\n",
      "Val Accuracy        : 0.9715909361839294\n",
      "Loss                : 0.01445234939455986\n",
      "Val Loss            : 0.004832873120903969\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:09<00:00,  4.31s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:32<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 12,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9375\n",
      "Val Accuracy        : 0.9545454382896423\n",
      "Loss                : 0.014180576428771019\n",
      "Val Loss            : 0.006863351445645094\n",
      "Learning Rate       : 0.0007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:07<00:00,  4.26s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:30<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13,  0, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14,  0,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9261363744735718\n",
      "Val Accuracy        : 0.9602272510528564\n",
      "Loss                : 0.01454642042517662\n",
      "Val Loss            : 0.006260852329432964\n",
      "Learning Rate       : 0.0006140399999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...: 100%|██████████| 44/44 [03:07<00:00,  4.27s/it]\n",
      "Validating Model...: 100%|██████████| 11/11 [00:31<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 18, 35, 29, 18, 18, 42]) tensor([22, 22, 42, 26, 13, 35, 10, 14, 22, 13, 35, 35, 29, 18, 18, 42])\n",
      "tensor([42, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9]) tensor([40, 33, 34, 23, 43, 32,  9,  5, 40, 25,  5, 32,  5, 17, 19,  9])\n",
      "tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33]) tensor([38,  5, 14, 42, 22, 23, 39,  8, 25, 24, 41, 38, 35, 13, 20, 33])\n",
      "tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 34, 37]) tensor([39,  3, 17, 22, 28, 41, 30, 26, 18, 35,  7, 30, 31, 34, 10, 37])\n",
      "tensor([ 8, 19, 28, 42,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35]) tensor([ 8, 19, 28, 40,  3, 36, 11, 38, 20, 36, 21, 32,  7, 21, 33, 35])\n",
      "tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28,  0, 34, 23, 25]) tensor([28, 39, 25, 33, 26, 42, 14,  7, 43,  9, 21, 28, 27, 34, 23, 25])\n",
      "tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33]) tensor([38, 37, 38, 17, 42, 38, 11, 11, 40, 19, 40, 42, 29, 41, 21, 33])\n",
      "tensor([27, 41,  5, 10, 12, 18, 43, 14, 18,  0, 17,  0, 42, 41, 12, 21]) tensor([27, 41,  5, 10, 12, 18, 43, 14, 18, 27, 17, 27, 42, 41, 40, 21])\n",
      "tensor([35, 34, 29, 40, 13, 21, 13,  5, 21, 43, 22, 14, 27, 36,  8, 43]) tensor([35, 34, 29, 40, 13, 21, 13, 30, 21, 43, 22, 14, 27, 36,  8, 43])\n",
      "tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27]) tensor([ 9, 13, 25, 26,  8, 26, 35, 18, 11, 26, 19, 27, 36, 24, 25, 27])\n",
      "tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5]) tensor([23, 14,  8, 23, 32,  8, 17, 35, 14, 32, 12, 40, 14, 32, 17,  5])\n",
      "Train Accuracy      : 0.9289772510528564\n",
      "Val Accuracy        : 0.9488636255264282\n",
      "Loss                : 0.014483309350907803\n",
      "Val Loss            : 0.006889604032039642\n",
      "Learning Rate       : 0.0006140399999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...:  11%|█▏        | 5/44 [00:25<03:18,  5.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m epoch_log: EpochLogs\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_log \u001b[38;5;129;01min\u001b[39;00m grid_search(\n\u001b[0;32m      3\u001b[0m     model_factory\u001b[38;5;241m=\u001b[39mAllCNN2D,\n\u001b[0;32m      4\u001b[0m     all_model_parameters\u001b[38;5;241m=\u001b[39mall_model_parameters,\n\u001b[0;32m      5\u001b[0m     optim_factory\u001b[38;5;241m=\u001b[39mAdamW,\n\u001b[0;32m      6\u001b[0m     all_optim_params\u001b[38;5;241m=\u001b[39mall_optim_parameters,\n\u001b[0;32m      7\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m      8\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),\n\u001b[0;32m      9\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     10\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[0;32m     11\u001b[0m     lr_decay_window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     12\u001b[0m     lr_decay_minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     13\u001b[0m     scheduler_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m,\n\u001b[0;32m     14\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m ):\n\u001b[0;32m     16\u001b[0m     train_logpoints: \u001b[38;5;28mlist\u001b[39m[LogPoint] \u001b[38;5;241m=\u001b[39m epoch_log\u001b[38;5;241m.\u001b[39mtrain_logs\n\u001b[0;32m     17\u001b[0m     val_logpoints: \u001b[38;5;28mlist\u001b[39m[LogPoint] \u001b[38;5;241m=\u001b[39m epoch_log\u001b[38;5;241m.\u001b[39mval_logs\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\training\\train.py:334\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(model_factory, all_model_parameters, optim_factory, all_optim_params, epochs, criterion, train_dataloader, val_dataloader, lr_decay_window_size, lr_decay_minimum, scheduler_scale, device, compile_model)\u001b[0m\n\u001b[0;32m    331\u001b[0m optimiser_loss: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m model_optimiser\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    333\u001b[0m train_log: LogPoint\n\u001b[1;32m--> 334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_log \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    335\u001b[0m     train(\n\u001b[0;32m    336\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    337\u001b[0m         optimiser\u001b[38;5;241m=\u001b[39mmodel_optimiser,\n\u001b[0;32m    338\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m    339\u001b[0m         train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m    340\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    341\u001b[0m     ),\n\u001b[0;32m    342\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    343\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[0;32m    344\u001b[0m ):\n\u001b[0;32m    345\u001b[0m     epoch_train_logs\u001b[38;5;241m.\u001b[39mappend(train_log)\n\u001b[0;32m    346\u001b[0m     epoch_cur_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m    347\u001b[0m         train_log\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mdetach(\n\u001b[0;32m    348\u001b[0m         )\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    349\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\training\\train.py:258\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimiser, criterion, train_dataloader, device)\u001b[0m\n\u001b[0;32m    256\u001b[0m X: Tensor\n\u001b[0;32m    257\u001b[0m y: Tensor\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m    260\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m    262\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\dataset\\character_dataset.py:102\u001b[0m, in \u001b[0;36mCharImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     99\u001b[0m image \u001b[38;5;241m=\u001b[39m reduce(image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc h w -> 1 h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Apply random transformations\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_random_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# One-hot encode the label\u001b[39;00m\n\u001b[0;32m    107\u001b[0m one_hot_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_set),\n\u001b[0;32m    109\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    110\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\dataset\\character_dataset.py:137\u001b[0m, in \u001b[0;36mCharImageDataset._apply_random_transformations\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Random rotation\u001b[39;00m\n\u001b[0;32m    133\u001b[0m rotation_angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_limit \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m365\u001b[39m,\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_limit \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m365\u001b[39m\n\u001b[0;32m    136\u001b[0m )\n\u001b[1;32m--> 137\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation_angle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Random translation\u001b[39;00m\n\u001b[0;32m    140\u001b[0m max_dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation_limit \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1140\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# we need to set -angle.\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m matrix \u001b[38;5;241m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[38;5;241m-\u001b[39mangle, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], \u001b[38;5;241m1.0\u001b[39m, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m])\n\u001b[1;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:669\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[0;32m    667\u001b[0m grid \u001b[38;5;241m=\u001b[39m _gen_affine_grid(theta, w\u001b[38;5;241m=\u001b[39mw, h\u001b[38;5;241m=\u001b[39mh, ow\u001b[38;5;241m=\u001b[39mow, oh\u001b[38;5;241m=\u001b[39moh)\n\u001b[1;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_grid_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:560\u001b[0m, in \u001b[0;36m_apply_grid_transform\u001b[1;34m(img, grid, mode, fill)\u001b[0m\n\u001b[0;32m    557\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    558\u001b[0m     img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((img, mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 560\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzeros\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Fill with required color\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:4304\u001b[0m, in \u001b[0;36mgrid_sample\u001b[1;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[0;32m   4296\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   4297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign_corners=True if the old behavior is desired. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee the documentation of grid_sample for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4301\u001b[0m     )\n\u001b[0;32m   4302\u001b[0m     align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epoch_log: EpochLogs\n",
    "for epoch_log in grid_search(\n",
    "    model_factory=AllCNN2D,\n",
    "    all_model_parameters=all_model_parameters,\n",
    "    optim_factory=AdamW,\n",
    "    all_optim_params=all_optim_parameters,\n",
    "    epochs=10000,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    lr_decay_window_size=10,\n",
    "    lr_decay_minimum=0.0,\n",
    "    scheduler_scale=0.85,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    train_logpoints: list[LogPoint] = epoch_log.train_logs\n",
    "    val_logpoints: list[LogPoint] = epoch_log.val_logs\n",
    "    \n",
    "    \n",
    "    train_count: int = 0\n",
    "    val_count: int = 0\n",
    "    \n",
    "    train_losses_tally: float = 0.0\n",
    "    val_losses_tally: float = 0.0\n",
    "    \n",
    "    train_correct_tally: int = 0\n",
    "    val_correct_tally: int = 0\n",
    "    \n",
    "    for log_point in train_logpoints: \n",
    "        \n",
    "        y_hat_pred: torch.Tensor = torch.argmax(\n",
    "            log_point.y_hat, \n",
    "            axis=-1\n",
    "        ).detach().cpu()\n",
    "        \n",
    "        y_pred: torch.Tensor = torch.argmax(\n",
    "            log_point.y, axis=-1\n",
    "        ).detach().cpu()\n",
    "\n",
    "        \n",
    "        train_is_correct = y_hat_pred==y_pred\n",
    "        train_correct_tally += torch.sum(train_is_correct)\n",
    "        \n",
    "        train_losses_tally += torch.sum(log_point.loss)\n",
    "        \n",
    "        train_count += len(y_hat_pred)\n",
    "        \n",
    "    for log_point in val_logpoints: \n",
    "        \n",
    "        y_hat_pred: torch.Tensor = torch.argmax(\n",
    "            log_point.y_hat, \n",
    "            axis=-1\n",
    "        ).detach().cpu()\n",
    "        \n",
    "        y_pred: torch.Tensor = torch.argmax(\n",
    "            log_point.y, axis=-1\n",
    "        ).detach().cpu()\n",
    "        print(y_hat_pred, y_pred)\n",
    "        val_correct_tally += torch.sum(y_hat_pred==y_pred)\n",
    "        \n",
    "        val_losses_tally += torch.sum(log_point.loss)\n",
    "        \n",
    "        val_count += len(y_hat_pred)\n",
    "        \n",
    "    train_accuracy: float = train_correct_tally/train_count\n",
    "    val_accuracy: float = val_correct_tally/val_count\n",
    "    \n",
    "    train_loss: float = train_losses_tally/train_count\n",
    "    val_loss: float = val_losses_tally/val_count\n",
    "    \n",
    "    cur_learning_rate: float = epoch_log.optimiser.param_groups[0][\"lr\"]\n",
    "    \n",
    "    model_checkpoint_path: str = os.path.join(\n",
    "        model_save_dirpath,\n",
    "        f\"{MODEL_NAME}_epoch{epoch_log.epoch}_trainacc{train_accuracy:.5}_valacc{val_accuracy:.5}_Tloss{train_loss:.5}_Vloss{val_loss:.5}_lr{cur_learning_rate}.pkl\"\n",
    "    )\n",
    "    \n",
    "    with open(model_checkpoint_path, \"wb\") as f:\n",
    "        torch.save(epoch_log.model.state_dict(), f)\n",
    "    \n",
    "    print(f\"Train Accuracy      : {train_accuracy}\")\n",
    "    print(f\"Val Accuracy        : {val_accuracy}\")\n",
    "    print(f\"Loss                : {train_loss}\")\n",
    "    print(f\"Val Loss            : {val_loss}\")\n",
    "    print(f\"Learning Rate       : {cur_learning_rate}\")\n",
    "    \n",
    "    log(\n",
    "        epoch_log.epoch,\n",
    "        train_accuracy,\n",
    "        train_loss,\n",
    "        val_accuracy,\n",
    "        val_loss,\n",
    "        cur_learning_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: encoder_conv_blocks.0.0.weight\n",
      "Loaded: encoder_conv_blocks.0.0.bias\n",
      "Loaded: encoder_conv_blocks.0.2.weight\n",
      "Loaded: encoder_conv_blocks.0.2.bias\n",
      "Loaded: encoder_conv_blocks.0.2.running_mean\n",
      "Loaded: encoder_conv_blocks.0.2.running_var\n",
      "Loaded: encoder_conv_blocks.0.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.0.4.weight\n",
      "Loaded: encoder_conv_blocks.0.4.bias\n",
      "Loaded: encoder_conv_blocks.0.6.weight\n",
      "Loaded: encoder_conv_blocks.0.6.bias\n",
      "Loaded: encoder_conv_blocks.0.6.running_mean\n",
      "Loaded: encoder_conv_blocks.0.6.running_var\n",
      "Loaded: encoder_conv_blocks.0.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.1.0.weight\n",
      "Loaded: encoder_conv_blocks.1.0.bias\n",
      "Loaded: encoder_conv_blocks.1.2.weight\n",
      "Loaded: encoder_conv_blocks.1.2.bias\n",
      "Loaded: encoder_conv_blocks.1.2.running_mean\n",
      "Loaded: encoder_conv_blocks.1.2.running_var\n",
      "Loaded: encoder_conv_blocks.1.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.1.4.weight\n",
      "Loaded: encoder_conv_blocks.1.4.bias\n",
      "Loaded: encoder_conv_blocks.1.6.weight\n",
      "Loaded: encoder_conv_blocks.1.6.bias\n",
      "Loaded: encoder_conv_blocks.1.6.running_mean\n",
      "Loaded: encoder_conv_blocks.1.6.running_var\n",
      "Loaded: encoder_conv_blocks.1.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.2.0.weight\n",
      "Loaded: encoder_conv_blocks.2.0.bias\n",
      "Loaded: encoder_conv_blocks.2.2.weight\n",
      "Loaded: encoder_conv_blocks.2.2.bias\n",
      "Loaded: encoder_conv_blocks.2.2.running_mean\n",
      "Loaded: encoder_conv_blocks.2.2.running_var\n",
      "Loaded: encoder_conv_blocks.2.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.2.4.weight\n",
      "Loaded: encoder_conv_blocks.2.4.bias\n",
      "Loaded: encoder_conv_blocks.2.6.weight\n",
      "Loaded: encoder_conv_blocks.2.6.bias\n",
      "Loaded: encoder_conv_blocks.2.6.running_mean\n",
      "Loaded: encoder_conv_blocks.2.6.running_var\n",
      "Loaded: encoder_conv_blocks.2.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.3.0.weight\n",
      "Loaded: encoder_conv_blocks.3.0.bias\n",
      "Loaded: encoder_conv_blocks.3.2.weight\n",
      "Loaded: encoder_conv_blocks.3.2.bias\n",
      "Loaded: encoder_conv_blocks.3.2.running_mean\n",
      "Loaded: encoder_conv_blocks.3.2.running_var\n",
      "Loaded: encoder_conv_blocks.3.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.3.4.weight\n",
      "Loaded: encoder_conv_blocks.3.4.bias\n",
      "Loaded: encoder_conv_blocks.3.6.weight\n",
      "Loaded: encoder_conv_blocks.3.6.bias\n",
      "Loaded: encoder_conv_blocks.3.6.running_mean\n",
      "Loaded: encoder_conv_blocks.3.6.running_var\n",
      "Loaded: encoder_conv_blocks.3.6.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.4.0.weight\n",
      "Loaded: encoder_conv_blocks.4.0.bias\n",
      "Loaded: encoder_conv_blocks.4.2.weight\n",
      "Loaded: encoder_conv_blocks.4.2.bias\n",
      "Loaded: encoder_conv_blocks.4.2.running_mean\n",
      "Loaded: encoder_conv_blocks.4.2.running_var\n",
      "Loaded: encoder_conv_blocks.4.2.num_batches_tracked\n",
      "Loaded: encoder_conv_blocks.4.4.weight\n",
      "Loaded: encoder_conv_blocks.4.4.bias\n",
      "Loaded: encoder_conv_blocks.4.6.weight\n",
      "Loaded: encoder_conv_blocks.4.6.bias\n",
      "Loaded: encoder_conv_blocks.4.6.running_mean\n",
      "Loaded: encoder_conv_blocks.4.6.running_var\n",
      "Loaded: encoder_conv_blocks.4.6.num_batches_tracked\n",
      "Loaded: fully_connected_blocks.0.0.weight\n",
      "Loaded: fully_connected_blocks.0.0.bias\n",
      "Loaded: fully_connected_blocks.1.0.weight\n",
      "Loaded: fully_connected_blocks.1.0.bias\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AllCNN2D                                 [1, 44]                   --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Sequential: 2-1                   [1, 16, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-1                  [1, 16, 64, 64]           160\n",
      "│    │    └─Dropout2d: 3-2               [1, 16, 64, 64]           --\n",
      "│    │    └─BatchNorm2d: 3-3             [1, 16, 64, 64]           32\n",
      "│    │    └─LeakyReLU: 3-4               [1, 16, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-5                  [1, 16, 32, 32]           2,320\n",
      "│    │    └─Dropout2d: 3-6               [1, 16, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
      "│    │    └─LeakyReLU: 3-8               [1, 16, 32, 32]           --\n",
      "│    └─Sequential: 2-2                   [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-9                  [1, 32, 32, 32]           4,640\n",
      "│    │    └─Dropout2d: 3-10              [1, 32, 32, 32]           --\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 32, 32, 32]           64\n",
      "│    │    └─LeakyReLU: 3-12              [1, 32, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-13                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-14              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-15            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-16              [1, 32, 16, 16]           --\n",
      "│    └─Sequential: 2-3                   [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-17                 [1, 32, 16, 16]           9,248\n",
      "│    │    └─Dropout2d: 3-18              [1, 32, 16, 16]           --\n",
      "│    │    └─BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
      "│    │    └─LeakyReLU: 3-20              [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-21                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-22              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-24              [1, 32, 8, 8]             --\n",
      "│    └─Sequential: 2-4                   [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-25                 [1, 32, 8, 8]             9,248\n",
      "│    │    └─Dropout2d: 3-26              [1, 32, 8, 8]             --\n",
      "│    │    └─BatchNorm2d: 3-27            [1, 32, 8, 8]             64\n",
      "│    │    └─LeakyReLU: 3-28              [1, 32, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-29                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-30              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-31            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-32              [1, 32, 4, 4]             --\n",
      "│    └─Sequential: 2-5                   [1, 32, 2, 2]             --\n",
      "│    │    └─Conv2d: 3-33                 [1, 32, 4, 4]             9,248\n",
      "│    │    └─Dropout2d: 3-34              [1, 32, 4, 4]             --\n",
      "│    │    └─BatchNorm2d: 3-35            [1, 32, 4, 4]             64\n",
      "│    │    └─LeakyReLU: 3-36              [1, 32, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-37                 [1, 32, 2, 2]             9,248\n",
      "│    │    └─Dropout2d: 3-38              [1, 32, 2, 2]             --\n",
      "│    │    └─BatchNorm2d: 3-39            [1, 32, 2, 2]             64\n",
      "│    │    └─LeakyReLU: 3-40              [1, 32, 2, 2]             --\n",
      "├─Sequential: 1-2                        [1, 128]                  --\n",
      "│    └─Flatten: 2-6                      [1, 128]                  --\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─Sequential: 2-7                   [1, 64]                   --\n",
      "│    │    └─Linear: 3-41                 [1, 64]                   8,256\n",
      "│    │    └─Dropout: 3-42                [1, 64]                   --\n",
      "│    │    └─LeakyReLU: 3-43              [1, 64]                   --\n",
      "│    └─Sequential: 2-8                   [1, 44]                   --\n",
      "│    │    └─Linear: 3-44                 [1, 44]                   2,860\n",
      "==========================================================================================\n",
      "Total params: 83,548\n",
      "Trainable params: 83,548\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 14.05\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.18\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 2.53\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model: AllCNN2D = AllCNN2D(\n",
    "    **{\n",
    "        \"conv_features\": (1, 16, 32, 32, 32, 32),\n",
    "        \"fully_connected_features\": (64, 44),\n",
    "        \"expected_input_size\": (64, 64),\n",
    "        \"device\": \"cuda\",\n",
    "        \"conv_dropout\": 0.0,#0.075,\n",
    "        \"verbose\": True,\n",
    "        \"name_prefix\": MODEL_NAME,\n",
    "        \"checkpoint_path\": r\"C:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\models\\allcnn\\Indigo_epoch26_trainacc0.71327_valacc0.99057_Tloss0.072851_Vloss0.0056362_lr0.0007224999999999999.pkl\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pad() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char, label \u001b[38;5;129;01min\u001b[39;00m val_char_dataset:\n\u001b[0;32m      2\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(char[\u001b[38;5;241m0\u001b[39m, :, :])\n\u001b[0;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\dataset\\character_dataset.py:100\u001b[0m, in \u001b[0;36mCharImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     97\u001b[0m image \u001b[38;5;241m=\u001b[39m reduce(image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc h w -> 1 h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Apply random transformations\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_random_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# One-hot encode the label\u001b[39;00m\n\u001b[0;32m    105\u001b[0m one_hot_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_set),\n\u001b[0;32m    107\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    108\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leon\\visual-studio\\repos\\Le-o-n\\ocr-model-training\\src\\cnn\\dataset\\character_dataset.py:128\u001b[0m, in \u001b[0;36mCharImageDataset._apply_random_transformations\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Pad the image to prevent cropping during transformations\u001b[39;00m\n\u001b[0;32m    127\u001b[0m pad_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# Adjust padding size as needed\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Random rotation\u001b[39;00m\n\u001b[0;32m    131\u001b[0m rotation_angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_limit \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m365\u001b[39m,\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_limit \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m365\u001b[39m\n\u001b[0;32m    134\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: pad() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "for char, label in val_char_dataset:\n",
    "    plt.imshow(char[0, :, :])\n",
    "    plt.show()\n",
    "    pred: torch.Tensor = model.forward(char.unsqueeze(0)).squeeze()\n",
    "    pred_index: int = torch.argmax(pred).item()\n",
    "    print(chr(int(all_label_classes[pred_index][1:], base=16)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
