Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {
  "attention_probs_dropout_prob": 0.0,
  "encoder_stride": 16,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "image_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "model_type": "vit",
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "patch_size": 16,
  "qkv_bias": false,
  "transformers_version": "4.46.3"
}

Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {
  "activation_dropout": 0.1,
  "activation_function": "relu",
  "add_cross_attention": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "cross_attention_hidden_size": 768,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "eos_token_id": 2,
  "init_std": 0.02,
  "is_decoder": true,
  "layernorm_embedding": false,
  "max_position_embeddings": 1024,
  "model_type": "trocr",
  "pad_token_id": 1,
  "scale_embedding": true,
  "tie_word_embeddings": false,
  "transformers_version": "4.46.3",
  "use_cache": false,
  "use_learned_position_embeddings": false,
  "vocab_size": 50265
}

/dcs/20/u2034788/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 2}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
Training model...:   0%|          | 0/25241 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training model...:   0%|          | 1/25241 [00:02<20:36:17,  2.94s/it]Training model...:   0%|          | 2/25241 [00:04<16:47:49,  2.40s/it]Training model...:   0%|          | 3/25241 [00:07<15:51:44,  2.26s/it]Training model...:   0%|          | 4/25241 [00:09<15:21:49,  2.19s/it]Training model...:   0%|          | 5/25241 [00:11<15:06:30,  2.16s/it]Training model...:   0%|          | 6/25241 [00:13<14:56:55,  2.13s/it]Training model...:   0%|          | 7/25241 [00:15<14:51:07,  2.12s/it]Training model...:   0%|          | 8/25241 [00:17<14:46:38,  2.11s/it]Training model...:   0%|          | 9/25241 [00:19<14:43:15,  2.10s/it]Training model...:   0%|          | 10/25241 [00:21<14:47:01,  2.11s/it]Training model...:   0%|          | 11/25241 [00:23<14:09:01,  2.02s/it]Training model...:   0%|          | 12/25241 [00:24<11:49:54,  1.69s/it]Training model...:   0%|          | 13/25241 [00:25<10:04:33,  1.44s/it]Training model...:   0%|          | 14/25241 [00:25<8:19:30,  1.19s/it] Training model...:   0%|          | 15/25241 [00:26<6:29:22,  1.08it/s]Training model...:   0%|          | 16/25241 [00:26<5:16:43,  1.33it/s]Training model...:   0%|          | 17/25241 [00:26<4:19:38,  1.62it/s]Training model...:   0%|          | 18/25241 [00:27<3:39:04,  1.92it/s]Training model...:   0%|          | 19/25241 [00:27<3:18:06,  2.12it/s]Training model...:   0%|          | 20/25241 [00:27<2:52:49,  2.43it/s]Training model...:   0%|          | 21/25241 [00:28<2:42:52,  2.58it/s]Training model...:   0%|          | 22/25241 [00:28<2:35:40,  2.70it/s]Training model...:   0%|          | 23/25241 [00:28<2:41:06,  2.61it/s]Training model...:   0%|          | 24/25241 [00:29<2:32:18,  2.76it/s]Training model...:   0%|          | 25/25241 [00:29<2:28:29,  2.83it/s]Training model...:   0%|          | 26/25241 [00:30<3:25:57,  2.04it/s]Training model...:   0%|          | 27/25241 [00:31<3:46:27,  1.86it/s]Training model...:   0%|          | 28/25241 [00:31<3:43:01,  1.88it/s]Training model...:   0%|          | 29/25241 [00:32<4:51:00,  1.44it/s]Training model...:   0%|          | 30/25241 [00:33<4:44:31,  1.48it/s]Training model...:   0%|          | 31/25241 [00:33<4:45:57,  1.47it/s]Training model...:   0%|          | 32/25241 [00:34<4:04:11,  1.72it/s]Training model...:   0%|          | 33/25241 [00:34<3:34:44,  1.96it/s]Training model...:   0%|          | 34/25241 [00:35<3:18:47,  2.11it/s]Training model...:   0%|          | 35/25241 [00:35<3:14:42,  2.16it/s]Training model...:   0%|          | 36/25241 [00:35<2:57:19,  2.37it/s]Training model...:   0%|          | 37/25241 [00:36<3:18:33,  2.12it/s]Training model...:   0%|          | 38/25241 [00:37<4:02:56,  1.73it/s]Training model...:   0%|          | 39/25241 [00:38<4:49:50,  1.45it/s]Training model...:   0%|          | 40/25241 [00:39<5:34:26,  1.26it/s]Training model...:   0%|          | 41/25241 [00:40<6:54:34,  1.01it/s]Training model...:   0%|          | 42/25241 [00:41<6:41:53,  1.05it/s]Training model...:   0%|          | 43/25241 [00:42<6:04:13,  1.15it/s]Training model...:   0%|          | 44/25241 [00:43<6:11:02,  1.13it/s]Training model...:   0%|          | 45/25241 [00:43<5:44:42,  1.22it/s]Training model...:   0%|          | 46/25241 [00:44<5:06:26,  1.37it/s]Training model...:   0%|          | 47/25241 [00:44<4:42:34,  1.49it/s]Training model...:   0%|          | 48/25241 [00:45<4:08:28,  1.69it/s]Training model...:   0%|          | 49/25241 [00:45<3:44:27,  1.87it/s]Training model...:   0%|          | 50/25241 [00:46<3:28:54,  2.01it/s]Training model...:   0%|          | 51/25241 [00:46<3:12:07,  2.19it/s]Training model...:   0%|          | 52/25241 [00:46<2:53:47,  2.42it/s]Training model...:   0%|          | 53/25241 [00:47<2:42:45,  2.58it/s]Training model...:   0%|          | 54/25241 [00:47<2:35:26,  2.70it/s]Training model...:   0%|          | 55/25241 [00:47<2:29:14,  2.81it/s]Training model...:   0%|          | 56/25241 [00:48<2:27:24,  2.85it/s]Training model...:   0%|          | 57/25241 [00:48<2:31:41,  2.77it/s]Training model...:   0%|          | 58/25241 [00:48<2:59:57,  2.33it/s]Training model...:   0%|          | 59/25241 [00:49<4:09:24,  1.68it/s]Training model...:   0%|          | 60/25241 [00:50<4:13:20,  1.66it/s]Training model...:   0%|          | 61/25241 [00:51<4:43:47,  1.48it/s]Training model...:   0%|          | 62/25241 [00:52<4:44:45,  1.47it/s]Training model...:   0%|          | 63/25241 [00:53<5:27:01,  1.28it/s]Training model...:   0%|          | 64/25241 [00:53<5:35:05,  1.25it/s]Training model...:   0%|          | 65/25241 [00:54<5:11:45,  1.35it/s]Training model...:   0%|          | 66/25241 [00:55<5:10:48,  1.35it/s]Training model...:   0%|          | 67/25241 [00:55<4:33:39,  1.53it/s]Training model...:   0%|          | 68/25241 [00:56<4:35:44,  1.52it/s]Training model...:   0%|          | 69/25241 [00:57<4:53:38,  1.43it/s]Training model...:   0%|          | 70/25241 [00:57<4:06:45,  1.70it/s]Training model...:   0%|          | 71/25241 [00:57<3:30:16,  1.99it/s]Training model...:   0%|          | 72/25241 [00:58<3:28:50,  2.01it/s]Training model...:   0%|          | 73/25241 [00:58<3:45:12,  1.86it/s]Training model...:   0%|          | 74/25241 [00:59<3:21:08,  2.09it/s]Training model...:   0%|          | 75/25241 [00:59<3:11:30,  2.19it/s]Training model...:   0%|          | 76/25241 [01:00<3:57:38,  1.76it/s]Training model...:   0%|          | 77/25241 [01:01<4:05:31,  1.71it/s]Training model...:   0%|          | 78/25241 [01:01<4:30:20,  1.55it/s]Training model...:   0%|          | 79/25241 [01:02<4:50:51,  1.44it/s]Training model...:   0%|          | 80/25241 [01:03<5:33:15,  1.26it/s]Training model...:   0%|          | 81/25241 [01:05<6:32:38,  1.07it/s]Training model...:   0%|          | 82/25241 [01:05<5:41:34,  1.23it/s]Training model...:   0%|          | 83/25241 [01:06<5:24:10,  1.29it/s]Training model...:   0%|          | 84/25241 [01:06<4:32:49,  1.54it/s]Training model...:   0%|          | 85/25241 [01:07<4:52:04,  1.44it/s]Training model...:   0%|          | 86/25241 [01:08<5:15:19,  1.33it/s]Training model...:   0%|          | 87/25241 [01:08<4:47:24,  1.46it/s]Training model...:   0%|          | 88/25241 [01:09<4:35:16,  1.52it/s]Training model...:   0%|          | 89/25241 [01:10<4:53:11,  1.43it/s]Training model...:   0%|          | 90/25241 [01:10<4:29:35,  1.55it/s]Training model...:   0%|          | 91/25241 [01:11<3:52:11,  1.81it/s]Training model...:   0%|          | 92/25241 [01:11<3:30:52,  1.99it/s]Training model...:   0%|          | 93/25241 [01:11<3:03:51,  2.28it/s]Training model...:   0%|          | 94/25241 [01:12<3:14:14,  2.16it/s]Training model...:   0%|          | 95/25241 [01:12<2:58:14,  2.35it/s]Training model...:   0%|          | 96/25241 [01:13<3:34:56,  1.95it/s]Training model...:   0%|          | 97/25241 [01:13<3:18:13,  2.11it/s]Training model...:   0%|          | 98/25241 [01:14<3:26:01,  2.03it/s]Training model...:   0%|          | 99/25241 [01:15<4:43:42,  1.48it/s]Training model...:   0%|          | 100/25241 [01:16<5:34:59,  1.25it/s]Training model...:   0%|          | 101/25241 [01:17<5:55:11,  1.18it/s]Training model...:   0%|          | 102/25241 [01:18<5:48:16,  1.20it/s]Training model...:   0%|          | 103/25241 [01:19<5:55:51,  1.18it/s]Training model...:   0%|          | 104/25241 [01:19<5:20:52,  1.31it/s]Training model...:   0%|          | 105/25241 [01:20<4:58:14,  1.40it/s]Training model...:   0%|          | 106/25241 [01:21<5:06:20,  1.37it/s]Training model...:   0%|          | 107/25241 [01:21<5:23:42,  1.29it/s]Training model...:   0%|          | 108/25241 [01:23<7:10:14,  1.03s/it]Training model...:   0%|          | 109/25241 [01:24<6:13:06,  1.12it/s]Training model...:   0%|          | 110/25241 [01:24<5:41:42,  1.23it/s]Training model...:   0%|          | 111/25241 [01:25<5:18:46,  1.31it/s]Training model...:   0%|          | 112/25241 [01:26<6:05:01,  1.15it/s]Training model...:   0%|          | 113/25241 [01:27<5:17:21,  1.32it/s]Training model...:   0%|          | 114/25241 [01:27<5:44:14,  1.22it/s]Training model...:   0%|          | 115/25241 [01:29<6:53:34,  1.01it/s]Training model...:   0%|          | 116/25241 [01:30<6:28:06,  1.08it/s]Training model...:   0%|          | 117/25241 [01:30<5:31:30,  1.26it/s]Training model...:   0%|          | 118/25241 [01:31<5:25:29,  1.29it/s]Training model...:   0%|          | 119/25241 [01:31<4:27:20,  1.57it/s]Training model...:   0%|          | 120/25241 [01:32<3:51:51,  1.81it/s]Training model...:   0%|          | 121/25241 [01:32<3:18:51,  2.11it/s]Training model...:   0%|          | 122/25241 [01:32<3:38:38,  1.91it/s]Training model...:   0%|          | 123/25241 [01:33<4:18:55,  1.62it/s]Training model...:   0%|          | 124/25241 [01:34<4:26:58,  1.57it/s]Training model...:   0%|          | 125/25241 [01:34<4:00:10,  1.74it/s]Training model...:   0%|          | 126/25241 [01:35<4:11:44,  1.66it/s]Training model...:   1%|          | 127/25241 [01:36<4:19:03,  1.62it/s]Training model...:   1%|          | 128/25241 [01:36<4:19:54,  1.61it/s]Training model...:   1%|          | 129/25241 [01:37<3:49:41,  1.82it/s]Training model...:   1%|          | 130/25241 [01:37<4:01:16,  1.73it/s]Training model...:   1%|          | 131/25241 [01:38<3:41:59,  1.89it/s]Training model...:   1%|          | 132/25241 [01:38<3:32:49,  1.97it/s]Training model...:   1%|          | 133/25241 [01:39<4:12:17,  1.66it/s]Training model...:   1%|          | 134/25241 [01:39<3:34:53,  1.95it/s]Training model...:   1%|          | 135/25241 [01:41<5:23:22,  1.29it/s]Training model...:   1%|          | 136/25241 [01:43<8:05:30,  1.16s/it]Training model...:   1%|          | 137/25241 [01:44<8:16:23,  1.19s/it]Training model...:   1%|          | 138/25241 [01:46<10:07:03,  1.45s/it]Training model...:   1%|          | 139/25241 [01:48<10:36:30,  1.52s/it]Training model...:   1%|          | 140/25241 [01:48<8:44:41,  1.25s/it] Training model...:   1%|          | 141/25241 [01:50<9:26:33,  1.35s/it]Training model...:   1%|          | 142/25241 [01:51<7:49:02,  1.12s/it]Training model...:   1%|          | 143/25241 [01:51<6:38:57,  1.05it/s]Training model...:   1%|          | 144/25241 [01:52<6:11:58,  1.12it/s]Training model...:   1%|          | 145/25241 [01:52<5:03:31,  1.38it/s]Training model...:   1%|          | 146/25241 [01:53<4:48:47,  1.45it/s]Training model...:   1%|          | 147/25241 [01:54<5:32:40,  1.26it/s]Training model...:   1%|          | 148/25241 [01:54<4:53:17,  1.43it/s]Training model...:   1%|          | 149/25241 [01:56<7:18:11,  1.05s/it]Training model...:   1%|          | 150/25241 [01:57<5:56:33,  1.17it/s]Training model...:   1%|          | 151/25241 [01:58<7:26:36,  1.07s/it]Training model...:   1%|          | 152/25241 [01:59<7:26:18,  1.07s/it]Training model...:   1%|          | 153/25241 [02:01<9:19:46,  1.34s/it]Training model...:   1%|          | 154/25241 [02:02<7:31:49,  1.08s/it]Training model...:   1%|          | 155/25241 [02:03<6:54:46,  1.01it/s]Training model...:   1%|          | 156/25241 [02:03<6:04:35,  1.15it/s]Training model...:   1%|          | 157/25241 [02:04<6:38:11,  1.05it/s]Training model...:   1%|          | 158/25241 [02:05<5:37:28,  1.24it/s]Training model...:   1%|          | 159/25241 [02:05<5:03:38,  1.38it/s]Training model...:   1%|          | 160/25241 [02:06<4:58:30,  1.40it/s]Training model...:   1%|          | 161/25241 [02:07<4:47:09,  1.46it/s]Training model...:   1%|          | 162/25241 [02:07<4:45:27,  1.46it/s]Training model...:   1%|          | 163/25241 [02:08<5:35:25,  1.25it/s]Training model...:   1%|          | 164/25241 [02:09<4:55:57,  1.41it/s]Training model...:   1%|          | 165/25241 [02:11<8:09:37,  1.17s/it]Training model...:   1%|          | 166/25241 [02:13<10:03:19,  1.44s/it]Training model...:   1%|          | 167/25241 [02:14<8:38:51,  1.24s/it] Training model...:   1%|          | 168/25241 [02:16<10:05:55,  1.45s/it]Training model...:   1%|          | 169/25241 [02:17<8:59:38,  1.29s/it] Training model...:   1%|          | 170/25241 [02:19<10:34:22,  1.52s/it]Training model...:   1%|          | 171/25241 [02:20<9:20:59,  1.34s/it] Training model...:   1%|          | 172/25241 [02:20<8:04:11,  1.16s/it]Training model...:   1%|          | 173/25241 [02:21<7:25:10,  1.07s/it]Training model...:   1%|          | 174/25241 [02:23<7:58:54,  1.15s/it]Training model...:   1%|          | 175/25241 [02:24<7:46:55,  1.12s/it]Training model...:   1%|          | 176/25241 [02:24<6:55:12,  1.01it/s]Training model...:   1%|          | 177/25241 [02:25<6:19:48,  1.10it/s]Training model...:   1%|          | 178/25241 [02:26<5:45:22,  1.21it/s]Training model...:   1%|          | 179/25241 [02:27<5:42:29,  1.22it/s]Training model...:   1%|          | 180/25241 [02:27<5:14:29,  1.33it/s]Training model...:   1%|          | 181/25241 [02:28<4:25:12,  1.57it/s]Training model...:   1%|          | 182/25241 [02:28<4:35:34,  1.52it/s]Training model...:   1%|          | 183/25241 [02:30<7:21:18,  1.06s/it]Training model...:   1%|          | 184/25241 [02:31<5:44:39,  1.21it/s]Training model...:   1%|          | 185/25241 [02:31<5:06:02,  1.36it/s]Training model...:   1%|          | 186/25241 [02:31<4:15:50,  1.63it/s]Training model...:   1%|          | 187/25241 [02:32<4:02:07,  1.72it/s]Training model...:   1%|          | 188/25241 [02:32<3:34:45,  1.94it/s]Training model...:   1%|          | 189/25241 [02:33<3:56:36,  1.76it/s]Training model...:   1%|          | 190/25241 [02:34<4:41:30,  1.48it/s]Training model...:   1%|          | 191/25241 [02:35<4:58:55,  1.40it/s]Training model...:   1%|          | 192/25241 [02:36<5:29:24,  1.27it/s]Training model...:   1%|          | 193/25241 [02:36<5:03:33,  1.38it/s]Training model...:   1%|          | 194/25241 [02:37<4:53:48,  1.42it/s]Training model...:   1%|          | 195/25241 [02:38<5:19:00,  1.31it/s]Training model...:   1%|          | 196/25241 [02:40<7:59:44,  1.15s/it]Training model...:   1%|          | 197/25241 [02:41<8:09:19,  1.17s/it]Training model...:   1%|          | 198/25241 [02:42<7:05:41,  1.02s/it]Training model...:   1%|          | 199/25241 [02:43<7:05:57,  1.02s/it]Training model...:   1%|          | 200/25241 [02:43<6:13:38,  1.12it/s]Training model...:   1%|          | 201/25241 [02:44<5:05:46,  1.36it/s]Training model...:   1%|          | 202/25241 [02:44<4:35:58,  1.51it/s]Training model...:   1%|          | 203/25241 [02:45<4:15:43,  1.63it/s]Training model...:   1%|          | 204/25241 [02:45<3:44:38,  1.86it/s]Training model...:   1%|          | 205/25241 [02:45<3:17:26,  2.11it/s]Training model...:   1%|          | 206/25241 [02:46<2:55:23,  2.38it/s]Training model...:   1%|          | 207/25241 [02:46<2:46:26,  2.51it/s]Training model...:   1%|          | 208/25241 [02:46<2:38:02,  2.64it/s]Training model...:   1%|          | 209/25241 [02:47<2:57:46,  2.35it/s]Training model...:   1%|          | 210/25241 [02:47<3:23:20,  2.05it/s]Training model...:   1%|          | 211/25241 [02:48<3:10:30,  2.19it/s]Training model...:   1%|          | 212/25241 [02:48<3:19:49,  2.09it/s]Training model...:   1%|          | 213/25241 [02:49<3:33:46,  1.95it/s]Training model...:   1%|          | 214/25241 [02:50<4:15:07,  1.63it/s]Training model...:   1%|          | 215/25241 [02:50<4:00:01,  1.74it/s]Training model...:   1%|          | 216/25241 [02:51<3:59:23,  1.74it/s]Training model...:   1%|          | 217/25241 [02:52<4:18:18,  1.61it/s]Training model...:   1%|          | 218/25241 [02:52<4:36:37,  1.51it/s]Training model...:   1%|          | 219/25241 [02:54<5:41:20,  1.22it/s]Training model...:   1%|          | 220/25241 [02:54<4:48:58,  1.44it/s]Training model...:   1%|          | 221/25241 [02:55<5:18:52,  1.31it/s]Training model...:   1%|          | 222/25241 [02:55<4:19:28,  1.61it/s]Training model...:   1%|          | 223/25241 [02:56<4:40:52,  1.48it/s]Training model...:   1%|          | 224/25241 [02:56<3:56:24,  1.76it/s]Training model...:   1%|          | 225/25241 [02:57<3:30:55,  1.98it/s]Training model...:   1%|          | 226/25241 [02:57<3:28:40,  2.00it/s]Training model...:   1%|          | 227/25241 [02:58<3:11:44,  2.17it/s]Training model...:   1%|          | 228/25241 [02:58<3:24:28,  2.04it/s]Training model...:   1%|          | 229/25241 [02:59<3:51:12,  1.80it/s]Training model...:   1%|          | 230/25241 [03:00<4:28:32,  1.55it/s]Training model...:   1%|          | 231/25241 [03:00<4:47:06,  1.45it/s]Training model...:   1%|          | 232/25241 [03:01<4:15:18,  1.63it/s]Training model...:   1%|          | 233/25241 [03:02<5:44:31,  1.21it/s]Training model...:   1%|          | 234/25241 [03:03<5:02:18,  1.38it/s]Training model...:   1%|          | 235/25241 [03:04<5:23:51,  1.29it/s]Training model...:   1%|          | 236/25241 [03:04<5:05:55,  1.36it/s]Training model...:   1%|          | 237/25241 [03:05<4:41:33,  1.48it/s]Training model...:   1%|          | 238/25241 [03:05<4:48:41,  1.44it/s]Training model...:   1%|          | 239/25241 [03:07<6:22:19,  1.09it/s]Training model...:   1%|          | 240/25241 [03:08<5:47:32,  1.20it/s]Training model...:   1%|          | 241/25241 [03:08<5:09:27,  1.35it/s]Training model...:   1%|          | 242/25241 [03:09<4:42:59,  1.47it/s]Training model...:   1%|          | 243/25241 [03:09<4:34:03,  1.52it/s]Training model...:   1%|          | 244/25241 [03:10<4:11:52,  1.65it/s]Training model...:   1%|          | 245/25241 [03:10<3:33:22,  1.95it/s]Training model...:   1%|          | 246/25241 [03:10<3:22:48,  2.05it/s]Training model...:   1%|          | 247/25241 [03:11<3:17:11,  2.11it/s]Training model...:   1%|          | 248/25241 [03:11<2:57:31,  2.35it/s]Training model...:   1%|          | 249/25241 [03:12<3:28:50,  1.99it/s]Training model...:   1%|          | 250/25241 [03:12<3:12:34,  2.16it/s]Training model...:   1%|          | 251/25241 [03:13<2:59:25,  2.32it/s]Training model...:   1%|          | 252/25241 [03:13<3:53:26,  1.78it/s]Training model...:   1%|          | 253/25241 [03:14<4:08:59,  1.67it/s]Training model...:   1%|          | 254/25241 [03:15<3:39:51,  1.89it/s]Training model...:   1%|          | 255/25241 [03:15<4:29:38,  1.54it/s]Training model...:   1%|          | 256/25241 [03:16<5:09:13,  1.35it/s]Training model...:   1%|          | 257/25241 [03:17<5:23:24,  1.29it/s]Training model...:   1%|          | 258/25241 [03:18<5:02:33,  1.38it/s]Training model...:   1%|          | 258/25241 [03:18<5:20:17,  1.30it/s]
Traceback (most recent call last):
  File "/dcs/20/u2034788/Documents/Project/ocr-model-training/src/main.py", line 216, in <module>
    for i, data in tqdm(
                   ^^^^^
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 757, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py", line 398, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate
    collate(samples, collate_fn_map=collate_fn_map)
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py", line 155, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/dcs/20/u2034788/.local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py", line 272, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [128] at entry 0 and [140] at entry 1
